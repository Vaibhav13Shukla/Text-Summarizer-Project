{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327bbe40-27b7-4845-a99d-9e22073d9b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NEW Block 1: Setup and Configuration (Modified for Speed)\n",
    "# import os\n",
    "# import re\n",
    "# import json\n",
    "# import time\n",
    "# import logging\n",
    "# from datetime import datetime\n",
    "# import importlib.metadata\n",
    "# import gc\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "# from tensorflow.keras import layers\n",
    "# from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
    "\n",
    "# # <<< CHANGE: Enable Mixed Precision >>>\n",
    "# # Using float16 for computations where possible can speed up training on compatible GPUs (like L4)\n",
    "# # and reduce memory usage.\n",
    "# tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "# print(\"Attempting to enable Mixed Precision (mixed_float16)\")\n",
    "# logging.info(\"Attempting to enable Mixed Precision (mixed_float16)\")\n",
    "\n",
    "\n",
    "# # --- Library Imports/Checks (Keep as before) ---\n",
    "# try:\n",
    "#     import sentencepiece as spm\n",
    "# except ImportError:\n",
    "#     print(\"SentencePiece not found. You might need to install it (`pip install sentencepiece`)\")\n",
    "#     spm = None # Indicate missing library\n",
    "\n",
    "# try:\n",
    "#     from langdetect import detect, DetectorFactory\n",
    "#     from langdetect.lang_detect_exception import LangDetectException\n",
    "#     DetectorFactory.seed = 0\n",
    "#     _langdetect_installed = True\n",
    "# except ImportError:\n",
    "#     print(\"langdetect not found. You might need to install it (`pip install langdetect`)\")\n",
    "#     _langdetect_installed = False\n",
    "\n",
    "# try:\n",
    "#     from rouge_score import rouge_scorer, scoring\n",
    "# except ImportError:\n",
    "#     print(\"rouge-score not found. You might need to install it (`pip install rouge-score nltk`)\")\n",
    "#     try:\n",
    "#         import nltk\n",
    "#         nltk.download('punkt', quiet=True)\n",
    "#     except ImportError:\n",
    "#         print(\"NLTK not found, which might be needed for rouge-score.\")\n",
    "\n",
    "# from tqdm.notebook import tqdm\n",
    "\n",
    "# # --- Configuration ---\n",
    "# OUTPUT_DIR = 'model3_files'\n",
    "# os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "# print(f\"Ensuring output directory exists: {os.path.abspath(OUTPUT_DIR)}\")\n",
    "\n",
    "# # File Paths\n",
    "# INPUT_JSONL = 'mergedt04.jsonl'\n",
    "# OUTPUT_PARQUET = os.path.join(OUTPUT_DIR, 'processed_dataframe.parquet')\n",
    "# TOKENIZER_MODEL_PREFIX = os.path.join(OUTPUT_DIR, 'pib_summarizer_spm_50k') # Keep same tokenizer\n",
    "# TOKENIZER_MODEL_FILE = f'{TOKENIZER_MODEL_PREFIX}.model'\n",
    "# LOG_DIR = os.path.join(OUTPUT_DIR, \"logs\", \"fit\", datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "# MODEL_SAVE_PATH = os.path.join(OUTPUT_DIR, 'pib_summarizer_no_attention_FAST.keras') # Changed model name\n",
    "\n",
    "# # Data Processing Params (Keep Same - avoid re-running Blocks 2-4 if possible)\n",
    "# MIN_INPUT_WORDS = 20\n",
    "# MIN_SUMMARY_WORDS = 5\n",
    "# LANG_DETECT_THRESHOLD = 0.90\n",
    "\n",
    "# # Tokenizer Params (Keep Same - avoid re-running Block 4)\n",
    "# VOCAB_SIZE = 50000\n",
    "# PAD_ID = 0\n",
    "# UNK_ID = 1\n",
    "# START_ID = 2\n",
    "# END_ID = 3\n",
    "\n",
    "# # <<< CHANGE: Model Hyperparameters for Speed >>>\n",
    "# EMBEDDING_DIM = 300 # Keep embedding size (important for representation)\n",
    "# LSTM_UNITS = 512  # Reduced from 1024\n",
    "# DECODER_LSTM_UNITS = LSTM_UNITS * 2 # Now 1024 (was 2048)\n",
    "# NUM_ENCODER_LAYERS = 2 # Reduced from 3\n",
    "# NUM_DECODER_LAYERS = 2 # Reduced from 3\n",
    "# DROPOUT_RATE = 0.2 # Keep dropout\n",
    "# MAX_LEN_INPUT = 1024 # Keep sequence lengths\n",
    "# MAX_LEN_SUMMARY = 150 # Keep sequence lengths\n",
    "\n",
    "# # <<< CHANGE: Training Params - Increase Batch Size (Trial) >>>\n",
    "# BATCH_SIZE = 128 # Increased from 32/64 - Relying on Mixed Precision & smaller model\n",
    "# EPOCHS = 30 # Reduce max epochs slightly, relying on early stopping\n",
    "# LEARNING_RATE = 0.001 # Keep LR\n",
    "# EARLY_STOPPING_PATIENCE = 5 # Keep patience\n",
    "# REDUCE_LR_PATIENCE = 3 # Keep patience\n",
    "# REDUCE_LR_FACTOR = 0.2 # Keep factor\n",
    "\n",
    "# # Inference Params\n",
    "# BEAM_WIDTH = 5\n",
    "\n",
    "# # --- Setup Logging ---\n",
    "# LOG_FILE_PATH = os.path.join(OUTPUT_DIR, 'training_log_FAST.log') # Changed log file name\n",
    "# # Ensure logging handlers are cleared if re-running the cell in the same kernel session\n",
    "# root_logger = logging.getLogger()\n",
    "# if root_logger.hasHandlers():\n",
    "#     root_logger.handlers.clear()\n",
    "\n",
    "# logging.basicConfig(level=logging.INFO,\n",
    "#                     format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "#                     handlers=[\n",
    "#                         logging.FileHandler(LOG_FILE_PATH),\n",
    "#                         logging.StreamHandler()\n",
    "#                     ])\n",
    "# print(f\"Logging setup complete. Log file: {LOG_FILE_PATH}\")\n",
    "\n",
    "\n",
    "# # --- Debugging Info ---\n",
    "# print(\"\\n--- Configuration (Optimized for Speed) ---\")\n",
    "# logging.info(\"--- Configuration (Optimized for Speed) ---\")\n",
    "# print(f\"Mixed Precision Enabled: {'mixed_float16' in tf.keras.mixed_precision.global_policy().name}\")\n",
    "# logging.info(f\"Mixed Precision Enabled: {'mixed_float16' in tf.keras.mixed_precision.global_policy().name}\")\n",
    "# # ... (rest of the debugging prints remain the same, but will reflect new values) ...\n",
    "# print(f\"TensorFlow Version: {tf.__version__}\"); logging.info(f\"TensorFlow Version: {tf.__version__}\")\n",
    "# try:\n",
    "#     print(f\"SentencePiece Version: {spm.__version__}\"); logging.info(f\"SentencePiece Version: {spm.__version__}\")\n",
    "# except NameError: print(\"SentencePiece not imported.\"); logging.warning(\"SentencePiece not imported.\")\n",
    "\n",
    "# if _langdetect_installed:\n",
    "#     try:\n",
    "#         langdetect_version = importlib.metadata.version(\"langdetect\")\n",
    "#         print(f\"Langdetect Version: {langdetect_version}\"); logging.info(f\"Langdetect Version: {langdetect_version}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Langdetect Version: Could not determine version ({e})\"); logging.warning(f\"Langdetect Version: Could not determine version ({e})\")\n",
    "# else:\n",
    "#      print(\"Langdetect: Not installed or failed to import.\"); logging.warning(\"Langdetect: Not installed or failed to import.\")\n",
    "\n",
    "# gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "# print(f\"GPU Available: {gpu_devices}\"); logging.info(f\"GPU Available: {gpu_devices}\")\n",
    "# print(f\"Input JSONL: {INPUT_JSONL}\"); logging.info(f\"Input JSONL: {INPUT_JSONL}\")\n",
    "# print(f\"Output Parquet Cache: {OUTPUT_PARQUET}\"); logging.info(f\"Output Parquet Cache: {OUTPUT_PARQUET}\")\n",
    "# print(f\"Tokenizer Model Prefix: {TOKENIZER_MODEL_PREFIX}\"); logging.info(f\"Tokenizer Model Prefix: {TOKENIZER_MODEL_PREFIX}\")\n",
    "# print(f\"Model Save Path: {MODEL_SAVE_PATH}\"); logging.info(f\"Model Save Path: {MODEL_SAVE_PATH}\")\n",
    "# print(f\"TensorBoard Log Dir: {LOG_DIR}\"); logging.info(f\"TensorBoard Log Dir: {LOG_DIR}\")\n",
    "# print(f\"Vocab Size: {VOCAB_SIZE}\"); logging.info(f\"Vocab Size: {VOCAB_SIZE}\")\n",
    "# print(f\"Embedding Dim: {EMBEDDING_DIM}\"); logging.info(f\"Embedding Dim: {EMBEDDING_DIM}\")\n",
    "# print(f\"Encoder LSTM Units (per direction): {LSTM_UNITS}\"); logging.info(f\"Encoder LSTM Units (per direction): {LSTM_UNITS}\")\n",
    "# print(f\"Decoder LSTM Units: {DECODER_LSTM_UNITS}\"); logging.info(f\"Decoder LSTM Units: {DECODER_LSTM_UNITS}\")\n",
    "# print(f\"Encoder Layers: {NUM_ENCODER_LAYERS}\"); logging.info(f\"Encoder Layers: {NUM_ENCODER_LAYERS}\")\n",
    "# print(f\"Decoder Layers: {NUM_DECODER_LAYERS}\"); logging.info(f\"Decoder Layers: {NUM_DECODER_LAYERS}\")\n",
    "# print(f\"Max Input Length: {MAX_LEN_INPUT}\"); logging.info(f\"Max Input Length: {MAX_LEN_INPUT}\")\n",
    "# print(f\"Max Summary Length: {MAX_LEN_SUMMARY}\"); logging.info(f\"Max Summary Length: {MAX_LEN_SUMMARY}\")\n",
    "# print(f\"Batch Size: {BATCH_SIZE}\"); logging.info(f\"Batch Size: {BATCH_SIZE}\")\n",
    "# print(f\"Max Epochs: {EPOCHS}\"); logging.info(f\"Max Epochs: {EPOCHS}\")\n",
    "# print(f\"Learning Rate: {LEARNING_RATE}\"); logging.info(f\"Learning Rate: {LEARNING_RATE}\")\n",
    "# print(\"-\" * 30); logging.info(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfec17f1-748b-4b21-97a8-83d4a30f4d08",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 15:44:49.109618: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-24 15:44:49.570731: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-24 15:44:51.480725: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/lib/x86_64-linux-gnu/:/opt/conda/lib\n",
      "2025-04-24 15:44:51.480876: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/lib/x86_64-linux-gnu/:/opt/conda/lib\n",
      "2025-04-24 15:44:51.480887: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2025-04-24 15:44:54,511 - INFO - --- Configuration ---\n",
      "2025-04-24 15:44:54,518 - INFO - TensorFlow Version: 2.11.0\n",
      "2025-04-24 15:44:54,519 - INFO - SentencePiece Version: 0.2.0\n",
      "2025-04-24 15:44:54,522 - INFO - Langdetect Version: 1.0.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensuring output directory exists: /home/jupyter/model3_files\n",
      "Logging setup complete. Log file: model3_files/training_log.log\n",
      "\n",
      "--- Configuration ---\n",
      "TensorFlow Version: 2.11.0\n",
      "SentencePiece Version: 0.2.0\n",
      "Langdetect Version: 1.0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 15:44:54,915 - INFO - GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "2025-04-24 15:44:54,916 - INFO - Input JSONL: mergedt04.jsonl\n",
      "2025-04-24 15:44:54,918 - INFO - Output Parquet Cache: model3_files/processed_dataframe.parquet\n",
      "2025-04-24 15:44:54,919 - INFO - Tokenizer Model Prefix: model3_files/pib_summarizer_spm_50k\n",
      "2025-04-24 15:44:54,919 - INFO - Model Save Path: model3_files/pib_summarizer_no_attention.keras\n",
      "2025-04-24 15:44:54,920 - INFO - TensorBoard Log Dir: model3_files/logs/fit/20250424-154454\n",
      "2025-04-24 15:44:54,922 - INFO - Vocab Size: 30000\n",
      "2025-04-24 15:44:54,922 - INFO - Embedding Dim: 100\n",
      "2025-04-24 15:44:54,923 - INFO - Encoder LSTM Units (per direction): 256\n",
      "2025-04-24 15:44:54,925 - INFO - Decoder LSTM Units: 512\n",
      "2025-04-24 15:44:54,926 - INFO - Max Input Length: 1024\n",
      "2025-04-24 15:44:54,926 - INFO - Max Summary Length: 150\n",
      "2025-04-24 15:44:54,927 - INFO - Batch Size: 64\n",
      "2025-04-24 15:44:54,929 - INFO - Epochs: 30\n",
      "2025-04-24 15:44:54,930 - INFO - Learning Rate: 0.001\n",
      "2025-04-24 15:44:54,931 - INFO - ------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Input JSONL: mergedt04.jsonl\n",
      "Output Parquet Cache: model3_files/processed_dataframe.parquet\n",
      "Tokenizer Model Prefix: model3_files/pib_summarizer_spm_50k\n",
      "Model Save Path: model3_files/pib_summarizer_no_attention.keras\n",
      "TensorBoard Log Dir: model3_files/logs/fit/20250424-154454\n",
      "Vocab Size: 30000\n",
      "Embedding Dim: 100\n",
      "Encoder LSTM Units (per direction): 256\n",
      "Decoder LSTM Units: 512\n",
      "Max Input Length: 1024\n",
      "Max Summary Length: 150\n",
      "Batch Size: 64\n",
      "Epochs: 30\n",
      "Learning Rate: 0.001\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Block 1: Setup and Configuration (Modified for Output Directory)\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import importlib.metadata # Use for getting package versions (Python 3.8+)\n",
    "# import pkg_resources # Fallback if importlib.metadata is not available or desired\n",
    "import gc # For garbage collection\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
    "\n",
    "# Check if SentencePiece is available, install if not (useful in notebooks)\n",
    "try:\n",
    "    import sentencepiece as spm\n",
    "except ImportError:\n",
    "    print(\"SentencePiece not found. You might need to install it (`pip install sentencepiece`)\")\n",
    "    # If in a notebook, uncomment and run the next line, then RESTART the kernel\n",
    "    # %pip install sentencepiece\n",
    "    # import sentencepiece as spm # Try importing again after potential install\n",
    "\n",
    "# Check if langdetect is available\n",
    "try:\n",
    "    from langdetect import detect, DetectorFactory\n",
    "    from langdetect.lang_detect_exception import LangDetectException\n",
    "    # Ensure consistent results for langdetect\n",
    "    DetectorFactory.seed = 0\n",
    "    _langdetect_installed = True\n",
    "except ImportError:\n",
    "    print(\"langdetect not found. You might need to install it (`pip install langdetect`)\")\n",
    "    # If in a notebook, uncomment and run the next line, then RESTART the kernel\n",
    "    # %pip install langdetect\n",
    "    _langdetect_installed = False\n",
    "\n",
    "\n",
    "# Check if rouge-score is available (for evaluation later)\n",
    "try:\n",
    "    from rouge_score import rouge_scorer, scoring\n",
    "except ImportError:\n",
    "    print(\"rouge-score not found. You might need to install it (`pip install rouge-score nltk`)\")\n",
    "    # If in a notebook, uncomment and run the next line, then RESTART the kernel\n",
    "    # %pip install rouge-score nltk\n",
    "    # Need to download nltk data if not already present\n",
    "    try:\n",
    "        import nltk\n",
    "        nltk.download('punkt', quiet=True)\n",
    "    except ImportError:\n",
    "        print(\"NLTK not found, which might be needed for rouge-score.\")\n",
    "\n",
    "\n",
    "from tqdm.notebook import tqdm # Use tqdm.notebook for Jupyter/Vertex AI Notebooks\n",
    "\n",
    "# --- Configuration ---\n",
    "# <<< CHANGE: Define Base Output Directory >>>\n",
    "OUTPUT_DIR = 'model3_files'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True) # Create the directory if it doesn't exist\n",
    "print(f\"Ensuring output directory exists: {os.path.abspath(OUTPUT_DIR)}\")\n",
    "\n",
    "# File Paths (now relative to OUTPUT_DIR)\n",
    "INPUT_JSONL = 'mergedt04.jsonl' # Input dataset (Assuming it's in the same dir as notebook or provide full path)\n",
    "OUTPUT_PARQUET = os.path.join(OUTPUT_DIR, 'processed_dataframe.parquet') # Cached processed data\n",
    "TOKENIZER_MODEL_PREFIX = os.path.join(OUTPUT_DIR, 'pib_summarizer_spm_50k') # Prefix for SentencePiece model files\n",
    "TOKENIZER_MODEL_FILE = f'{TOKENIZER_MODEL_PREFIX}.model'\n",
    "LOG_DIR = os.path.join(OUTPUT_DIR, \"logs\", \"fit\", datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "MODEL_SAVE_PATH = os.path.join(OUTPUT_DIR, 'pib_summarizer_no_attention.keras') # Where to save the best model\n",
    "\n",
    "# Data Processing Params\n",
    "MIN_INPUT_WORDS = 20\n",
    "MIN_SUMMARY_WORDS = 5\n",
    "LANG_DETECT_THRESHOLD = 0.90 # Note: Threshold not used in current langdetect implementation, just checking 'en'\n",
    "\n",
    "# Tokenizer Params\n",
    "VOCAB_SIZE = 30000 # Target vocabulary size\n",
    "PAD_ID = 0\n",
    "UNK_ID = 1\n",
    "START_ID = 2\n",
    "END_ID = 3\n",
    "\n",
    "# Model Hyperparameters\n",
    "EMBEDDING_DIM = 100\n",
    "LSTM_UNITS = 256\n",
    "DECODER_LSTM_UNITS = LSTM_UNITS * 2\n",
    "NUM_ENCODER_LAYERS = 2\n",
    "NUM_DECODER_LAYERS = 2\n",
    "DROPOUT_RATE = 0.2\n",
    "MAX_LEN_INPUT = 1024\n",
    "MAX_LEN_SUMMARY = 150\n",
    "\n",
    "# Training Params\n",
    "BATCH_SIZE = 64 # Tune based on GPU memory\n",
    "EPOCHS = 30\n",
    "LEARNING_RATE = 0.001\n",
    "EARLY_STOPPING_PATIENCE = 5\n",
    "REDUCE_LR_PATIENCE = 3\n",
    "REDUCE_LR_FACTOR = 0.2\n",
    "\n",
    "# Inference Params\n",
    "BEAM_WIDTH = 5 # For beam search (if implemented later)\n",
    "\n",
    "# --- Setup Logging ---\n",
    "# <<< CHANGE: Log file within OUTPUT_DIR >>>\n",
    "LOG_FILE_PATH = os.path.join(OUTPUT_DIR, 'training_log.log')\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                    handlers=[\n",
    "                        logging.FileHandler(LOG_FILE_PATH), # Log to file\n",
    "                        logging.StreamHandler() # Also log to console\n",
    "                    ])\n",
    "print(f\"Logging setup complete. Log file: {LOG_FILE_PATH}\")\n",
    "\n",
    "\n",
    "# --- Debugging Info ---\n",
    "print(\"\\n--- Configuration ---\")\n",
    "logging.info(\"--- Configuration ---\")\n",
    "print(f\"TensorFlow Version: {tf.__version__}\"); logging.info(f\"TensorFlow Version: {tf.__version__}\")\n",
    "try:\n",
    "    print(f\"SentencePiece Version: {spm.__version__}\"); logging.info(f\"SentencePiece Version: {spm.__version__}\")\n",
    "except NameError: print(\"SentencePiece not imported.\"); logging.warning(\"SentencePiece not imported.\")\n",
    "\n",
    "if _langdetect_installed:\n",
    "    try:\n",
    "        langdetect_version = importlib.metadata.version(\"langdetect\")\n",
    "        print(f\"Langdetect Version: {langdetect_version}\"); logging.info(f\"Langdetect Version: {langdetect_version}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Langdetect Version: Could not determine version ({e})\"); logging.warning(f\"Langdetect Version: Could not determine version ({e})\")\n",
    "else:\n",
    "     print(\"Langdetect: Not installed or failed to import.\"); logging.warning(\"Langdetect: Not installed or failed to import.\")\n",
    "\n",
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "print(f\"GPU Available: {gpu_devices}\"); logging.info(f\"GPU Available: {gpu_devices}\")\n",
    "print(f\"Input JSONL: {INPUT_JSONL}\"); logging.info(f\"Input JSONL: {INPUT_JSONL}\")\n",
    "print(f\"Output Parquet Cache: {OUTPUT_PARQUET}\"); logging.info(f\"Output Parquet Cache: {OUTPUT_PARQUET}\")\n",
    "print(f\"Tokenizer Model Prefix: {TOKENIZER_MODEL_PREFIX}\"); logging.info(f\"Tokenizer Model Prefix: {TOKENIZER_MODEL_PREFIX}\")\n",
    "print(f\"Model Save Path: {MODEL_SAVE_PATH}\"); logging.info(f\"Model Save Path: {MODEL_SAVE_PATH}\")\n",
    "print(f\"TensorBoard Log Dir: {LOG_DIR}\"); logging.info(f\"TensorBoard Log Dir: {LOG_DIR}\")\n",
    "print(f\"Vocab Size: {VOCAB_SIZE}\"); logging.info(f\"Vocab Size: {VOCAB_SIZE}\")\n",
    "print(f\"Embedding Dim: {EMBEDDING_DIM}\"); logging.info(f\"Embedding Dim: {EMBEDDING_DIM}\")\n",
    "print(f\"Encoder LSTM Units (per direction): {LSTM_UNITS}\"); logging.info(f\"Encoder LSTM Units (per direction): {LSTM_UNITS}\")\n",
    "print(f\"Decoder LSTM Units: {DECODER_LSTM_UNITS}\"); logging.info(f\"Decoder LSTM Units: {DECODER_LSTM_UNITS}\")\n",
    "print(f\"Max Input Length: {MAX_LEN_INPUT}\"); logging.info(f\"Max Input Length: {MAX_LEN_INPUT}\")\n",
    "print(f\"Max Summary Length: {MAX_LEN_SUMMARY}\"); logging.info(f\"Max Summary Length: {MAX_LEN_SUMMARY}\")\n",
    "print(f\"Batch Size: {BATCH_SIZE}\"); logging.info(f\"Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"Epochs: {EPOCHS}\"); logging.info(f\"Epochs: {EPOCHS}\")\n",
    "print(f\"Learning Rate: {LEARNING_RATE}\"); logging.info(f\"Learning Rate: {LEARNING_RATE}\")\n",
    "print(\"-\" * 30); logging.info(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9808e747-80bb-4821-8787-8715f53a2a54",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 15:45:02,311 - INFO - Attempting to load data from: mergedt04.jsonl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cleaning Function Test (Block 2) ---\n",
      "Original:\n",
      "\n",
      "Press Information Bureau\n",
      "Government of India\n",
      "Ministry of Finance\n",
      "Posted on: 25 JUL 2024 6:00PM by PIB Delhi\n",
      "This is a [ 1] test document from file:///path/to/doc.pdf. Check www.example.com.\n",
      "It has “quotes” and ‘apostrophes’.   Extra spaces. And some !?.,'\"- punctuation.\n",
      "Bad chars: #$%^&*()_+={}[]|\\:;<>~/\n",
      "***DS/AK***\n",
      "(Release ID: 12345)\n",
      "\n",
      "\n",
      "Cleaned:\n",
      "25 jul 2024 600pm by this is a test document from . check it has \"quotes\" and 'apostrophes'. extra spaces. and some !?.,'\"- punctuation. bad chars\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 15:45:04,558 - INFO - Loaded 74128 records successfully out of 74128 lines (0 failed/skipped).\n",
      "2025-04-24 15:45:04,594 - INFO - Block 2 completed. `raw_df` created successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Data Loading ---\n",
      "Processed 74128 lines from mergedt04.jsonl.\n",
      "Successfully loaded 74128 records.\n",
      "Skipped/failed 0 lines.\n",
      "Columns: ['pdf_filename', 'extracted_text', 'gemini_summary', 'gemini_topics']\n",
      "Data Types:\n",
      " pdf_filename      object\n",
      "extracted_text    object\n",
      "gemini_summary    object\n",
      "gemini_topics     object\n",
      "dtype: object\n",
      "Sample record (first 5 rows raw_df):\n",
      "                 pdf_filename  \\\n",
      "0  PIB_115363_2015_02_11.pdf   \n",
      "1  PIB_115365_2015_02_11.pdf   \n",
      "2  PIB_115366_2015_02_11.pdf   \n",
      "3  PIB_115367_2015_02_12.pdf   \n",
      "4  PIB_115368_2015_02_12.pdf   \n",
      "\n",
      "                                      extracted_text  \\\n",
      "0  Ministry of Housing & Urban Affairs\\nStates ca...   \n",
      "1  Ministry of Culture\\nConsultative Committee of...   \n",
      "2  Ministry of Tourism\\nConsultative Committee of...   \n",
      "3  Prime Minister's Office\\nPM pays tributes to S...   \n",
      "4  Prime Minister's Office\\nPM appalled at the ne...   \n",
      "\n",
      "                                      gemini_summary  \\\n",
      "0  A meeting of the Parliamentary Consultative Co...   \n",
      "1  A meeting of the Parliamentary Consultative Co...   \n",
      "2  A meeting of the Parliamentary Consultative Co...   \n",
      "3  Prime Minister Narendra Modi paid tributes to ...   \n",
      "4  Prime Minister Narendra Modi expressed his str...   \n",
      "\n",
      "                                       gemini_topics  \n",
      "0                                               None  \n",
      "1  [{'main_topic': 'Parliamentary Consultative Co...  \n",
      "2  [{'main_topic': 'Meeting of Parliamentary Cons...  \n",
      "3  [{'main_topic': 'Swami Dayananda Saraswati'}, ...  \n",
      "4  [{'main_topic': 'Prime Minister Narendra Modi'...  \n",
      "------------------------------\n",
      "Block 2 completed. `raw_df` created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Block 2: Data Loading and Initial Cleaning Functions + Execution\n",
    "\n",
    "# --- Function Definitions ---\n",
    "def load_data(jsonl_path):\n",
    "    \"\"\"Loads data from a JSONL file.\"\"\"\n",
    "    logging.info(f\"Attempting to load data from: {jsonl_path}\")\n",
    "    data = []\n",
    "    lines_processed = 0\n",
    "    lines_failed = 0\n",
    "    if not os.path.exists(jsonl_path):\n",
    "        logging.error(f\"Input file not found: {jsonl_path}\")\n",
    "        print(f\"\\n--- Data Loading Error ---\")\n",
    "        print(f\"Error: Input file not found at {jsonl_path}\")\n",
    "        print(\"Please ensure the file exists and the path is correct.\")\n",
    "        print(\"-\" * 30)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    try:\n",
    "        with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                lines_processed += 1\n",
    "                try:\n",
    "                    # Skip empty lines\n",
    "                    if not line.strip():\n",
    "                        logging.warning(f\"Skipping empty line: {i+1}\")\n",
    "                        lines_failed +=1\n",
    "                        continue\n",
    "                    data.append(json.loads(line))\n",
    "                except json.JSONDecodeError as e:\n",
    "                    logging.warning(f\"Skipping malformed JSON line: {i+1}. Error: {e}\")\n",
    "                    lines_failed += 1\n",
    "                    continue\n",
    "        df = pd.DataFrame(data)\n",
    "        logging.info(f\"Loaded {len(df)} records successfully out of {lines_processed} lines ({lines_failed} failed/skipped).\")\n",
    "        # --- Debugging Info ---\n",
    "        print(\"\\n--- Data Loading ---\")\n",
    "        print(f\"Processed {lines_processed} lines from {jsonl_path}.\")\n",
    "        print(f\"Successfully loaded {len(df)} records.\")\n",
    "        print(f\"Skipped/failed {lines_failed} lines.\")\n",
    "        if not df.empty:\n",
    "            print(\"Columns:\", df.columns.tolist())\n",
    "            print(\"Data Types:\\n\", df.dtypes)\n",
    "            print(\"Sample record (first 5 rows raw_df):\\n\", df.head())\n",
    "        else:\n",
    "            print(\"Loaded DataFrame (raw_df) is empty. Check input file content and format.\")\n",
    "            logging.warning(\"Loaded DataFrame (raw_df) is empty after processing the file.\")\n",
    "        print(\"-\" * 30)\n",
    "        return df\n",
    "    except FileNotFoundError: # This case is already handled above, but keep for robustness\n",
    "        logging.error(f\"Error: Input file not found at {jsonl_path}\")\n",
    "        print(f\"\\n--- Data Loading Error ---\")\n",
    "        print(f\"Error: Input file not found at {jsonl_path}\")\n",
    "        print(\"-\" * 30)\n",
    "        return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred during data loading: {e}\", exc_info=True)\n",
    "        print(f\"\\n--- Data Loading Error ---\")\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        print(\"-\" * 30)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Applies cleaning steps to a single text string.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    text = text.lower()\n",
    "    text = text.replace('“', '\"').replace('”', '\"').replace(\"‘\", \"'\").replace(\"’\", \"'\")\n",
    "    text = re.sub(r'file:///[^ ]+\\.pdf', '', text)\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'\\[\\s*\\d+\\s*\\]', '', text)\n",
    "    headers_footers = [\n",
    "        r\"press information bureau\", r\"government of india\", r\"ministry of [\\w\\s]+\",\n",
    "        # Make date pattern more robust (optional day, month formats, etc.)\n",
    "        r\"posted on:\\s*\\d{1,2}\\s+\\w{3,}\\s+\\d{4}\\s+\\d{1,2}:\\d{2}\\s*[ap]m\\s*(by pib \\w+)?\",\n",
    "        r\"release id: \\d+\",\n",
    "        r\"\\(release id.*?\\)\",\n",
    "        r\"pib \\w+\", # Keep this general\n",
    "        r\"\\*{3,}\\s*[a-z\\/]+\\s*\\*{3,}\", # More general footer pattern\n",
    "    ]\n",
    "    for pattern in headers_footers:\n",
    "        text = re.sub(pattern, '', text, flags=re.IGNORECASE)\n",
    "\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r\"[^a-z0-9\\s.,!?'\\\"-]\", \"\", text) # Keep allowed chars\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "# --- Execution for Block 2 ---\n",
    "\n",
    "# --- Debugging: Test cleaning function ---\n",
    "print(\"\\n--- Cleaning Function Test (Block 2) ---\")\n",
    "test_text = \"\"\"\n",
    "Press Information Bureau\\nGovernment of India\\nMinistry of Finance\\nPosted on: 25 JUL 2024 6:00PM by PIB Delhi\n",
    "This is a [ 1] test document from file:///path/to/doc.pdf. Check www.example.com.\n",
    "It has “quotes” and ‘apostrophes’.   Extra spaces. And some !?.,'\"- punctuation.\n",
    "Bad chars: #$%^&*()_+={}[]|\\\\:;<>~/\n",
    "***DS/AK***\n",
    "(Release ID: 12345)\n",
    "\"\"\"\n",
    "cleaned_test = clean_text(test_text)\n",
    "print(f\"Original:\\n{test_text}\")\n",
    "print(f\"\\nCleaned:\\n{cleaned_test}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Load the raw data into a DataFrame called 'raw_df'\n",
    "raw_df = load_data(INPUT_JSONL)\n",
    "\n",
    "# <<< CRUCIAL CHECK >>>\n",
    "if 'raw_df' not in locals() or raw_df.empty:\n",
    "    print(\"************************************************************\")\n",
    "    print(\"ERROR: Block 2 failed to load data into `raw_df`.\")\n",
    "    print(\"Cannot proceed to Block 3. Please check the 'Data Loading' output above.\")\n",
    "    print(\"Verify the INPUT_JSONL path and the file content.\")\n",
    "    print(\"************************************************************\")\n",
    "    # Optional: Stop execution here in a notebook context if desired\n",
    "    # raise RuntimeError(\"Failed to load raw data. Stopping execution.\")\n",
    "else:\n",
    "    print(\"Block 2 completed. `raw_df` created successfully.\")\n",
    "    logging.info(\"Block 2 completed. `raw_df` created successfully.\")\n",
    "\n",
    "# A variable 'processed_df' will be created in the next block IF raw_df is valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9486ed78-f741-4f87-bbd7-6703fca1f983",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 15:45:11,161 - INFO - Preprocessing started. Cache path: model3_files/processed_dataframe.parquet\n",
      "2025-04-24 15:45:11,163 - INFO - Loading processed data from cache: model3_files/processed_dataframe.parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceeding with preprocessing using `raw_df`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 15:45:12,746 - INFO - Successfully loaded 73568 records from cache.\n",
      "2025-04-24 15:45:12,749 - INFO - Block 3 completed. `processed_df` created or loaded successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preprocessing Pipeline (Block 3) ---\n",
      "Loaded 73568 records from cache: model3_files/processed_dataframe.parquet\n",
      "Columns: ['cleaned_text', 'target_summary']\n",
      "Sample processed data (from cache):\n",
      "                                         cleaned_text  \\\n",
      "0  urban affairs states can flexibly use of centr...   \n",
      "1  dr mahesh sharma the parliamentary consultativ...   \n",
      "2  dr mahesh sharma the parliamentary consultativ...   \n",
      "3  prime minister's office pm pays tributes to sw...   \n",
      "4  prime minister's office pm appalled at the new...   \n",
      "\n",
      "                                      target_summary  \n",
      "0  <start> a meeting of the parliamentary consult...  \n",
      "1  <start> a meeting of the parliamentary consult...  \n",
      "2  <start> a meeting of the parliamentary consult...  \n",
      "3  <start> prime minister narendra modi paid trib...  \n",
      "4  <start> prime minister narendra modi expressed...  \n",
      "------------------------------\n",
      "Block 3 completed. `processed_df` created or loaded successfully.\n",
      "Cleaning up raw_df from memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 15:45:13,005 - INFO - Cleaned up raw_df from memory.\n"
     ]
    }
   ],
   "source": [
    "# Block 3: Preprocessing Pipeline + Execution\n",
    "\n",
    "# --- Function Definition ---\n",
    "def preprocess_data(df, text_col='extracted_text', summary_col='gemini_summary', cache_path=OUTPUT_PARQUET):\n",
    "    \"\"\"Applies the full preprocessing pipeline to the dataframe.\"\"\"\n",
    "    logging.info(f\"Preprocessing started. Cache path: {cache_path}\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # --- Cache Check ---\n",
    "    if os.path.exists(cache_path):\n",
    "        try:\n",
    "            logging.info(f\"Loading processed data from cache: {cache_path}\")\n",
    "            processed_df_from_cache = pd.read_parquet(cache_path)\n",
    "            logging.info(f\"Successfully loaded {len(processed_df_from_cache)} records from cache.\")\n",
    "            # --- Debugging Info ---\n",
    "            print(\"\\n--- Preprocessing Pipeline (Block 3) ---\")\n",
    "            print(f\"Loaded {len(processed_df_from_cache)} records from cache: {cache_path}\")\n",
    "            if not processed_df_from_cache.empty:\n",
    "                print(\"Columns:\", processed_df_from_cache.columns.tolist())\n",
    "                print(\"Sample processed data (from cache):\\n\", processed_df_from_cache.head())\n",
    "            else:\n",
    "                print(\"Warning: Cache file loaded an empty DataFrame.\")\n",
    "                logging.warning(f\"Cache file {cache_path} contained an empty DataFrame.\")\n",
    "            print(\"-\" * 30)\n",
    "            return processed_df_from_cache # Return cached data\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to load or parse cache file {cache_path}: {e}\", exc_info=True)\n",
    "            print(f\"\\n--- Preprocessing Cache Error ---\")\n",
    "            print(f\"Error reading cache file {cache_path}: {e}. Proceeding with reprocessing.\")\n",
    "            # If cache fails, delete it to force reprocessing next time? Optional.\n",
    "            # try: os.remove(cache_path) except OSError: pass\n",
    "\n",
    "\n",
    "    # --- Preprocessing Steps (if cache doesn't exist or failed) ---\n",
    "    print(\"\\n--- Preprocessing Pipeline (Block 3) ---\")\n",
    "    print(f\"Cache not used or failed. Starting processing of {len(df)} raw records...\")\n",
    "    logging.info(f\"Cache not used or failed. Starting processing of {len(df)} raw records...\")\n",
    "\n",
    "\n",
    "    # Ensure required columns exist in the input df\n",
    "    if text_col not in df.columns or summary_col not in df.columns:\n",
    "        logging.error(f\"Input DataFrame missing required columns: '{text_col}' or '{summary_col}'. Available: {df.columns.tolist()}\")\n",
    "        print(\"\\n--- Preprocessing Pipeline Error ---\")\n",
    "        print(f\"Input DataFrame missing required columns: '{text_col}' or '{summary_col}'. Available: {df.columns.tolist()}\")\n",
    "        print(\"Cannot preprocess.\")\n",
    "        print(\"-\" * 30)\n",
    "        return pd.DataFrame() # Return empty dataframe\n",
    "\n",
    "    # Make a copy to avoid SettingWithCopyWarning on the original raw_df\n",
    "    df_processed = df.copy()\n",
    "\n",
    "    logging.info(\"Applying text cleaning...\")\n",
    "    tqdm.pandas(desc=\"Cleaning Text\")\n",
    "    df_processed['cleaned_text'] = df_processed[text_col].progress_apply(clean_text)\n",
    "    tqdm.pandas(desc=\"Cleaning Summary\")\n",
    "    df_processed['cleaned_summary'] = df_processed[summary_col].progress_apply(clean_text)\n",
    "\n",
    "    initial_count = len(df_processed)\n",
    "    logging.info(f\"Initial record count for processing: {initial_count}\")\n",
    "    print(f\"Initial record count for processing: {initial_count}\")\n",
    "    print(\"Sample after basic cleaning:\")\n",
    "    cols_to_display = [c for c in ['cleaned_text', 'cleaned_summary'] if c in df_processed.columns]\n",
    "    if cols_to_display and not df_processed.empty:\n",
    "        print(df_processed[cols_to_display].head())\n",
    "\n",
    "\n",
    "    # Language Filtering\n",
    "    if _langdetect_installed: # Only run if library is available\n",
    "        logging.info(\"Applying language filtering...\")\n",
    "        valid_indices = []\n",
    "        skipped_lang = 0\n",
    "        for index, text in tqdm(df_processed['cleaned_text'].items(), total=len(df_processed), desc=\"Language Filtering\"):\n",
    "            try:\n",
    "                if not text or len(text.split()) < 5:\n",
    "                    skipped_lang += 1\n",
    "                    continue\n",
    "                # Use detect_langs for probability, though simple detect('en') is faster if threshold isn't strict\n",
    "                lang = detect(text[:500]) # Detect on first 500 chars\n",
    "                if lang == 'en': # Simple check for English\n",
    "                    valid_indices.append(index)\n",
    "                else:\n",
    "                    skipped_lang += 1\n",
    "            except LangDetectException:\n",
    "                skipped_lang += 1\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Language detection error on index {index}: {e}\")\n",
    "                skipped_lang += 1\n",
    "\n",
    "        df_processed = df_processed.loc[valid_indices].copy()\n",
    "        lang_filtered_count = len(df_processed)\n",
    "        logging.info(f\"Language filtering: Kept {lang_filtered_count}, Removed {initial_count - lang_filtered_count} non-English/error records.\")\n",
    "        print(f\"Count after language filtering: {lang_filtered_count} ({initial_count - lang_filtered_count} removed)\")\n",
    "    else:\n",
    "        print(\"Skipping language filtering as langdetect is not available.\")\n",
    "        logging.warning(\"Skipping language filtering as langdetect is not available.\")\n",
    "\n",
    "\n",
    "    # Length Filtering (only if records remain)\n",
    "    if not df_processed.empty:\n",
    "        logging.info(\"Applying length filtering...\")\n",
    "        df_processed['text_word_count'] = df_processed['cleaned_text'].apply(lambda x: len(x.split()))\n",
    "        df_processed['summary_word_count'] = df_processed['cleaned_summary'].apply(lambda x: len(x.split()))\n",
    "\n",
    "        original_count_before_len_filter = len(df_processed)\n",
    "        df_processed = df_processed[df_processed['text_word_count'] >= MIN_INPUT_WORDS].copy()\n",
    "        df_processed = df_processed[df_processed['summary_word_count'] >= MIN_SUMMARY_WORDS].copy()\n",
    "        len_filtered_count = len(df_processed)\n",
    "        logging.info(f\"Length filtering: Kept {len_filtered_count}, Removed {original_count_before_len_filter - len_filtered_count} short records.\")\n",
    "        print(f\"Count after length filtering: {len_filtered_count} ({original_count_before_len_filter - len_filtered_count} removed)\")\n",
    "    else:\n",
    "        print(\"Skipping length filtering as DataFrame is empty after previous steps.\")\n",
    "        logging.warning(\"Skipping length filtering due to empty DataFrame.\")\n",
    "\n",
    "\n",
    "    # Add Start/End Tokens (only if records remain)\n",
    "    final_processed_df = pd.DataFrame() # Initialize\n",
    "    if not df_processed.empty:\n",
    "        logging.info(\"Adding <start> and <end> tokens to summaries...\")\n",
    "        df_processed['cleaned_summary_tagged'] = df_processed['cleaned_summary'].apply(lambda x: f\"<start> {x} <end>\")\n",
    "        # Final Selection and Renaming\n",
    "        final_processed_df = df_processed[['cleaned_text', 'cleaned_summary_tagged']].rename(columns={'cleaned_summary_tagged': 'target_summary'})\n",
    "    else:\n",
    "        print(\"Skipping token tagging as DataFrame is empty.\")\n",
    "        logging.warning(\"Skipping token tagging due to empty DataFrame.\")\n",
    "\n",
    "\n",
    "    # --- Final Output and Caching ---\n",
    "    print(f\"Final processed record count: {len(final_processed_df)}\")\n",
    "    logging.info(f\"Final processed record count: {len(final_processed_df)}\")\n",
    "    if not final_processed_df.empty:\n",
    "        print(\"Sample processed data (final):\\n\", final_processed_df.head())\n",
    "        # Cache the result\n",
    "        try:\n",
    "            logging.info(f\"Caching processed data to: {cache_path}\")\n",
    "            final_processed_df.to_parquet(cache_path, index=False)\n",
    "            logging.info(\"Caching successful.\")\n",
    "            print(f\"Processed data cached successfully to {cache_path}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to cache processed data to {cache_path}: {e}\", exc_info=True)\n",
    "            print(f\"\\n--- Caching Error ---\")\n",
    "            print(f\"Failed to cache data to {cache_path}: {e}\")\n",
    "    else:\n",
    "        logging.warning(\"Processed DataFrame is empty. No data will be cached.\")\n",
    "        print(\"Warning: Processed DataFrame is empty. Nothing to cache.\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    logging.info(f\"Preprocessing finished in {end_time - start_time:.2f} seconds.\")\n",
    "    print(f\"Preprocessing finished in {end_time - start_time:.2f} seconds.\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    return final_processed_df\n",
    "\n",
    "# --- Execution for Block 3 ---\n",
    "processed_df = pd.DataFrame() # Initialize as empty DataFrame\n",
    "\n",
    "# Only proceed if raw_df from Block 2 exists and is not empty\n",
    "if 'raw_df' in locals() and isinstance(raw_df, pd.DataFrame) and not raw_df.empty:\n",
    "    print(\"Proceeding with preprocessing using `raw_df`...\")\n",
    "    processed_df = preprocess_data(raw_df, cache_path=OUTPUT_PARQUET)\n",
    "else:\n",
    "    print(\"Skipping Block 3 execution because `raw_df` is not available or is empty.\")\n",
    "    print(\"Check the output of Block 2 for errors.\")\n",
    "    logging.error(\"Skipping Block 3 execution because `raw_df` is not available or is empty.\")\n",
    "\n",
    "# <<< CRUCIAL CHECK >>>\n",
    "if 'processed_df' not in locals() or processed_df.empty:\n",
    "    print(\"************************************************************\")\n",
    "    print(\"WARNING: Block 3 resulted in an empty `processed_df`.\")\n",
    "    print(\"This could be due to loading errors, aggressive filtering, or issues during processing.\")\n",
    "    print(\"Subsequent blocks (Tokenizer, Model Training) will likely fail or be skipped.\")\n",
    "    print(\"Review the 'Preprocessing Pipeline' output above.\")\n",
    "    print(\"************************************************************\")\n",
    "    logging.warning(\"Block 3 resulted in an empty `processed_df`.\")\n",
    "else:\n",
    "    print(\"Block 3 completed. `processed_df` created or loaded successfully.\")\n",
    "    logging.info(\"Block 3 completed. `processed_df` created or loaded successfully.\")\n",
    "\n",
    "# Clean up raw_df if memory is a concern and processing was successful (optional)\n",
    "if 'processed_df' in locals() and not processed_df.empty and 'raw_df' in locals():\n",
    "   print(\"Cleaning up raw_df from memory...\")\n",
    "   del raw_df\n",
    "   gc.collect()\n",
    "   logging.info(\"Cleaned up raw_df from memory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3131fbcb-5376-429f-b7cb-4145085013bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 15:45:19,034 - INFO - Block 4: `processed_df` is valid. Proceeding with tokenizer.\n",
      "2025-04-24 15:45:19,060 - INFO - Found existing tokenizer model: model3_files/pib_summarizer_spm_50k.model. Skipping training.\n",
      "2025-04-24 15:45:19,061 - INFO - Loading SentencePiece tokenizer from: model3_files/pib_summarizer_spm_50k.model\n",
      "2025-04-24 15:45:19,158 - INFO - Successfully loaded tokenizer: model3_files/pib_summarizer_spm_50k.model\n",
      "2025-04-24 15:45:19,159 - INFO - Tokenizing 73568 texts with max_len=1024...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block 4: `processed_df` is valid. Proceeding with tokenizer.\n",
      "Preparing data for tokenizer training...\n",
      "Full corpus size for tokenizer: 147136\n",
      "Tokenizer model model3_files/pib_summarizer_spm_50k.model already exists. Skipping training.\n",
      "\n",
      "--- Tokenizer Loading ---\n",
      "Successfully loaded tokenizer: model3_files/pib_summarizer_spm_50k.model\n",
      "Vocabulary Size: 30000\n",
      "PAD ID (<pad>): 0\n",
      "UNK ID (<unk>): 1\n",
      "BOS/Start ID (<start>): 2\n",
      "EOS/End ID (<end>): 3\n",
      "------------------------------\n",
      "Tokenizing cleaned text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 15:45:38,959 - INFO - Tokenization successful for 73568 texts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Tokenization ---\n",
      "Tokenized 73568 texts.\n",
      "Shape of padded sequences: (73568, 1024)\n",
      "Original Text (sample 0): urban affairs states can flexibly use of central assistance of rs.4,000 per toilet, says shri venkai...\n",
      "Tokenized IDs (sample 0): [  253   188    82   203 18188   606   320   289     7    76   235     7\n",
      "    43     6  8390    80  4236     5  2429    31]...\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 15:45:39,328 - INFO - Tokenizing 73568 texts with max_len=150...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing target summaries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 15:45:44,221 - INFO - Tokenization successful for 73568 texts.\n",
      "2025-04-24 15:45:44,314 - INFO - Tokenization successful. Shapes: Encoder=(73568, 1024), DecoderIn=(73568, 149), DecoderOut=(73568, 149)\n",
      "2025-04-24 15:45:44,316 - INFO - Block 4 completed successfully. Tokenized data created.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Tokenization ---\n",
      "Tokenized 73568 texts.\n",
      "Shape of padded sequences: (73568, 150)\n",
      "Original Text (sample 0): <start> a meeting of the parliamentary consultative committee discussed shortcomings in the jawaharl...\n",
      "Tokenized IDs (sample 0): [   11     1  8959     1    13    88     7     4  1239  2583   135   569\n",
      " 10151    10     4  2925  2058   253   629   184]...\n",
      "------------------------------\n",
      "Creating decoder input/target sequences...\n",
      "\n",
      "--- Data Shapes After Tokenization & Shifting ---\n",
      "Encoder Input Shape: (73568, 1024)\n",
      "Decoder Input Shape: (73568, 149)\n",
      "Decoder Target Shape: (73568, 149)\n",
      "------------------------------\n",
      "Block 4 completed successfully. Tokenized data created.\n"
     ]
    }
   ],
   "source": [
    "# Block 4: Tokenizer Training and Usage + Execution\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tqdm.notebook import tqdm\n",
    "import sentencepiece as spm\n",
    "\n",
    "# --- Function Definitions ---\n",
    "\n",
    "def train_sentencepiece(data_series, model_prefix, vocab_size, special_tokens):\n",
    "    \"\"\"Trains a SentencePiece Unigram model.\"\"\"\n",
    "    logging.info(f\"Starting SentencePiece training. Output prefix: {model_prefix}\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Create a temporary file to store the text data for training\n",
    "    # Ensure the directory for the temp file exists (it should be OUTPUT_DIR)\n",
    "    temp_dir = os.path.dirname(model_prefix)\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "    temp_text_file = f\"{model_prefix}_training_data.txt\" # Use prefix for temp file name\n",
    "\n",
    "    try:\n",
    "        if data_series.empty:\n",
    "            logging.error(\"Cannot train SentencePiece on empty data series.\")\n",
    "            print(\"\\n--- SentencePiece Training Error ---\")\n",
    "            print(\"Input data series is empty. Cannot train tokenizer.\")\n",
    "            print(\"-\" * 30)\n",
    "            return False # Indicate failure\n",
    "\n",
    "        with open(temp_text_file, 'w', encoding='utf-8') as f:\n",
    "            for text in tqdm(data_series, desc=\"Writing Training Data\"):\n",
    "                f.write(str(text) + '\\n') # Ensure text is string\n",
    "        logging.info(f\"Training data written to {temp_text_file}\")\n",
    "\n",
    "        spm_command = (\n",
    "            f'--input={temp_text_file} --model_prefix={model_prefix} '\n",
    "            f'--vocab_size={vocab_size} --model_type=unigram '\n",
    "            f'--pad_id={PAD_ID} --unk_id={UNK_ID} '\n",
    "            f'--bos_id={START_ID} --eos_id={END_ID} ' # bos = <start>, eos = <end>\n",
    "            f'--unk_piece=<unk> --bos_piece=<start> --eos_piece=<end> --pad_piece=<pad> '\n",
    "            f'--hard_vocab_limit=false '\n",
    "            f'--character_coverage=1.0 '\n",
    "            f'--shuffle_input_sentence=true --input_sentence_size=10000000' # Sample size\n",
    "        )\n",
    "\n",
    "        print(\"\\n--- SentencePiece Training ---\")\n",
    "        logging.info(f\"Running SentencePiece with command args...\")\n",
    "        print(f\"Running SentencePiece with command args (simplified): {spm_command.split('--input')[0]}...\")\n",
    "\n",
    "        spm.SentencePieceTrainer.train(spm_command)\n",
    "\n",
    "        training_duration = time.time() - start_time\n",
    "        logging.info(f\"SentencePiece training completed in {training_duration:.2f} seconds.\")\n",
    "        print(f\"SentencePiece model files created: {model_prefix}.model, {model_prefix}.vocab\")\n",
    "        print(f\"Training duration: {training_duration:.2f} seconds.\")\n",
    "\n",
    "        os.remove(temp_text_file)\n",
    "        logging.info(f\"Removed temporary training file: {temp_text_file}\")\n",
    "        print(\"-\" * 30)\n",
    "        return True # Indicate success\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"SentencePiece training failed: {e}\", exc_info=True)\n",
    "        print(f\"\\n--- SentencePiece Training Error ---\")\n",
    "        print(f\"SentencePiece training failed: {e}\")\n",
    "        if os.path.exists(f\"{model_prefix}.model\"): os.remove(f\"{model_prefix}.model\")\n",
    "        if os.path.exists(f\"{model_prefix}.vocab\"): os.remove(f\"{model_prefix}.vocab\")\n",
    "        if os.path.exists(temp_text_file): os.remove(temp_text_file)\n",
    "        print(\"-\" * 30)\n",
    "        return False # Indicate failure\n",
    "\n",
    "\n",
    "# <<< CORRECTED VERSION of load_tokenizer >>>\n",
    "def load_tokenizer(model_path):\n",
    "    \"\"\"Loads a trained SentencePiece model.\"\"\"\n",
    "    logging.info(f\"Loading SentencePiece tokenizer from: {model_path}\")\n",
    "    if not os.path.exists(model_path):\n",
    "        logging.error(f\"Tokenizer model file not found at {model_path}\")\n",
    "        print(f\"\\n--- Tokenizer Loading Error ---\")\n",
    "        print(f\"Error: Tokenizer model file not found at {model_path}\")\n",
    "        print(\"-\" * 30)\n",
    "        return None\n",
    "    try:\n",
    "        tokenizer = spm.SentencePieceProcessor()\n",
    "        tokenizer.load(model_path)\n",
    "        print(\"\\n--- Tokenizer Loading ---\")\n",
    "        logging.info(f\"Successfully loaded tokenizer: {model_path}\")\n",
    "        print(f\"Successfully loaded tokenizer: {model_path}\")\n",
    "\n",
    "        # <<< FIX: Use id_to_piece() for string representation >>>\n",
    "        pad_id_val = tokenizer.pad_id()\n",
    "        unk_id_val = tokenizer.unk_id()\n",
    "        bos_id_val = tokenizer.bos_id()\n",
    "        eos_id_val = tokenizer.eos_id()\n",
    "\n",
    "        # Handle cases where special IDs might not be set (though unlikely with our training flags)\n",
    "        pad_piece_str = tokenizer.id_to_piece(pad_id_val) if pad_id_val is not None and pad_id_val >= 0 else 'N/A'\n",
    "        unk_piece_str = tokenizer.id_to_piece(unk_id_val) if unk_id_val is not None and unk_id_val >= 0 else 'N/A'\n",
    "        bos_piece_str = tokenizer.id_to_piece(bos_id_val) if bos_id_val is not None and bos_id_val >= 0 else 'N/A'\n",
    "        eos_piece_str = tokenizer.id_to_piece(eos_id_val) if eos_id_val is not None and eos_id_val >= 0 else 'N/A'\n",
    "\n",
    "        print(f\"Vocabulary Size: {tokenizer.vocab_size()}\")\n",
    "        print(f\"PAD ID ({pad_piece_str}): {pad_id_val}\")\n",
    "        print(f\"UNK ID ({unk_piece_str}): {unk_id_val}\")\n",
    "        print(f\"BOS/Start ID ({bos_piece_str}): {bos_id_val}\")\n",
    "        print(f\"EOS/End ID ({eos_piece_str}): {eos_id_val}\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "        # Sanity check special token IDs (remains the same)\n",
    "        if not (pad_id_val == PAD_ID and unk_id_val == UNK_ID and \\\n",
    "                bos_id_val == START_ID and eos_id_val == END_ID):\n",
    "             logging.warning(\"Loaded tokenizer special token IDs DO NOT match configured IDs!\")\n",
    "             print(\"WARNING: Loaded tokenizer special token IDs DO NOT match configured IDs!\")\n",
    "             print(f\"  Loaded: PAD={pad_id_val}, UNK={unk_id_val}, BOS={bos_id_val}, EOS={eos_id_val}\")\n",
    "             print(f\"  Config: PAD={PAD_ID}, UNK={UNK_ID}, BOS={START_ID}, EOS={END_ID}\")\n",
    "             # Optional: Treat as error? For now, just warn.\n",
    "             # return None # Uncomment to treat ID mismatch as a fatal error\n",
    "\n",
    "        return tokenizer\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to load or process tokenizer model from {model_path}: {e}\", exc_info=True)\n",
    "        print(f\"\\n--- Tokenizer Loading Error ---\")\n",
    "        print(f\"Failed to load or process tokenizer model from {model_path}: {e}\") # Show the specific error\n",
    "        print(\"-\" * 30)\n",
    "        return None\n",
    "\n",
    "\n",
    "def tokenize_texts(texts, tokenizer, max_len):\n",
    "    \"\"\"Tokenizes a list/series of texts and pads/truncates them.\"\"\"\n",
    "    if texts is None or texts.empty:\n",
    "        logging.warning(\"Attempted to tokenize an empty list/series of texts.\")\n",
    "        print(\"Warning: Input texts for tokenization is empty.\")\n",
    "        return np.array([]) # Return empty numpy array\n",
    "    if tokenizer is None:\n",
    "        logging.error(\"Cannot tokenize texts: Tokenizer is None.\")\n",
    "        return np.array([])\n",
    "\n",
    "    logging.info(f\"Tokenizing {len(texts)} texts with max_len={max_len}...\")\n",
    "    try:\n",
    "        texts_list = [str(text) if pd.notna(text) else '' for text in texts.tolist()]\n",
    "        tokenized_sequences = tokenizer.encode(texts_list)\n",
    "\n",
    "        padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "            tokenized_sequences,\n",
    "            maxlen=max_len,\n",
    "            padding='post',\n",
    "            truncating='post',\n",
    "            value=PAD_ID # Explicitly use PAD_ID\n",
    "        )\n",
    "        print(\"\\n--- Tokenization ---\")\n",
    "        logging.info(f\"Tokenization successful for {len(texts_list)} texts.\")\n",
    "        print(f\"Tokenized {len(texts_list)} texts.\")\n",
    "        print(f\"Shape of padded sequences: {padded_sequences.shape}\")\n",
    "        if len(texts_list) > 0 and len(padded_sequences) > 0:\n",
    "            print(f\"Original Text (sample 0): {texts_list[0][:100]}...\")\n",
    "            print(f\"Tokenized IDs (sample 0): {padded_sequences[0][:20]}...\")\n",
    "        print(\"-\" * 30)\n",
    "        return padded_sequences\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during text tokenization: {e}\", exc_info=True)\n",
    "        print(f\"\\n--- Tokenization Error ---\")\n",
    "        print(f\"An error occurred during tokenization: {e}\")\n",
    "        print(\"-\" * 30)\n",
    "        return np.array([])\n",
    "\n",
    "\n",
    "def detokenize_sequences(sequences, tokenizer):\n",
    "    \"\"\"Converts sequences of token IDs back to text. Handles single sequence or batch.\"\"\"\n",
    "    if tokenizer is None:\n",
    "        logging.error(\"Detokenization failed: Tokenizer is None.\")\n",
    "        return \"[Detokenization Error: No Tokenizer]\"\n",
    "    if sequences is None: return [] if isinstance(sequences, list) else \"\"\n",
    "\n",
    "    try:\n",
    "        if isinstance(sequences, tf.Tensor): sequences = sequences.numpy()\n",
    "        if isinstance(sequences, np.ndarray): sequences = sequences.tolist()\n",
    "\n",
    "        is_batch = isinstance(sequences, list) and (len(sequences) == 0 or isinstance(sequences[0], list))\n",
    "        if not is_batch: sequences = [sequences] # Wrap single list for uniform processing\n",
    "\n",
    "        texts = []\n",
    "        for seq in sequences:\n",
    "            actual_tokens = []\n",
    "            # Ensure seq is iterable (list/tuple)\n",
    "            if not hasattr(seq, '__iter__'): continue\n",
    "            for token_id_float in seq:\n",
    "                token_id = int(token_id_float)\n",
    "                if token_id == END_ID: break\n",
    "                if token_id != PAD_ID and token_id != START_ID:\n",
    "                     actual_tokens.append(token_id)\n",
    "            texts.append(tokenizer.decode(actual_tokens))\n",
    "\n",
    "        return texts if is_batch else texts[0]\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during detokenization: {e}\", exc_info=True)\n",
    "        return \"[Detokenization Error]\"\n",
    "\n",
    "\n",
    "# --- Execution for Block 4 ---\n",
    "tokenizer = None\n",
    "encoder_input_data = np.array([])\n",
    "decoder_input_data = np.array([])\n",
    "decoder_target_data = np.array([])\n",
    "tokenization_failed = False # Flag to track status\n",
    "\n",
    "# Only proceed if processed_df from Block 3 exists and is not empty\n",
    "if 'processed_df' in locals() and isinstance(processed_df, pd.DataFrame) and not processed_df.empty:\n",
    "    print(\"Block 4: `processed_df` is valid. Proceeding with tokenizer.\")\n",
    "    logging.info(\"Block 4: `processed_df` is valid. Proceeding with tokenizer.\")\n",
    "\n",
    "    # Combine text and summary for tokenizer training data\n",
    "    if 'cleaned_text' in processed_df.columns and 'target_summary' in processed_df.columns:\n",
    "        print(\"Preparing data for tokenizer training...\")\n",
    "        full_corpus = pd.concat([processed_df['cleaned_text'], processed_df['target_summary']], ignore_index=True)\n",
    "        full_corpus.dropna(inplace=True)\n",
    "        print(f\"Full corpus size for tokenizer: {len(full_corpus)}\")\n",
    "\n",
    "        special_tokens = ['<pad>', '<unk>', '<start>', '<end>'] # For reference\n",
    "\n",
    "        # Train only if model file doesn't exist\n",
    "        if not os.path.exists(TOKENIZER_MODEL_FILE):\n",
    "            print(f\"Tokenizer model {TOKENIZER_MODEL_FILE} not found. Starting training...\")\n",
    "            training_successful = train_sentencepiece(full_corpus, TOKENIZER_MODEL_PREFIX, VOCAB_SIZE, special_tokens)\n",
    "            if not training_successful:\n",
    "                 print(\"Tokenizer training failed. Cannot proceed with tokenization.\")\n",
    "                 logging.error(\"Tokenizer training failed.\")\n",
    "                 tokenization_failed = True # Set failure flag\n",
    "            else:\n",
    "                 print(\"Tokenizer training successful.\")\n",
    "                 logging.info(\"Tokenizer training successful.\")\n",
    "        else:\n",
    "            print(f\"Tokenizer model {TOKENIZER_MODEL_FILE} already exists. Skipping training.\")\n",
    "            logging.info(f\"Found existing tokenizer model: {TOKENIZER_MODEL_FILE}. Skipping training.\")\n",
    "\n",
    "        # Load the tokenizer (only if training didn't fail)\n",
    "        if not tokenization_failed:\n",
    "            tokenizer = load_tokenizer(TOKENIZER_MODEL_FILE) # Use the corrected function\n",
    "\n",
    "            if tokenizer:\n",
    "                print(\"Tokenizing cleaned text...\")\n",
    "                encoder_input_data = tokenize_texts(processed_df['cleaned_text'], tokenizer, MAX_LEN_INPUT)\n",
    "                print(\"Tokenizing target summaries...\")\n",
    "                decoder_full_data = tokenize_texts(processed_df['target_summary'], tokenizer, MAX_LEN_SUMMARY)\n",
    "\n",
    "                if encoder_input_data.size > 0 and decoder_full_data.size > 0:\n",
    "                    print(\"Creating decoder input/target sequences...\")\n",
    "                    decoder_input_data = decoder_full_data[:, :-1]\n",
    "                    decoder_target_data = decoder_full_data[:, 1:]\n",
    "\n",
    "                    print(\"\\n--- Data Shapes After Tokenization & Shifting ---\")\n",
    "                    print(\"Encoder Input Shape:\", encoder_input_data.shape)\n",
    "                    print(\"Decoder Input Shape:\", decoder_input_data.shape)\n",
    "                    print(\"Decoder Target Shape:\", decoder_target_data.shape)\n",
    "                    print(\"-\" * 30)\n",
    "                    logging.info(f\"Tokenization successful. Shapes: Encoder={encoder_input_data.shape}, DecoderIn={decoder_input_data.shape}, DecoderOut={decoder_target_data.shape}\")\n",
    "\n",
    "                else:\n",
    "                    print(\"Tokenization resulted in empty arrays. Cannot proceed.\")\n",
    "                    logging.error(\"Tokenization resulted in empty arrays.\")\n",
    "                    tokenization_failed = True # Set failure flag\n",
    "                    # Clear arrays\n",
    "                    encoder_input_data = np.array([])\n",
    "                    decoder_input_data = np.array([])\n",
    "                    decoder_target_data = np.array([])\n",
    "\n",
    "\n",
    "            else:\n",
    "                print(\"Failed to load tokenizer. Cannot proceed.\")\n",
    "                logging.error(\"Failed to load tokenizer.\")\n",
    "                tokenization_failed = True # Set failure flag\n",
    "\n",
    "    else:\n",
    "         print(\"Skipping tokenizer step: required columns ('cleaned_text', 'target_summary') missing in processed_df.\")\n",
    "         logging.error(\"Skipping tokenizer step: required columns missing.\")\n",
    "         tokenization_failed = True # Set failure flag\n",
    "else:\n",
    "     # This case should have been caught earlier if Blocks 2/3 failed\n",
    "     print(\"Skipping Block 4 execution because `processed_df` is not available or is empty.\")\n",
    "     print(\"Check the output of Block 3.\")\n",
    "     logging.error(\"Skipping Block 4 execution because `processed_df` is not available or is empty.\")\n",
    "     tokenization_failed = True # Set failure flag\n",
    "\n",
    "\n",
    "# Final status check for Block 4\n",
    "if tokenization_failed:\n",
    "    print(\"************************************************************\")\n",
    "    print(\"ERROR: Block 4 failed or was skipped due to issues in previous blocks or during tokenization.\")\n",
    "    print(\"Cannot proceed to Block 5 (Dataset Creation).\")\n",
    "    print(\"************************************************************\")\n",
    "else:\n",
    "    print(\"Block 4 completed successfully. Tokenized data created.\")\n",
    "    logging.info(\"Block 4 completed successfully. Tokenized data created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2327d37d-44ab-444d-8b12-524cb884021b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 15:48:23,349 - INFO - Block 5: Tokenized data is valid. Proceeding with dataset creation.\n",
      "2025-04-24 15:48:23,351 - INFO - Splitting data: 66212 train, 7356 validation.\n",
      "2025-04-24 15:48:23,516 - INFO - Creating tf.data.Dataset. Shuffle=True, Batch Size=64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block 5: Tokenized data is valid. Proceeding with dataset creation.\n",
      "Splitting data: 66212 train, 7356 validation.\n",
      "Shuffled data indices before splitting.\n",
      "\n",
      "Creating training dataset...\n",
      "\n",
      "--- tf.data.Dataset Creation ---\n",
      "Input shapes: Encoder=(66212, 1024), DecoderIn=(66212, 149), DecoderOut=(66212, 149)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 15:48:23.524812: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-24 15:48:23.688464: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20750 MB memory:  -> device: 0, name: NVIDIA L4, pci bus id: 0000:00:03.0, compute capability: 8.9\n",
      "2025-04-24 15:48:24,134 - INFO - Shuffling dataset with buffer size 66212\n",
      "2025-04-24 15:48:24,141 - INFO - tf.data.Dataset created successfully.\n",
      "2025-04-24 15:48:24,142 - INFO - Creating tf.data.Dataset. Shuffle=False, Batch Size=64\n",
      "2025-04-24 15:48:24,161 - INFO - tf.data.Dataset created successfully.\n",
      "2025-04-24 15:48:24,166 - INFO - Block 5 completed successfully. Train/Validation datasets created.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.data.Dataset created (shuffle=True).\n",
      "Element Spec (structure of one batch):\n",
      "({'encoder_inputs': TensorSpec(shape=(64, 1024), dtype=tf.int32, name=None), 'decoder_inputs': TensorSpec(shape=(64, 149), dtype=tf.int32, name=None)}, {'output_layer': TensorSpec(shape=(64, 149), dtype=tf.int32, name=None)})\n",
      "------------------------------\n",
      "\n",
      "Creating validation dataset...\n",
      "\n",
      "--- tf.data.Dataset Creation ---\n",
      "Input shapes: Encoder=(7356, 1024), DecoderIn=(7356, 149), DecoderOut=(7356, 149)\n",
      "tf.data.Dataset created (shuffle=False).\n",
      "Element Spec (structure of one batch):\n",
      "({'encoder_inputs': TensorSpec(shape=(64, 1024), dtype=tf.int32, name=None), 'decoder_inputs': TensorSpec(shape=(64, 149), dtype=tf.int32, name=None)}, {'output_layer': TensorSpec(shape=(64, 149), dtype=tf.int32, name=None)})\n",
      "------------------------------\n",
      "\n",
      "--- Dataset Splitting and Creation Summary ---\n",
      "Total samples tokenized: 73568\n",
      "Training samples: 66212, Validation samples: 7356\n",
      "Train dataset created: Yes\n",
      "Validation dataset created: Yes\n",
      " Approx. train steps per epoch: 1034\n",
      " Approx. validation steps per epoch: 114\n",
      "------------------------------\n",
      "Block 5 completed successfully. Train/Validation datasets created.\n"
     ]
    }
   ],
   "source": [
    "# Block 5: Data Preparation for TensorFlow (tf.data.Dataset) + Execution\n",
    "\n",
    "# --- Function Definition ---\n",
    "# (create_tf_dataset defined as in the previous good version)\n",
    "# ... (insert definition for create_tf_dataset here) ...\n",
    "def create_tf_dataset(encoder_inputs, decoder_inputs, decoder_targets, batch_size, shuffle=True):\n",
    "    \"\"\"Creates a tf.data.Dataset for training or validation.\"\"\"\n",
    "    if not isinstance(encoder_inputs, np.ndarray) or \\\n",
    "       not isinstance(decoder_inputs, np.ndarray) or \\\n",
    "       not isinstance(decoder_targets, np.ndarray):\n",
    "        logging.error(\"Inputs to create_tf_dataset must be numpy arrays.\")\n",
    "        print(\"Error: Inputs for dataset creation are not numpy arrays.\")\n",
    "        return None\n",
    "\n",
    "    if encoder_inputs.size == 0 or decoder_inputs.size == 0 or decoder_targets.size == 0:\n",
    "        logging.error(\"Cannot create dataset from empty numpy arrays.\")\n",
    "        print(\"Error: Input arrays for dataset creation are empty.\")\n",
    "        return None\n",
    "    if not (encoder_inputs.shape[0] == decoder_inputs.shape[0] == decoder_targets.shape[0]):\n",
    "        logging.error(f\"Mismatch in number of samples: Enc={encoder_inputs.shape[0]}, DecIn={decoder_inputs.shape[0]}, DecOut={decoder_targets.shape[0]}\")\n",
    "        print(\"Error: Mismatch in number of samples between input/output arrays.\")\n",
    "        return None\n",
    "\n",
    "    logging.info(f\"Creating tf.data.Dataset. Shuffle={shuffle}, Batch Size={batch_size}\")\n",
    "    print(\"\\n--- tf.data.Dataset Creation ---\")\n",
    "    print(f\"Input shapes: Encoder={encoder_inputs.shape}, DecoderIn={decoder_inputs.shape}, DecoderOut={decoder_targets.shape}\")\n",
    "\n",
    "    try:\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(\n",
    "            (\n",
    "                {\"encoder_inputs\": encoder_inputs, \"decoder_inputs\": decoder_inputs}, # Model inputs dict\n",
    "                {\"output_layer\": decoder_targets} # Model outputs dict\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if shuffle:\n",
    "            # Use a buffer size approx the size of the dataset for good shuffling\n",
    "            buffer_size = len(encoder_inputs)\n",
    "            dataset = dataset.shuffle(buffer_size=buffer_size, reshuffle_each_iteration=True)\n",
    "            logging.info(f\"Shuffling dataset with buffer size {buffer_size}\")\n",
    "\n",
    "        dataset = dataset.batch(batch_size, drop_remainder=True) # Drop remainder is important for stateful RNNs if used, generally good practice\n",
    "        dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "        logging.info(\"tf.data.Dataset created successfully.\")\n",
    "        print(f\"tf.data.Dataset created (shuffle={shuffle}).\")\n",
    "        print(\"Element Spec (structure of one batch):\")\n",
    "        print(dataset.element_spec)\n",
    "        print(\"-\" * 30)\n",
    "        return dataset\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to create tf.data.Dataset: {e}\", exc_info=True)\n",
    "        print(f\"\\n--- Dataset Creation Error ---\")\n",
    "        print(f\"Failed to create tf.data.Dataset: {e}\")\n",
    "        print(\"-\" * 30)\n",
    "        return None\n",
    "\n",
    "\n",
    "# --- Execution for Block 5 ---\n",
    "train_dataset = None\n",
    "val_dataset = None\n",
    "num_train_samples = 0\n",
    "num_val_samples = 0\n",
    "dataset_creation_failed = False # Flag\n",
    "\n",
    "# Only proceed if Block 4 succeeded (tokenization_failed is False)\n",
    "# and the resulting numpy arrays exist and are not empty\n",
    "if 'tokenization_failed' in locals() and not tokenization_failed and \\\n",
    "   'encoder_input_data' in locals() and encoder_input_data.size > 0 and \\\n",
    "   'decoder_input_data' in locals() and decoder_input_data.size > 0 and \\\n",
    "   'decoder_target_data' in locals() and decoder_target_data.size > 0:\n",
    "\n",
    "    print(\"Block 5: Tokenized data is valid. Proceeding with dataset creation.\")\n",
    "    logging.info(\"Block 5: Tokenized data is valid. Proceeding with dataset creation.\")\n",
    "\n",
    "    num_samples = encoder_input_data.shape[0]\n",
    "    if num_samples > 0:\n",
    "        num_val_samples = int(0.1 * num_samples)\n",
    "        # Ensure there's at least one validation sample if possible, and enough training samples\n",
    "        if num_val_samples == 0 and num_samples > 1 : num_val_samples = 1\n",
    "        num_train_samples = num_samples - num_val_samples\n",
    "\n",
    "        if num_train_samples <= 0:\n",
    "             print(f\"Error: Not enough samples ({num_samples}) for a train/validation split (Val samples = {num_val_samples}).\")\n",
    "             logging.error(f\"Not enough samples ({num_samples}) for train/val split.\")\n",
    "             dataset_creation_failed = True\n",
    "        else:\n",
    "            print(f\"Splitting data: {num_train_samples} train, {num_val_samples} validation.\")\n",
    "            logging.info(f\"Splitting data: {num_train_samples} train, {num_val_samples} validation.\")\n",
    "\n",
    "            # Shuffle indices *before* splitting\n",
    "            indices = np.arange(num_samples)\n",
    "            np.random.shuffle(indices)\n",
    "            encoder_input_data = encoder_input_data[indices]\n",
    "            decoder_input_data = decoder_input_data[indices]\n",
    "            decoder_target_data = decoder_target_data[indices]\n",
    "            print(\"Shuffled data indices before splitting.\")\n",
    "\n",
    "            # Perform the split\n",
    "            encoder_input_train = encoder_input_data[:num_train_samples]\n",
    "            decoder_input_train = decoder_input_data[:num_train_samples]\n",
    "            decoder_target_train = decoder_target_data[:num_train_samples]\n",
    "\n",
    "            encoder_input_val = encoder_input_data[num_train_samples:]\n",
    "            decoder_input_val = decoder_input_data[num_train_samples:]\n",
    "            decoder_target_val = decoder_target_data[num_train_samples:]\n",
    "\n",
    "            # Create datasets\n",
    "            print(\"\\nCreating training dataset...\")\n",
    "            train_dataset = create_tf_dataset(\n",
    "                encoder_input_train, decoder_input_train, decoder_target_train, BATCH_SIZE, shuffle=True\n",
    "            )\n",
    "            print(\"\\nCreating validation dataset...\")\n",
    "            val_dataset = create_tf_dataset(\n",
    "                encoder_input_val, decoder_input_val, decoder_target_val, BATCH_SIZE, shuffle=False # No need to shuffle validation\n",
    "            )\n",
    "\n",
    "            if train_dataset is None or val_dataset is None:\n",
    "                 print(\"Error: Failed to create train or validation dataset.\")\n",
    "                 logging.error(\"Failed to create train or validation dataset.\")\n",
    "                 dataset_creation_failed = True\n",
    "            else:\n",
    "                print(\"\\n--- Dataset Splitting and Creation Summary ---\")\n",
    "                print(f\"Total samples tokenized: {num_samples}\")\n",
    "                print(f\"Training samples: {num_train_samples}, Validation samples: {num_val_samples}\")\n",
    "                print(f\"Train dataset created: Yes\")\n",
    "                print(f\"Validation dataset created: Yes\")\n",
    "                try: print(f\" Approx. train steps per epoch: {len(train_dataset)}\")\n",
    "                except TypeError: print(\" Approx. train steps per epoch: Unknown (infinite dataset?)\")\n",
    "                try: print(f\" Approx. validation steps per epoch: {len(val_dataset)}\")\n",
    "                except TypeError: print(\" Approx. validation steps per epoch: Unknown (infinite dataset?)\")\n",
    "                print(\"-\" * 30)\n",
    "\n",
    "    else:\n",
    "        print(\"No samples found in tokenized data. Cannot create datasets.\")\n",
    "        logging.error(\"No samples found in tokenized data. Cannot create datasets.\")\n",
    "        dataset_creation_failed = True\n",
    "\n",
    "else:\n",
    "    print(\"Skipping Block 5 execution: Tokenization failed or resulted in empty data.\")\n",
    "    logging.error(\"Skipping Block 5 execution: Tokenization failed or resulted in empty data.\")\n",
    "    dataset_creation_failed = True\n",
    "\n",
    "\n",
    "# Final status check for Block 5\n",
    "if dataset_creation_failed:\n",
    "    print(\"************************************************************\")\n",
    "    print(\"ERROR: Block 5 failed or was skipped.\")\n",
    "    print(\"Cannot proceed to Block 6 (Model Building).\")\n",
    "    print(\"Review the output from Block 4 and 5.\")\n",
    "    print(\"************************************************************\")\n",
    "else:\n",
    "    print(\"Block 5 completed successfully. Train/Validation datasets created.\")\n",
    "    logging.info(\"Block 5 completed successfully. Train/Validation datasets created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1be8eb3e-2abb-4b7c-bbc1-b9b2db2c4a68",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 15:48:30,248 - INFO - Building Combined Training Model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Combined Training Model Build ---\n",
      "Building Encoder with 2 BiLSTM layers...\n",
      "Building Decoder with 2 LSTM layers...\n",
      "\n",
      "--- Combined Training Model Summary ---\n",
      "Model: \"model\"\n",
      "________________________________________________________________________________________________________________________\n",
      " Layer (type)                          Output Shape               Param #       Connected to                            \n",
      "========================================================================================================================\n",
      " encoder_inputs (InputLayer)           [(None, 1024)]             0             []                                      \n",
      "                                                                                                                        \n",
      " encoder_embedding (Embedding)         (None, 1024, 100)          3000000       ['encoder_inputs[0][0]']                \n",
      "                                                                                                                        \n",
      " bidirectional (Bidirectional)         [(None, 1024, 512),        731136        ['encoder_embedding[0][0]']             \n",
      "                                        (None, 256),                                                                    \n",
      "                                        (None, 256),                                                                    \n",
      "                                        (None, 256),                                                                    \n",
      "                                        (None, 256)]                                                                    \n",
      "                                                                                                                        \n",
      " decoder_inputs (InputLayer)           [(None, 149)]              0             []                                      \n",
      "                                                                                                                        \n",
      " bidirectional_1 (Bidirectional)       [(None, 1024, 512),        1574912       ['bidirectional[0][0]']                 \n",
      "                                        (None, 256),                                                                    \n",
      "                                        (None, 256),                                                                    \n",
      "                                        (None, 256),                                                                    \n",
      "                                        (None, 256)]                                                                    \n",
      "                                                                                                                        \n",
      " decoder_embedding (Embedding)         (None, 149, 100)           3000000       ['decoder_inputs[0][0]']                \n",
      "                                                                                                                        \n",
      " encoder_final_h (Concatenate)         (None, 512)                0             ['bidirectional_1[0][1]',               \n",
      "                                                                                 'bidirectional_1[0][3]']               \n",
      "                                                                                                                        \n",
      " encoder_final_c (Concatenate)         (None, 512)                0             ['bidirectional_1[0][2]',               \n",
      "                                                                                 'bidirectional_1[0][4]']               \n",
      "                                                                                                                        \n",
      " decoder_lstm_1 (LSTM)                 (None, 149, 512)           1255424       ['decoder_embedding[0][0]',             \n",
      "                                                                                 'encoder_final_h[0][0]',               \n",
      "                                                                                 'encoder_final_c[0][0]']               \n",
      "                                                                                                                        \n",
      " decoder_lstm_2 (LSTM)                 (None, 149, 512)           2099200       ['decoder_lstm_1[0][0]']                \n",
      "                                                                                                                        \n",
      " output_layer (Dense)                  (None, 149, 30000)         15390000      ['decoder_lstm_2[0][0]']                \n",
      "                                                                                                                        \n",
      "========================================================================================================================\n",
      "Total params: 27,050,672\n",
      "Trainable params: 27,050,672\n",
      "Non-trainable params: 0\n",
      "________________________________________________________________________________________________________________________\n",
      "Model Inputs: ['encoder_inputs: (None, 1024)', 'decoder_inputs: (None, 149)']\n",
      "Model Outputs: ['output_layer/Softmax:0: (None, 149, 30000)']\n",
      "Output layer name: output_layer (should match dataset target key: 'output_layer')\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Block 6: Model Architecture + Execution\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Bidirectional, Dense, Dropout, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# --- Function Definition ---\n",
    "def build_training_model(vocab_size, embedding_dim, lstm_units, decoder_lstm_units,\n",
    "                         num_encoder_layers, num_decoder_layers, dropout_rate,\n",
    "                         max_len_input, max_len_summary):\n",
    "    \"\"\"Builds the combined Encoder-Decoder model for training.\"\"\"\n",
    "    logging.info(\"Building Combined Training Model...\")\n",
    "    print(\"\\n--- Combined Training Model Build ---\")\n",
    "\n",
    "    # --- Encoder part ---\n",
    "    encoder_input_layer = Input(shape=(max_len_input,), name=\"encoder_inputs\")\n",
    "    # Use mask_zero=True for Embedding layers\n",
    "    encoder_embedding_layer = Embedding(vocab_size, embedding_dim, mask_zero=True, name=\"encoder_embedding\")\n",
    "    encoder_embeddings = encoder_embedding_layer(encoder_input_layer)\n",
    "\n",
    "    current_sequence = encoder_embeddings\n",
    "    encoder_states = [] # Will hold final states [h, c] from the last layer\n",
    "\n",
    "    print(f\"Building Encoder with {num_encoder_layers} BiLSTM layers...\")\n",
    "    for i in range(num_encoder_layers):\n",
    "        is_last_layer = (i == num_encoder_layers - 1)\n",
    "        bilstm = Bidirectional(\n",
    "            LSTM(lstm_units, return_sequences=True, return_state=True, dropout=dropout_rate, name=f\"encoder_bilstm_{i+1}\")\n",
    "        )\n",
    "        # We need return_sequences=True for stacking, even on the last layer,\n",
    "        # but we only *use* the states from the last layer for the decoder init.\n",
    "        # The state outputs (forward_h, forward_c, backward_h, backward_c) are always returned when return_state=True.\n",
    "        encoder_output_seq, forward_h, forward_c, backward_h, backward_c = bilstm(current_sequence)\n",
    "\n",
    "        if is_last_layer:\n",
    "            state_h = Concatenate(name=\"encoder_final_h\")([forward_h, backward_h])\n",
    "            state_c = Concatenate(name=\"encoder_final_c\")([forward_c, backward_c])\n",
    "            encoder_states = [state_h, state_c] # Final states for decoder init\n",
    "        # Update current_sequence for the next layer (always use the sequence output)\n",
    "        current_sequence = encoder_output_seq # Output sequence from BiLSTM\n",
    "\n",
    "    # --- Decoder part (using teacher forcing) ---\n",
    "    decoder_input_layer = Input(shape=(max_len_summary - 1,), name=\"decoder_inputs\") # Shifted target summary\n",
    "    # Use a *separate* Embedding layer for the decoder is good practice\n",
    "    decoder_embedding_layer = Embedding(vocab_size, embedding_dim, mask_zero=True, name=\"decoder_embedding\")\n",
    "    decoder_embeddings = decoder_embedding_layer(decoder_input_layer)\n",
    "\n",
    "    current_sequence = decoder_embeddings\n",
    "    # Use encoder final states as initial state for the *first* decoder LSTM\n",
    "    current_states = encoder_states\n",
    "\n",
    "    print(f\"Building Decoder with {num_decoder_layers} LSTM layers...\")\n",
    "    for i in range(num_decoder_layers):\n",
    "        # We need return_sequences=True to feed into the final Dense layer\n",
    "        # We don't need return_state=True here for the *training* model output\n",
    "        decoder_lstm = LSTM(decoder_lstm_units, return_sequences=True, return_state=False, dropout=dropout_rate, name=f\"decoder_lstm_{i+1}\")\n",
    "        # Pass initial state only to the first layer\n",
    "        if i == 0:\n",
    "            current_sequence = decoder_lstm(current_sequence, initial_state=current_states)\n",
    "        else:\n",
    "            # Keras handles state propagation between stacked LSTMs internally during training\n",
    "            current_sequence = decoder_lstm(current_sequence)\n",
    "\n",
    "    # --- Final output layer ---\n",
    "    output_dense_layer = Dense(vocab_size, activation='softmax', name=\"output_layer\")\n",
    "    decoder_outputs = output_dense_layer(current_sequence)\n",
    "\n",
    "    # Define the complete model for training\n",
    "    training_model = Model(inputs=[encoder_input_layer, decoder_input_layer], outputs=decoder_outputs)\n",
    "\n",
    "    # --- Debugging Info ---\n",
    "    print(\"\\n--- Combined Training Model Summary ---\")\n",
    "    training_model.summary(line_length=120)\n",
    "    print(f\"Model Inputs: {[inp.name + ': ' + str(inp.shape) for inp in training_model.inputs]}\")\n",
    "    print(f\"Model Outputs: {[out.name + ': ' + str(out.shape) for out in training_model.outputs]}\")\n",
    "    # Verify output layer name matches dataset key\n",
    "    print(f\"Output layer name: {training_model.layers[-1].name} (should match dataset target key: 'output_layer')\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    return training_model\n",
    "\n",
    "# --- Execution for Block 6 ---\n",
    "model = None # Initialize model variable\n",
    "\n",
    "# Build the model\n",
    "# Use the constants defined in Block 1\n",
    "model = build_training_model(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    lstm_units=LSTM_UNITS, # Encoder units per direction\n",
    "    decoder_lstm_units=DECODER_LSTM_UNITS, # Decoder units (matching concatenated encoder state)\n",
    "    num_encoder_layers=NUM_ENCODER_LAYERS,\n",
    "    num_decoder_layers=NUM_DECODER_LAYERS, # Use separate constant if needed\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    max_len_input=MAX_LEN_INPUT,\n",
    "    max_len_summary=MAX_LEN_SUMMARY # Use max_len_summary - 1 due to decoder input shape? No, model handles length internally\n",
    ")\n",
    "\n",
    "if model is None:\n",
    "    print(\"Model building failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d94962fd-a2d7-4070-be2d-a869016d71f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 15:48:47,275 - INFO - Compiling model with AdamW optimizer and learning rate 0.001...\n",
      "2025-04-24 15:48:47,277 - WARNING - tf.keras.optimizers.AdamW not found, using Adam instead.\n",
      "2025-04-24 15:48:47,292 - INFO - Setting up callbacks...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: tf.keras.optimizers.AdamW not found, using Adam instead.\n",
      "\n",
      "--- Model Compilation ---\n",
      "Optimizer: Adam (LR=0.001)\n",
      "Loss Function: SparseCategoricalCrossentropy\n",
      "Metrics: ['accuracy']\n",
      "Model compiled successfully.\n",
      "------------------------------\n",
      "TensorBoard log directory: model3_files/logs/fit/20250424-154454\n",
      "\n",
      "--- Callbacks Setup ---\n",
      "ModelCheckpoint: monitor='val_loss', save_best_only=True, path='model3_files/pib_summarizer_no_attention.keras'\n",
      "EarlyStopping: monitor='val_loss', patience=5, restore_best_weights=True\n",
      "ReduceLROnPlateau: monitor='val_loss', patience=3, factor=0.2\n",
      "TensorBoard: log_dir='model3_files/logs/fit/20250424-154454'\n",
      "Total callbacks: 4\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Block 7: Model Compilation and Callbacks + Execution\n",
    "\n",
    "# --- Function Definitions ---\n",
    "def compile_model(model_to_compile, learning_rate):\n",
    "    \"\"\"Compiles the Keras model.\"\"\"\n",
    "    if model_to_compile is None:\n",
    "         print(\"Error: Cannot compile a None model.\")\n",
    "         return None\n",
    "\n",
    "    logging.info(f\"Compiling model with AdamW optimizer and learning rate {learning_rate}...\")\n",
    "\n",
    "    # Optimizer - Use AdamW if available (good default), fallback to Adam\n",
    "    try:\n",
    "        optimizer = tf.keras.optimizers.AdamW(learning_rate=learning_rate)\n",
    "        opt_name = \"AdamW\"\n",
    "    except AttributeError:\n",
    "        logging.warning(\"tf.keras.optimizers.AdamW not found, using Adam instead.\")\n",
    "        print(\"Warning: tf.keras.optimizers.AdamW not found, using Adam instead.\")\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        opt_name = \"Adam\"\n",
    "\n",
    "    # Loss Function - SparseCategoricalCrossentropy because target tokens are integers (not one-hot)\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "    # Metrics - Accuracy is useful to monitor during training\n",
    "    metrics = ['accuracy'] # Can add custom metrics later if needed\n",
    "\n",
    "    # Set jit_compile=False for stability based on project description\n",
    "    model_to_compile.compile(optimizer=optimizer, loss=loss, metrics=metrics, jit_compile=False)\n",
    "\n",
    "    # --- Debugging Info ---\n",
    "    print(\"\\n--- Model Compilation ---\")\n",
    "    print(f\"Optimizer: {opt_name} (LR={learning_rate})\")\n",
    "    print(f\"Loss Function: {loss.__class__.__name__}\")\n",
    "    print(f\"Metrics: {metrics}\")\n",
    "    print(\"Model compiled successfully.\")\n",
    "    print(\"-\" * 30)\n",
    "    return model_to_compile # Return compiled model\n",
    "\n",
    "\n",
    "def get_callbacks(model_save_path, log_dir, early_stopping_patience, reduce_lr_patience, reduce_lr_factor):\n",
    "    \"\"\"Sets up Keras callbacks for training.\"\"\"\n",
    "    logging.info(\"Setting up callbacks...\")\n",
    "\n",
    "    # Ensure log directory exists\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    print(f\"TensorBoard log directory: {log_dir}\")\n",
    "\n",
    "    # ModelCheckpoint: Save the best model based on validation loss\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        filepath=model_save_path,\n",
    "        monitor='val_loss',          # Monitor validation loss\n",
    "        save_best_only=True,         # Only save if validation loss improves\n",
    "        save_weights_only=False,     # Save the entire model (architecture + weights + optimizer state)\n",
    "        mode='min',                  # The lower the validation loss, the better\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # EarlyStopping: Stop training if validation loss doesn't improve\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=early_stopping_patience,\n",
    "        restore_best_weights=True,   # Restore weights from the epoch with the best val_loss\n",
    "        mode='min',\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # ReduceLROnPlateau: Reduce learning rate if validation loss plateaus\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=reduce_lr_factor,     # Factor by which LR is reduced (new_lr = lr * factor)\n",
    "        patience=reduce_lr_patience,\n",
    "        min_lr=1e-6,                 # Lower bound for the learning rate\n",
    "        mode='min',\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # TensorBoard: Log metrics for visualization\n",
    "    tensorboard = TensorBoard(\n",
    "        log_dir=log_dir,\n",
    "        histogram_freq=1,            # Log histogram visualizations every 1 epoch\n",
    "        write_graph=True,           # Visualize the graph in TensorBoard\n",
    "        update_freq='epoch'          # Log metrics after each epoch\n",
    "    )\n",
    "\n",
    "    callback_list = [checkpoint, early_stopping, reduce_lr, tensorboard]\n",
    "\n",
    "    # --- Debugging Info ---\n",
    "    print(\"\\n--- Callbacks Setup ---\")\n",
    "    print(f\"ModelCheckpoint: monitor='val_loss', save_best_only=True, path='{model_save_path}'\")\n",
    "    print(f\"EarlyStopping: monitor='val_loss', patience={early_stopping_patience}, restore_best_weights=True\")\n",
    "    print(f\"ReduceLROnPlateau: monitor='val_loss', patience={reduce_lr_patience}, factor={reduce_lr_factor}\")\n",
    "    print(f\"TensorBoard: log_dir='{log_dir}'\")\n",
    "    print(f\"Total callbacks: {len(callback_list)}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    return callback_list\n",
    "\n",
    "# --- Execution for Block 7 ---\n",
    "callbacks = [] # Initialize callbacks list\n",
    "\n",
    "if 'model' in locals() and model is not None:\n",
    "     # Compile the model created in Block 6\n",
    "     model = compile_model(model, LEARNING_RATE) # Reassign model to the compiled version\n",
    "     # Get the callbacks\n",
    "     callbacks = get_callbacks(\n",
    "         MODEL_SAVE_PATH,\n",
    "         LOG_DIR,\n",
    "         EARLY_STOPPING_PATIENCE,\n",
    "         REDUCE_LR_PATIENCE,\n",
    "         REDUCE_LR_FACTOR\n",
    "     )\n",
    "else:\n",
    "     print(\"Skipping compilation and callback setup as the model was not built successfully in Block 6.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc43fc95-b501-463d-8322-7c6d50ff22dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 15:48:51,965 - INFO - Starting model training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with BATCH_SIZE = 64\n",
      "\n",
      "--- Model Training ---\n",
      "Training for a maximum of 30 epochs.\n",
      "Using Batch Size: 64\n",
      "Using 4 callbacks: ['ModelCheckpoint', 'EarlyStopping', 'ReduceLROnPlateau', 'TensorBoard']\n",
      "Train dataset steps per epoch: 1034\n",
      "Validation dataset steps per epoch: 114\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 15:49:06.804629: W tensorflow/core/common_runtime/type_inference.cc:339] Type inference failed. This indicates an invalid graph that escaped type checking. Error message: INVALID_ARGUMENT: expected compatible input types, but input 1:\n",
      "type_id: TFT_OPTIONAL\n",
      "args {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_TENSOR\n",
      "    args {\n",
      "      type_id: TFT_INT32\n",
      "    }\n",
      "  }\n",
      "}\n",
      " is neither a subtype nor a supertype of the combined inputs preceding it:\n",
      "type_id: TFT_OPTIONAL\n",
      "args {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_TENSOR\n",
      "    args {\n",
      "      type_id: TFT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\twhile inferring type of node 'cond_20/output/_23'\n",
      "2025-04-24 15:49:07.112046: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8900\n",
      "2025-04-24 15:49:07.986716: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2025-04-24 15:49:08.107215: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x7f0eabd7ddf0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-04-24 15:49:08.107247: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA L4, Compute Capability 8.9\n",
      "2025-04-24 15:49:08.150458: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-04-24 15:49:08.576087: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1034/1034 [==============================] - ETA: 0s - loss: 6.9200 - accuracy: 0.0776\n",
      "Epoch 1: val_loss improved from inf to 6.47542, saving model to model3_files/pib_summarizer_no_attention.keras\n",
      "1034/1034 [==============================] - 484s 450ms/step - loss: 6.9200 - accuracy: 0.0776 - val_loss: 6.4754 - val_accuracy: 0.1421 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "1034/1034 [==============================] - ETA: 0s - loss: 5.7853 - accuracy: 0.2090\n",
      "Epoch 2: val_loss improved from 6.47542 to 5.04802, saving model to model3_files/pib_summarizer_no_attention.keras\n",
      "1034/1034 [==============================] - 399s 386ms/step - loss: 5.7853 - accuracy: 0.2090 - val_loss: 5.0480 - val_accuracy: 0.2728 - lr: 0.0010\n",
      "Epoch 3/30\n",
      "1034/1034 [==============================] - ETA: 0s - loss: 4.6957 - accuracy: 0.2971\n",
      "Epoch 3: val_loss improved from 5.04802 to 4.37027, saving model to model3_files/pib_summarizer_no_attention.keras\n",
      "1034/1034 [==============================] - 379s 366ms/step - loss: 4.6957 - accuracy: 0.2971 - val_loss: 4.3703 - val_accuracy: 0.3238 - lr: 0.0010\n",
      "Epoch 4/30\n",
      "1034/1034 [==============================] - ETA: 0s - loss: 4.2164 - accuracy: 0.3330\n",
      "Epoch 4: val_loss improved from 4.37027 to 4.03666, saving model to model3_files/pib_summarizer_no_attention.keras\n",
      "1034/1034 [==============================] - 367s 355ms/step - loss: 4.2164 - accuracy: 0.3330 - val_loss: 4.0367 - val_accuracy: 0.3513 - lr: 0.0010\n",
      "Epoch 5/30\n",
      " 473/1034 [============>.................] - ETA: 3:03 - loss: 3.9727 - accuracy: 0.3512"
     ]
    }
   ],
   "source": [
    "# Block 8: Model Training + Execution\n",
    "\n",
    "import time\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "import numpy as np # Needed for np.argmin in history analysis\n",
    "import pandas as pd # Needed for plotting history\n",
    "import matplotlib.pyplot as plt # Needed for plotting history\n",
    "from tensorflow.keras.callbacks import EarlyStopping # Needed for type checking in history analysis\n",
    "\n",
    "\n",
    "# --- Function Definition (Corrected Indentation) ---\n",
    "def train_model(model_to_train, train_data, val_data, epochs_to_run, callback_list):\n",
    "    \"\"\"Trains the model using model.fit()\"\"\"\n",
    "    logging.info(\"Starting model training...\")\n",
    "    # --- Debugging Info ---\n",
    "    print(\"\\n--- Model Training ---\")\n",
    "\n",
    "    # --- Input Checks ---\n",
    "    if model_to_train is None:\n",
    "        print(\"Error: Model is not defined or not compiled. Cannot train.\")\n",
    "        logging.error(\"Attempted to train a None or uncompiled model.\")\n",
    "        print(\"-\" * 30)\n",
    "        return None\n",
    "    if not hasattr(model_to_train, 'optimizer') or model_to_train.optimizer is None:\n",
    "        print(\"Error: Model has not been compiled. Cannot train.\")\n",
    "        logging.error(\"Attempted to train an uncompiled model.\")\n",
    "        print(\"-\" * 30)\n",
    "        return None\n",
    "    if train_data is None:\n",
    "        print(\"Error: Training dataset is not defined. Cannot train.\")\n",
    "        logging.error(\"Attempted to train with a None training dataset.\")\n",
    "        print(\"-\" * 30)\n",
    "        return None\n",
    "    if val_data is None:\n",
    "        print(\"Warning: Validation dataset is not defined. Training without validation monitoring (EarlyStopping/ModelCheckpoint might not work as expected).\")\n",
    "        logging.warning(\"Training without validation dataset.\")\n",
    "\n",
    "    # --- Print Training Info ---\n",
    "    print(f\"Training for a maximum of {epochs_to_run} epochs.\")\n",
    "    # Use BATCH_SIZE constant defined in Block 1 for reporting\n",
    "    print(f\"Using Batch Size: {BATCH_SIZE}\")\n",
    "    print(f\"Using {len(callback_list)} callbacks: {[cb.__class__.__name__ for cb in callback_list]}\")\n",
    "    try:\n",
    "        train_steps = len(train_data)\n",
    "        print(f\"Train dataset steps per epoch: {train_steps}\")\n",
    "    except TypeError:\n",
    "        print(\"Could not determine train dataset length (infinite dataset?).\")\n",
    "        train_steps = None\n",
    "    if val_data:\n",
    "        try:\n",
    "             val_steps = len(val_data)\n",
    "             print(f\"Validation dataset steps per epoch: {val_steps}\")\n",
    "        except TypeError:\n",
    "             print(\"Could not determine validation dataset length.\")\n",
    "             val_steps = None\n",
    "    else:\n",
    "         val_steps = None\n",
    "\n",
    "    # --- Training Loop with Corrected try...except ---\n",
    "    start_time = time.time()\n",
    "    train_history = None\n",
    "    try:  # <--- Start of the try block\n",
    "        train_history = model_to_train.fit(\n",
    "            train_data,\n",
    "            epochs=epochs_to_run,\n",
    "            validation_data=val_data, # Pass validation data\n",
    "            callbacks=callback_list,\n",
    "            verbose=1 # Show progress bar and metrics per epoch\n",
    "        )\n",
    "        training_time = time.time() - start_time\n",
    "        logging.info(f\"Model training finished in {training_time:.2f} seconds.\")\n",
    "        print(f\"\\nTraining complete. Total time: {training_time:.2f} seconds.\")\n",
    "        # --- Debugging: Print final metrics ---\n",
    "        if train_history and train_history.history:\n",
    "            print(\"Final Training Metrics (from last epoch):\")\n",
    "            for metric, value in train_history.history.items():\n",
    "                 if value:\n",
    "                     print(f\"  {metric}: {value[-1]:.4f}\")\n",
    "                 else:\n",
    "                     print(f\"  {metric}: No data recorded\")\n",
    "            # Note about EarlyStopping\n",
    "            # Check if EarlyStopping callback is present in the list\n",
    "            early_stopping_callback = next((cb for cb in callback_list if isinstance(cb, EarlyStopping)), None)\n",
    "            if early_stopping_callback and early_stopping_callback.restore_best_weights:\n",
    "                 if 'val_loss' in train_history.history and train_history.history['val_loss']:\n",
    "                     # Find the epoch with the minimum validation loss\n",
    "                     best_epoch_idx = np.argmin(train_history.history['val_loss'])\n",
    "                     best_val_loss = train_history.history['val_loss'][best_epoch_idx]\n",
    "                     print(f\"Best validation loss ({best_val_loss:.4f}) occurred at epoch {best_epoch_idx + 1}.\")\n",
    "                 else:\n",
    "                      print(\"Note: Early stopping with restore_best_weights used, but val_loss history unavailable to determine best epoch.\")\n",
    "        else:\n",
    "            print(\"Training history is not available.\")\n",
    "\n",
    "    # <<< CORRECTED INDENTATION for except/finally >>>\n",
    "    except tf.errors.ResourceExhaustedError as e: # Catch the more common OOM error type\n",
    "        logging.error(f\"Out of Memory (ResourceExhaustedError) during training: {e}\", exc_info=True)\n",
    "        print(f\"\\n--- Training Error: Out of Memory ---\")\n",
    "        print(f\"GPU ran out of memory. Reduce BATCH_SIZE in Block 1 and restart the kernel.\")\n",
    "        # Access BATCH_SIZE directly (assuming it's a global constant from Block 1)\n",
    "        print(f\"Current BATCH_SIZE: {BATCH_SIZE}\")\n",
    "        print(f\"Error details: {e}\")\n",
    "        print(\"Suggestion: If memory fragmentation is suspected, try setting environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' before starting Python/Jupyter.\")\n",
    "    except Exception as e: # Keep generic catch-all\n",
    "        if \"OOM\" in str(e) or \"out of memory\" in str(e).lower() or \"resource exhausted\" in str(e).lower():\n",
    "             logging.error(f\"Likely Out of Memory error during training (caught by general Exception): {e}\", exc_info=True)\n",
    "             print(f\"\\n--- Training Error: Likely Out of Memory ---\")\n",
    "             print(f\"GPU ran out of memory. Reduce BATCH_SIZE in Block 1 and restart the kernel.\")\n",
    "             print(f\"Current BATCH_SIZE: {BATCH_SIZE}\")\n",
    "             print(f\"Error details: {e}\")\n",
    "        else:\n",
    "             logging.error(f\"An unexpected error occurred during model training: {e}\", exc_info=True)\n",
    "             print(f\"\\n--- Training Error ---\")\n",
    "             print(f\"An unexpected error occurred during training: {e}\")\n",
    "\n",
    "    finally: # <--- Correctly indented finally\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "    return train_history # <--- Return statement should be outside finally, aligned with try/except\n",
    "\n",
    "\n",
    "# --- Execution for Block 8 ---\n",
    "history = None # Initialize history variable\n",
    "\n",
    "# Check if all required components are available\n",
    "if ('model' in locals() and model and hasattr(model, 'optimizer') and model.optimizer and\n",
    "    'train_dataset' in locals() and train_dataset and\n",
    "    'val_dataset' in locals() and val_dataset and # Make sure validation exists for callbacks\n",
    "    'callbacks' in locals() and callbacks):\n",
    "\n",
    "    # Assuming BATCH_SIZE is accessible from Block 1's scope\n",
    "    print(f\"Starting training with BATCH_SIZE = {BATCH_SIZE}\")\n",
    "    history = train_model(\n",
    "        model,\n",
    "        train_dataset,\n",
    "        val_dataset,\n",
    "        EPOCHS, # Use constant from Block 1\n",
    "        callbacks\n",
    "    )\n",
    "else:\n",
    "     print(\"Skipping training due to missing compiled model, datasets, or callbacks.\")\n",
    "     # Add more detailed checks for debugging\n",
    "     print(f\"  Model exists and compiled: {'model' in locals() and model and hasattr(model, 'optimizer') and model.optimizer is not None}\")\n",
    "     print(f\"  Train dataset exists: {'train_dataset' in locals() and train_dataset is not None}\")\n",
    "     print(f\"  Validation dataset exists: {'val_dataset' in locals() and val_dataset is not None}\")\n",
    "     print(f\"  Callbacks exist: {'callbacks' in locals() and callbacks and len(callbacks)>0}\")\n",
    "\n",
    "\n",
    "# --- Debugging: Plot training history (optional) ---\n",
    "if history and history.history:\n",
    "    print(\"\\n--- Plotting Training History ---\")\n",
    "    try:\n",
    "        # Ensure Matplotlib is imported if not already done globally\n",
    "        # import matplotlib.pyplot as plt\n",
    "        # Ensure pandas is imported if not already done globally\n",
    "        # import pandas as pd\n",
    "        pd.DataFrame(history.history).plot(figsize=(10, 6))\n",
    "        plt.grid(True)\n",
    "        plt.title(\"Model Training History\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Metric Value\")\n",
    "        # Adjust ylim based on metrics present\n",
    "        min_loss = float('inf')\n",
    "        if 'loss' in history.history and history.history['loss']:\n",
    "            min_loss = min(min_loss, min(history.history['loss']))\n",
    "        if 'val_loss' in history.history and history.history['val_loss']:\n",
    "             min_loss = min(min_loss, min(history.history['val_loss']))\n",
    "\n",
    "        # Set bottom ylim slightly below min loss, but not below 0\n",
    "        plt.ylim(bottom=max(0, min_loss - 0.1 if min_loss != float('inf') else 0))\n",
    "\n",
    "        # Save plot to file\n",
    "        plot_path = os.path.join(OUTPUT_DIR, 'training_history.png') # Use OUTPUT_DIR from Block 1\n",
    "        plt.savefig(plot_path)\n",
    "        print(f\"Plot saved to {plot_path}\")\n",
    "        plt.show() # Display the plot in the notebook\n",
    "        print(\"Plot displayed.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not plot or save history: {e}\")\n",
    "        logging.error(f\"Could not plot or save history: {e}\", exc_info=True)\n",
    "    finally:\n",
    "        print(\"-\" * 30)\n",
    "elif 'history' in locals() and history is None:\n",
    "     print(\"Skipping history plot because training did not run or failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733c0736-5db7-422a-b42d-ad5dcb520951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 9: Inference Setup + Execution\n",
    "\n",
    "# --- Function Definition ---\n",
    "def setup_inference_models(trained_model_path, num_decoder_layers, decoder_lstm_units):\n",
    "    \"\"\"Loads the trained model and creates separate encoder/decoder models for inference.\"\"\"\n",
    "    logging.info(f\"Setting up inference models from: {trained_model_path}\")\n",
    "    # --- Debugging Info ---\n",
    "    print(\"\\n--- Inference Setup ---\")\n",
    "\n",
    "    # --- 1. Load the trained model ---\n",
    "    if not os.path.exists(trained_model_path):\n",
    "        print(f\"Error: Trained model file not found at {trained_model_path}. Cannot setup inference.\")\n",
    "        logging.error(f\"Trained model not found: {trained_model_path}\")\n",
    "        print(\"-\" * 30)\n",
    "        return None, None\n",
    "\n",
    "    try:\n",
    "        logging.info(\"Loading the full trained model...\")\n",
    "        # No custom objects expected based on the architecture\n",
    "        trained_model = tf.keras.models.load_model(trained_model_path)\n",
    "        print(f\"Successfully loaded trained model from {trained_model_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to load trained model from {trained_model_path}: {e}\", exc_info=True)\n",
    "        print(f\"Error: Failed to load trained model from {trained_model_path}: {e}\")\n",
    "        print(\"-\" * 30)\n",
    "        return None, None\n",
    "\n",
    "    # --- 2. Create Inference Encoder Model ---\n",
    "    logging.info(\"Creating inference encoder model...\")\n",
    "    inf_encoder = None\n",
    "    try:\n",
    "        # Extract layers by name from the loaded trained_model\n",
    "        encoder_input_layer = trained_model.get_layer(\"encoder_inputs\").input\n",
    "        # Get the final concatenated states from the encoder part\n",
    "        encoder_state_h = trained_model.get_layer(\"encoder_final_h\").output\n",
    "        encoder_state_c = trained_model.get_layer(\"encoder_final_c\").output\n",
    "        encoder_states = [encoder_state_h, encoder_state_c]\n",
    "\n",
    "        inf_encoder = Model(inputs=encoder_input_layer, outputs=encoder_states, name=\"inference_encoder\")\n",
    "        print(\"Inference Encoder created.\")\n",
    "        # --- Debugging Info ---\n",
    "        print(\"\\n--- Inference Encoder Summary ---\")\n",
    "        inf_encoder.summary(line_length=100)\n",
    "        print(f\"Inference Encoder Inputs: {inf_encoder.input_shape}\")\n",
    "        print(f\"Inference Encoder Outputs (States): {[s.shape for s in inf_encoder.output]}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to create inference encoder: {e}\", exc_info=True)\n",
    "        print(f\"Error: Failed to create inference encoder: {e}\")\n",
    "        inf_encoder = None # Ensure it's None on failure\n",
    "\n",
    "    # --- 3. Create Inference Decoder Model ---\n",
    "    # This requires rebuilding the decoder part to handle state inputs/outputs explicitly.\n",
    "    logging.info(\"Creating inference decoder model...\")\n",
    "    inf_decoder = None\n",
    "    try:\n",
    "        # Decoder Inputs: Single token + state from previous step for EACH LSTM layer\n",
    "        decoder_input_single_token = Input(shape=(1,), name=\"inf_decoder_input_token\")\n",
    "\n",
    "        # State inputs: one pair (h, c) for EACH decoder LSTM layer\n",
    "        decoder_state_inputs = []\n",
    "        for i in range(num_decoder_layers):\n",
    "            state_h = Input(shape=(decoder_lstm_units,), name=f'inf_decoder_input_h_{i}')\n",
    "            state_c = Input(shape=(decoder_lstm_units,), name=f'inf_decoder_input_c_{i}')\n",
    "            decoder_state_inputs.extend([state_h, state_c])\n",
    "\n",
    "        # Embedding layer (reuse weights from trained model)\n",
    "        decoder_embedding_layer = trained_model.get_layer(\"decoder_embedding\")\n",
    "        # Need to ensure the inference embedding layer has the same config\n",
    "        inf_embedding_layer = Embedding(decoder_embedding_layer.input_dim,\n",
    "                                        decoder_embedding_layer.output_dim,\n",
    "                                        mask_zero=decoder_embedding_layer.mask_zero, # Usually False or not needed for single token input\n",
    "                                        name=\"inf_decoder_embedding\")\n",
    "        inf_embedding_layer.build(input_shape=(None, 1)) # Build layer with expected input shape\n",
    "        inf_embedding_layer.set_weights(decoder_embedding_layer.get_weights())\n",
    "        decoder_embeddings = inf_embedding_layer(decoder_input_single_token)\n",
    "\n",
    "        # Recreate LSTM layers, setting return_state=True and loading weights\n",
    "        current_sequence = decoder_embeddings\n",
    "        decoder_state_outputs = [] # To collect output states from each layer\n",
    "\n",
    "        for i in range(num_decoder_layers):\n",
    "            # Get the corresponding trained LSTM layer\n",
    "            trained_lstm_layer = trained_model.get_layer(f\"decoder_lstm_{i+1}\")\n",
    "\n",
    "            # Create a new LSTM layer configured for inference\n",
    "            inf_decoder_lstm = LSTM(decoder_lstm_units, return_sequences=True, return_state=True, name=f\"inf_decoder_lstm_{i+1}\")\n",
    "\n",
    "            # Build the layer before setting weights (important!)\n",
    "            # Determine input shape for build: sequence comes from previous layer or embedding, states come from input list\n",
    "            if i == 0:\n",
    "                 lstm_input_shape = [decoder_embeddings.shape, decoder_state_inputs[0].shape, decoder_state_inputs[1].shape]\n",
    "            else:\n",
    "                 # Input sequence shape is output sequence shape of previous LSTM\n",
    "                 lstm_input_shape = [current_sequence.shape, decoder_state_inputs[i*2].shape, decoder_state_inputs[i*2+1].shape]\n",
    "            # inf_decoder_lstm.build(input_shape=lstm_input_shape) # Build may not be needed if called directly\n",
    "\n",
    "            # Call the layer, passing the states for this specific layer\n",
    "            # States for layer 'i' are at indices i*2 and i*2+1 in decoder_state_inputs\n",
    "            current_sequence, state_h_out, state_c_out = inf_decoder_lstm(\n",
    "                current_sequence, initial_state=decoder_state_inputs[i*2 : i*2+2]\n",
    "            )\n",
    "\n",
    "            # Set weights *after* the layer has been called/built implicitly\n",
    "            inf_decoder_lstm.set_weights(trained_lstm_layer.get_weights())\n",
    "\n",
    "\n",
    "            # Store the output states for this layer\n",
    "            decoder_state_outputs.extend([state_h_out, state_c_out])\n",
    "\n",
    "        # Final Dense layer (reuse weights)\n",
    "        output_dense_layer = trained_model.get_layer(\"output_layer\")\n",
    "        # Create a new Dense layer with same config\n",
    "        inf_dense_layer = Dense(output_dense_layer.units,\n",
    "                                activation=output_dense_layer.activation,\n",
    "                                name=\"inf_output_layer\")\n",
    "        # Build layer before setting weights\n",
    "        inf_dense_layer.build(input_shape=current_sequence.shape)\n",
    "        # Set weights\n",
    "        inf_dense_layer.set_weights(output_dense_layer.get_weights())\n",
    "        decoder_dense_outputs = inf_dense_layer(current_sequence) # Shape: (batch, 1, vocab_size)\n",
    "\n",
    "        # Define the inference decoder model\n",
    "        inf_decoder = Model(\n",
    "             inputs=[decoder_input_single_token] + decoder_state_inputs,\n",
    "             outputs=[decoder_dense_outputs] + decoder_state_outputs, # Return logits + ALL output states\n",
    "             name=\"inference_decoder\"\n",
    "        )\n",
    "\n",
    "        print(\"Inference Decoder created.\")\n",
    "        # --- Debugging Info ---\n",
    "        print(\"\\n--- Inference Decoder Summary ---\")\n",
    "        inf_decoder.summary(line_length=120)\n",
    "        print(f\"Inference Decoder Inputs: {[inp.name + ': ' + str(inp.shape) for inp in inf_decoder.inputs]}\")\n",
    "        print(f\"Inference Decoder Outputs: {[out.name + ': ' + str(out.shape) for out in inf_decoder.outputs]}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to create inference decoder: {e}\", exc_info=True)\n",
    "        print(f\"Error: Failed to create inference decoder: {e}\")\n",
    "        print(\"Hint: Rebuilding the decoder for inference with correct state flow and weight loading is complex.\")\n",
    "        inf_decoder = None # Ensure it's None on failure\n",
    "\n",
    "    finally:\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "    return inf_encoder, inf_decoder\n",
    "\n",
    "\n",
    "# --- Execution for Block 9 ---\n",
    "inference_encoder = None\n",
    "inference_decoder = None\n",
    "\n",
    "# Load the best model saved by ModelCheckpoint during training\n",
    "if os.path.exists(MODEL_SAVE_PATH):\n",
    "    inference_encoder, inference_decoder = setup_inference_models(\n",
    "        MODEL_SAVE_PATH,\n",
    "        num_decoder_layers=NUM_DECODER_LAYERS, # Pass necessary dimensions\n",
    "        decoder_lstm_units=DECODER_LSTM_UNITS\n",
    "    )\n",
    "    if inference_encoder is None or inference_decoder is None:\n",
    "        print(\"Inference model setup failed.\")\n",
    "else:\n",
    "    print(f\"Skipping inference setup: Trained model not found at {MODEL_SAVE_PATH}\")\n",
    "    print(\"Ensure that training (Block 8) ran successfully and saved the model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b07ede4-556a-421d-b5a9-eae0db7b3e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 11: Evaluation (ROUGE Score) + Execution\n",
    "\n",
    "# --- Function Definitions ---\n",
    "def calculate_rouge_scores(predictions, references):\n",
    "    \"\"\"Calculates ROUGE-1, ROUGE-2, ROUGE-L F1 scores.\"\"\"\n",
    "    logging.info(f\"Calculating ROUGE scores for {len(predictions)} pairs...\")\n",
    "    # --- Debugging Info ---\n",
    "    print(\"\\n--- ROUGE Score Calculation ---\")\n",
    "\n",
    "    if not isinstance(predictions, list) or not isinstance(references, list):\n",
    "         print(\"Error: Predictions and References must be lists.\")\n",
    "         return {}\n",
    "    if not predictions or not references :\n",
    "        print(\"Error: Predictions or References list is empty.\")\n",
    "        return {}\n",
    "    if len(predictions) != len(references):\n",
    "        print(f\"Error: Mismatch in number of predictions ({len(predictions)}) and references ({len(references)}).\")\n",
    "        logging.error(\"ROUGE calculation failed: Mismatched lengths.\")\n",
    "        print(\"-\" * 30)\n",
    "        return {}\n",
    "\n",
    "    # Initialize scorer\n",
    "    try:\n",
    "        # Uses stemming by default which is standard practice\n",
    "        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing RougeScorer: {e}\")\n",
    "        print(\"Make sure the 'rouge-score' library and potentially 'nltk' are installed.\")\n",
    "        return {}\n",
    "\n",
    "    # Aggregate scores\n",
    "    aggregator = scoring.BootstrapAggregator()\n",
    "\n",
    "    print(f\"Calculating scores for {len(predictions)} prediction-reference pairs...\")\n",
    "    skipped_pairs = 0\n",
    "    for i, (pred, ref) in enumerate(tqdm(zip(predictions, references), total=len(predictions), desc=\"ROUGE Calculation\")):\n",
    "        # Ensure inputs are strings\n",
    "        pred_str = str(pred) if pred is not None else \"\"\n",
    "        ref_str = str(ref) if ref is not None else \"\"\n",
    "\n",
    "        # Handle potential empty strings (assign zero score)\n",
    "        if not pred_str or not ref_str:\n",
    "            skipped_pairs += 1\n",
    "            # Create a zero score dictionary matching RougeScorer output structure\n",
    "            zero_scores = {\n",
    "                'rouge1': scoring.Score(precision=0.0, recall=0.0, fmeasure=0.0),\n",
    "                'rouge2': scoring.Score(precision=0.0, recall=0.0, fmeasure=0.0),\n",
    "                'rougeL': scoring.Score(precision=0.0, recall=0.0, fmeasure=0.0),\n",
    "            }\n",
    "            aggregator.add_scores(zero_scores)\n",
    "        else:\n",
    "            try:\n",
    "                 scores = scorer.score(target=ref_str, prediction=pred_str)\n",
    "                 aggregator.add_scores(scores)\n",
    "            except Exception as e:\n",
    "                 print(f\"\\nError scoring pair {i}: Pred='{pred_str[:50]}...', Ref='{ref_str[:50]}...'. Error: {e}\")\n",
    "                 skipped_pairs += 1\n",
    "                 # Add zero scores on error as well\n",
    "                 zero_scores = {\n",
    "                     'rouge1': scoring.Score(precision=0.0, recall=0.0, fmeasure=0.0),\n",
    "                     'rouge2': scoring.Score(precision=0.0, recall=0.0, fmeasure=0.0),\n",
    "                     'rougeL': scoring.Score(precision=0.0, recall=0.0, fmeasure=0.0),\n",
    "                 }\n",
    "                 aggregator.add_scores(zero_scores)\n",
    "\n",
    "\n",
    "    if skipped_pairs > 0:\n",
    "        print(f\"Warning: Skipped or encountered errors for {skipped_pairs} pairs during ROUGE calculation.\")\n",
    "\n",
    "    # Compute aggregated results (confidence intervals available via result.low/mid/high)\n",
    "    try:\n",
    "        result = aggregator.aggregate()\n",
    "    except ValueError as e:\n",
    "         print(f\"Error during ROUGE aggregation: {e}\")\n",
    "         print(\"This might happen if all pairs were skipped or resulted in errors.\")\n",
    "         return {}\n",
    "\n",
    "\n",
    "    # Extract F1 scores (mid represents the point estimate)\n",
    "    rouge_f1_scores = {key: result[key].mid.fmeasure * 100 for key in result} # Convert to percentage\n",
    "\n",
    "    logging.info(f\"ROUGE calculation complete. Scores: {rouge_f1_scores}\")\n",
    "    # --- Debugging Info ---\n",
    "    print(\"\\nAggregated ROUGE F1 Scores (%):\")\n",
    "    for key, score in rouge_f1_scores.items():\n",
    "        print(f\"  {key}: {score:.2f}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    return rouge_f1_scores\n",
    "\n",
    "\n",
    "def evaluate_model_on_set(data_df, text_col, target_col, # Use target_col which has <start>/<end>\n",
    "                          tokenizer_obj, inf_encoder_model, inf_decoder_model,\n",
    "                          max_len_input, max_len_summary, start_token_id, end_token_id,\n",
    "                          num_decoder_layers):\n",
    "    \"\"\"Generates summaries for a given DataFrame slice and calculates ROUGE scores.\"\"\"\n",
    "    logging.info(\"Starting model evaluation...\")\n",
    "    # --- Debugging Info ---\n",
    "    print(\"\\n--- Model Evaluation ---\")\n",
    "\n",
    "    if data_df is None or data_df.empty:\n",
    "        print(\"Error: Input DataFrame for evaluation is empty. Cannot evaluate.\")\n",
    "        logging.error(\"Evaluation failed: Input DataFrame empty.\")\n",
    "        return {}\n",
    "\n",
    "    # Check for required columns\n",
    "    if text_col not in data_df.columns or target_col not in data_df.columns:\n",
    "         print(f\"Error: Missing required columns in evaluation DataFrame: need '{text_col}' and '{target_col}'. Found: {data_df.columns.tolist()}\")\n",
    "         logging.error(f\"Evaluation failed: Missing columns '{text_col}' or '{target_col}'\")\n",
    "         return {}\n",
    "\n",
    "\n",
    "    # Prepare input texts (assuming they are already cleaned if df came from processed_df)\n",
    "    input_texts = data_df[text_col].tolist()\n",
    "\n",
    "    # Prepare reference summaries: Need to strip <start> and <end> tokens for ROUGE\n",
    "    reference_summaries_raw = data_df[target_col].tolist()\n",
    "    reference_summaries_cleaned = []\n",
    "    print(\"Cleaning reference summaries (removing <start>/<end>)...\")\n",
    "    for ref in reference_summaries_raw:\n",
    "        cleaned_ref = str(ref).replace(\"<start>\", \"\").replace(\"<end>\", \"\").strip()\n",
    "        reference_summaries_cleaned.append(cleaned_ref)\n",
    "\n",
    "    print(f\"Prepared {len(input_texts)} input texts and {len(reference_summaries_cleaned)} reference summaries for evaluation.\")\n",
    "\n",
    "    # Generate summaries for all evaluation inputs\n",
    "    generated_summaries = []\n",
    "    print(\"Generating summaries for evaluation set...\")\n",
    "    for text in tqdm(input_texts, desc=\"Generating Evaluation Summaries\"):\n",
    "        # Use the greedy generation function (replace with beam search if implemented)\n",
    "        summary = generate_summary_greedy(\n",
    "            input_text=text, # Pass raw (but cleaned) text\n",
    "            tokenizer_obj=tokenizer_obj,\n",
    "            inf_encoder_model=inf_encoder_model,\n",
    "            inf_decoder_model=inf_decoder_model,\n",
    "            max_len_input=max_len_input,\n",
    "            max_len_summary=max_len_summary,\n",
    "            start_token_id=start_token_id,\n",
    "            end_token_id=end_token_id,\n",
    "            num_decoder_layers=num_decoder_layers\n",
    "        )\n",
    "        # Handle potential errors from generation\n",
    "        if summary.startswith(\"[Error\"):\n",
    "            print(f\"Warning: Generation failed for one input, using empty string for evaluation.\")\n",
    "            generated_summaries.append(\"\") # Append empty string on error\n",
    "        else:\n",
    "             generated_summaries.append(summary)\n",
    "\n",
    "    print(f\"Generated {len(generated_summaries)} summaries.\")\n",
    "    # --- Debugging: Show a few generated vs reference ---\n",
    "    print(\"\\nSample Generated vs Reference Summaries (Evaluation Set):\")\n",
    "    num_samples_to_show = min(3, len(generated_summaries))\n",
    "    for i in range(num_samples_to_show):\n",
    "        print(f\"\\n--- Eval Sample {i+1} ---\")\n",
    "        print(f\"Input Text (start): {str(input_texts[i])[:200]}...\") # Ensure string conversion\n",
    "        print(f\"Reference Summary: {reference_summaries_cleaned[i]}\")\n",
    "        print(f\"Generated Summary: {generated_summaries[i]}\")\n",
    "\n",
    "\n",
    "    # Calculate ROUGE scores\n",
    "    final_rouge_scores = calculate_rouge_scores(generated_summaries, reference_summaries_cleaned)\n",
    "\n",
    "    logging.info(\"Evaluation complete.\")\n",
    "    print(\"-\" * 30)\n",
    "    return final_rouge_scores\n",
    "\n",
    "\n",
    "# --- Execution for Block 11 ---\n",
    "rouge_results = {} # Initialize results dictionary\n",
    "\n",
    "# Evaluate on the validation set created in Block 5.\n",
    "# We need the DataFrame slice corresponding to the validation data.\n",
    "# We need the original 'processed_df' before shuffling for consistent indices,\n",
    "# OR reconstruct the val df from the numpy arrays if 'processed_df' was modified/deleted.\n",
    "\n",
    "# Let's try to reconstruct from numpy arrays (assuming they exist from Block 5)\n",
    "if ('encoder_input_val' in locals() and encoder_input_val.size > 0 and\n",
    "    'decoder_target_val' in locals() and decoder_target_val.size > 0 and # Use decoder target for reference summary IDs\n",
    "    'tokenizer' in locals() and tokenizer):\n",
    "\n",
    "    print(\"\\nReconstructing validation DataFrame subset for evaluation...\")\n",
    "    # Detokenize encoder inputs to get the original 'cleaned_text' (approx)\n",
    "    # This might not be perfect due to truncation/padding\n",
    "    # For accurate evaluation, it's better to save the test split *before* tokenization.\n",
    "    # Let's proceed with detokenization, acknowledging potential inaccuracies.\n",
    "\n",
    "    # Detokenize the validation encoder inputs - this is computationally expensive!\n",
    "    # val_texts_detokenized = [detokenize_sequences(seq, tokenizer) for seq in tqdm(encoder_input_val, desc=\"Detokenizing val inputs\")]\n",
    "    # print(\"Warning: Evaluating on detokenized validation inputs, which might differ slightly from original text due to tokenization limits.\")\n",
    "\n",
    "    # Alternative: If 'processed_df' still exists and indices were tracked/saved, use that.\n",
    "    # Assuming we don't have the original text readily available, we'll use the reference summaries.\n",
    "\n",
    "    # Detokenize the validation *target* summaries (which have start/end)\n",
    "    val_summaries_detokenized = [detokenize_sequences(seq, tokenizer) for seq in tqdm(decoder_target_val, desc=\"Detokenizing val targets\")]\n",
    "\n",
    "    # Create a temporary DataFrame for evaluation\n",
    "    # We need input texts. Since detokenizing encoder inputs is slow/lossy,\n",
    "    # let's *skip* providing the original text source for evaluation printouts,\n",
    "    # and focus only on comparing generated vs reference summaries.\n",
    "    # We will generate summaries based on the *tokenized* validation encoder input.\n",
    "\n",
    "    print(\"Generating summaries for validation set using tokenized inputs...\")\n",
    "    generated_val_summaries = []\n",
    "    skipped_val_gen = 0\n",
    "\n",
    "    # Check if inference models are ready\n",
    "    if ('inference_encoder' in locals() and inference_encoder and\n",
    "        'inference_decoder' in locals() and inference_decoder):\n",
    "\n",
    "        # Need to iterate through validation *encoder inputs* and generate\n",
    "        for i, enc_input_seq in enumerate(tqdm(encoder_input_val, desc=\"Generating Val Summaries\")):\n",
    "             # We need to feed the *encoder input sequence* to the encoder,\n",
    "             # then use the states to start greedy search.\n",
    "             # The `generate_summary_greedy` expects raw text, so we adapt the logic here.\n",
    "\n",
    "             # 1. Encode the input sequence\n",
    "             try:\n",
    "                  # Reshape sequence for batch dimension (1, max_len_input)\n",
    "                  input_seq_batch = np.expand_dims(enc_input_seq, axis=0)\n",
    "                  initial_encoder_states = inference_encoder.predict(input_seq_batch, verbose=0)\n",
    "                  # Initialize decoder states (same logic as in generate_summary_greedy)\n",
    "                  decoder_states_value = initial_encoder_states * NUM_DECODER_LAYERS\n",
    "             except Exception as e:\n",
    "                  print(f\"Error encoding val sequence {i}: {e}\")\n",
    "                  generated_val_summaries.append(\"[Error: Encoding failed]\")\n",
    "                  skipped_val_gen += 1\n",
    "                  continue\n",
    "\n",
    "             # 2. Initialize Decoder Input\n",
    "             target_seq = np.array([[START_ID]])\n",
    "\n",
    "             # 3. Greedy Decoding Loop (Simplified version of the function's loop)\n",
    "             decoded_tokens_val = []\n",
    "             for step in range(MAX_LEN_SUMMARY):\n",
    "                 try:\n",
    "                      decoder_inputs = [target_seq] + decoder_states_value\n",
    "                      decoder_outputs = inference_decoder.predict(decoder_inputs, verbose=0)\n",
    "                      output_tokens_logits = decoder_outputs[0]\n",
    "                      new_states = decoder_outputs[1:]\n",
    "                 except Exception as e:\n",
    "                      print(f\"Error during val decoder prediction step {step+1} for sequence {i}: {e}\")\n",
    "                      decoded_tokens_val.append(END_ID) # Try to end gracefully\n",
    "                      break\n",
    "\n",
    "                 sampled_token_id = np.argmax(output_tokens_logits[0, -1, :])\n",
    "                 if sampled_token_id == END_ID:\n",
    "                      break\n",
    "                 if sampled_token_id != START_ID and sampled_token_id != PAD_ID:\n",
    "                    decoded_tokens_val.append(sampled_token_id)\n",
    "\n",
    "                 target_seq = np.array([[sampled_token_id]])\n",
    "                 if len(new_states) == len(decoder_states_value):\n",
    "                    decoder_states_value = new_states\n",
    "                 else:\n",
    "                    print(f\"State mismatch during val generation {i}, step {step+1}\")\n",
    "                    break\n",
    "                 if len(decoded_tokens_val) >= MAX_LEN_SUMMARY:\n",
    "                     break\n",
    "\n",
    "             # 4. Detokenize\n",
    "             summary = detokenize_sequences(np.array(decoded_tokens_val), tokenizer)\n",
    "             generated_val_summaries.append(summary)\n",
    "\n",
    "        if skipped_val_gen > 0:\n",
    "            print(f\"Skipped generation for {skipped_val_gen} validation samples due to errors.\")\n",
    "\n",
    "        # --- Debugging: Show a few generated vs reference ---\n",
    "        print(\"\\nSample Generated vs Reference Summaries (Validation Set):\")\n",
    "        num_samples_to_show = min(3, len(generated_val_summaries))\n",
    "        for i in range(num_samples_to_show):\n",
    "            print(f\"\\n--- Val Sample {i+1} ---\")\n",
    "            # print(f\"Input Text (Detokenized - Approx): {val_texts_detokenized[i][:200]}...\") # If available\n",
    "            print(f\"Reference Summary (Detokenized): {val_summaries_detokenized[i]}\")\n",
    "            print(f\"Generated Summary: {generated_val_summaries[i]}\")\n",
    "\n",
    "        # Calculate ROUGE scores using the generated summaries and the detokenized references\n",
    "        rouge_results = calculate_rouge_scores(generated_val_summaries, val_summaries_detokenized)\n",
    "\n",
    "        print(\"\\n--- Final Evaluation ROUGE Scores (on Validation Set) ---\")\n",
    "        print(rouge_results)\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "    else:\n",
    "        print(\"Skipping validation set generation: Missing inference models.\")\n",
    "\n",
    "elif not ('encoder_input_val' in locals() and encoder_input_val.size > 0):\n",
    "     print(\"Skipping evaluation: Validation input data not found (Block 5 might have failed or was not run).\")\n",
    "elif not ('decoder_target_val' in locals() and decoder_target_val.size > 0):\n",
    "      print(\"Skipping evaluation: Validation target data not found (Block 5 might have failed or was not run).\")\n",
    "elif not ('tokenizer' in locals() and tokenizer):\n",
    "      print(\"Skipping evaluation: Tokenizer not found (Block 4 might have failed or was not run).\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-tensorflow-tensorflow",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "TensorFlow 2-11 (Local)",
   "language": "python",
   "name": "conda-env-tensorflow-tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
