{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4ca4245-5128-43fd-8075-e531d910b5c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Phase 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b502306-cc58-435c-a3ab-4545c0370f9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: /home/jupyter/V4_seq2seq/notebooks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "print(f\"Current Working Directory: {cwd}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58a49769-9ce0-44bc-89be-ace9f22cec1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Setup ---\n",
      "TensorFlow Version: 2.11.0\n",
      "SentencePiece Version: 0.2.0\n",
      "Num GPUs Available: 1\n",
      "GPU memory growth enabled.\n",
      "Parameters defined using relative paths.\n",
      "Checking DATA_PATH: ../data/mergedt04.jsonl - Exists: True\n",
      "Checking GLOVE_PATH: ../data/glove.6B/glove.6B.100d.txt - Exists: True\n"
     ]
    }
   ],
   "source": [
    "# Phase 1: Setup (Revised Paths)\n",
    "\n",
    "# --- Standard & ML Libraries ---\n",
    "# (Keep all imports the same)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, LSTM, GRU, Embedding, Dense, Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import sentencepiece as spm\n",
    "from langdetect import detect, LangDetectException\n",
    "import os\n",
    "import traceback\n",
    "\n",
    "print(f\"--- Setup ---\")\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "print(f\"SentencePiece Version: {spm.__version__}\")\n",
    "\n",
    "# -- GPU Check & Configuration --\n",
    "# (Keep GPU check code the same)\n",
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpu_devices:\n",
    "    print(f\"Num GPUs Available: {len(gpu_devices)}\")\n",
    "    try:\n",
    "        for gpu in gpu_devices: tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"GPU memory growth enabled.\")\n",
    "    except RuntimeError as e: print(f\"Could not set memory growth: {e}\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected by TensorFlow. Training will proceed on CPU.\")\n",
    "\n",
    "# --- Parameters ---\n",
    "# Project Base Directory (relative to notebook location)\n",
    "BASE_DIR = \"..\" # Goes up one level from 'notebooks/' to 'pib_summarizer/'\n",
    "\n",
    "\n",
    "# File Paths (relative to BASE_DIR now)\n",
    "DATA_PATH = os.path.join(BASE_DIR, 'data', 'mergedt04.jsonl')\n",
    "GLOVE_PATH = os.path.join(BASE_DIR, 'data', 'glove.6B', 'glove.6B.100d.txt') # <--- Verify filename and 100d\n",
    "\n",
    "# SentencePiece Parameters\n",
    "SP_MODEL_PREFIX = os.path.join(BASE_DIR, 'models', 'pib_spm_70k') # Save model in 'models' folder\n",
    "SP_VOCAB_SIZE = 16000\n",
    "SP_TEMP_INPUT_FILE = os.path.join(BASE_DIR, 'spm_training_data_70k.txt') # Temp file in base project dir\n",
    "\n",
    "# Embedding Parameters\n",
    "EMBEDDING_DIM = 100        # *** MUST MATCH the dimension of your GloVe file (e.g., 100 for 100d) ***\n",
    "\n",
    "# Sequence Lengths\n",
    "MAXLEN_INPUT = 1000\n",
    "MAXLEN_OUTPUT = 200\n",
    "\n",
    "# Model Hyperparameters\n",
    "LSTM_UNITS = 256\n",
    "\n",
    "# Training Parameters\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "VALIDATION_SPLIT = 0.15\n",
    "LEARNING_RATE = 0.001\n",
    "EARLY_STOPPING_PATIENCE = 3\n",
    "\n",
    "# Scheduled Sampling (Placeholder)\n",
    "INITIAL_SCHEDULED_SAMPLING_PROB = 1.0\n",
    "SCHEDULED_SAMPLING_K = 10.0\n",
    "\n",
    "# Special tokens managed by SentencePiece: UNK_ID=0, BOS_ID=1 (start), EOS_ID=2 (end)\n",
    "# Keras Padding ID will be 0.\n",
    "START_TOKEN = '<s>'\n",
    "END_TOKEN = '</s>'\n",
    "\n",
    "# Word count limits for filtering AFTER cleaning\n",
    "MIN_TEXT_WORDS = 15\n",
    "MIN_SUMMARY_WORDS = 7\n",
    "\n",
    "# File paths for saving artifacts (relative to BASE_DIR)\n",
    "SP_MODEL_PATH = f'{SP_MODEL_PREFIX}.model' # Will be BASE_DIR/models/pib_spm_70k.model\n",
    "EMBEDDING_MATRIX_PATH = os.path.join(BASE_DIR, 'models', f'embedding_matrix_{SP_VOCAB_SIZE}v_{EMBEDDING_DIM}d.npy')\n",
    "MODEL_SAVE_PATH = os.path.join(BASE_DIR, 'models', f'adv_seq2seq_{SP_VOCAB_SIZE}v_{LSTM_UNITS}u_no_attention.keras')\n",
    "HISTORY_PLOT_PATH = os.path.join(BASE_DIR, 'output', f'adv_training_history_{SP_VOCAB_SIZE}v.png') # Save plot in 'output'\n",
    "\n",
    "print(\"Parameters defined using relative paths.\")\n",
    "# Check if paths exist now\n",
    "print(f\"Checking DATA_PATH: {DATA_PATH} - Exists: {os.path.exists(DATA_PATH)}\")\n",
    "print(f\"Checking GLOVE_PATH: {GLOVE_PATH} - Exists: {os.path.exists(GLOVE_PATH)}\")\n",
    "if not os.path.exists(GLOVE_PATH):\n",
    "     print(f\"CRITICAL WARNING: GloVe file NOT FOUND at the specified path!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644eb5a1-2189-4f40-a6eb-261dedfc0f7a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Phase 2: Data Loading & Initial Preprocessing (Adapted for SP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "754f2e45-e5d4-4694-bce3-85acc322611b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Phase 2: Data Loading & Initial Preprocessing ---\n",
      "Loading and processing data from ../data/mergedt04.jsonl...\n",
      "  Processed 5000 lines...\n",
      "  Processed 10000 lines...\n",
      "  Processed 15000 lines...\n",
      "  Processed 20000 lines...\n",
      "  Processed 25000 lines...\n",
      "  Processed 30000 lines...\n",
      "  Processed 35000 lines...\n",
      "  Processed 40000 lines...\n",
      "  Processed 45000 lines...\n",
      "  Processed 50000 lines...\n",
      "  Processed 55000 lines...\n",
      "  Processed 60000 lines...\n",
      "  Processed 65000 lines...\n",
      "  Processed 70000 lines...\n",
      "\n",
      "--- Data Loading & Initial Cleaning Summary ---\n",
      "Total lines read: 74128\n",
      "Valid English records loaded & cleaned: 73935\n",
      "Skipped due to JSON Error: 0\n",
      "Skipped due to Missing Keys ('extracted_text'/'gemini_summary'): 44\n",
      "Skipped due to Invalid Data Types (not str): 0\n",
      "Skipped due to Empty Content (after strip): 0\n",
      "Skipped non-English / lang detect fail: 17\n",
      "Skipped records failing length filter: 132\n",
      "\n",
      "Preparing text file '../spm_training_data_70k.txt' for SentencePiece training...\n",
      "SentencePiece training file created with approx 147870 non-empty lines.\n",
      "\n",
      "Adding start/end tokens to DataFrame...\n",
      "Final DataFrame 'df_model' created for modeling.\n",
      "\n",
      "Initial Preprocessing Complete.\n",
      "Final DataFrame size for modeling: 73935\n",
      "\n",
      "Sample final data (first 3 rows):\n",
      "                                                                encoder_input_text  \\\n",
      "0  ministry of housing urban affairs states can flexibly use of central assista...   \n",
      "1  ministry of culture consultative committee of the ministry of tourism and mi...   \n",
      "2  ministry of tourism consultative committee of the ministry of tourism and mi...   \n",
      "\n",
      "                                                                decoder_input_text  \\\n",
      "0  <s> a meeting of the parliamentary consultative committee discussed shortcom...   \n",
      "1  <s> a meeting of the parliamentary consultative committee of the ministries ...   \n",
      "2  <s> a meeting of the parliamentary consultative committee of the ministries ...   \n",
      "\n",
      "                                                               decoder_target_text  \n",
      "0  a meeting of the parliamentary consultative committee discussed shortcomings...  \n",
      "1  a meeting of the parliamentary consultative committee of the ministries of t...  \n",
      "2  a meeting of the parliamentary consultative committee of the ministries of t...  \n"
     ]
    }
   ],
   "source": [
    "# Phase 2: Data Loading & Initial Preprocessing (Corrected Final Selection)\n",
    "\n",
    "print(f\"\\n--- Phase 2: Data Loading & Initial Preprocessing ---\")\n",
    "\n",
    "# --- Preprocessing Function (Simplified for SentencePiece Training) ---\n",
    "def initial_preprocess(text):\n",
    "    \"\"\"Basic cleaning suitable BEFORE SentencePiece tokenization.\"\"\"\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    text = text.lower()\n",
    "    text = text.replace('\\u2018', \"'\").replace('\\u2019', \"'\")\n",
    "    text = text.replace('\\u201c', '\"').replace('\\u201d', '\"')\n",
    "    text = re.sub(r'c:\\\\users\\\\.*?(\\.jpg|\\.png|\\.doc|\\.docx)\\b', ' ', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', ' ', text)\n",
    "    text = re.sub(r'\\[\\d+\\]', ' ', text)\n",
    "    # Keep headers/footers for now, let SP handle\n",
    "    # text = re.sub(r'^ministry of [^\\n]+\\n?', '', text)\n",
    "    # text = re.sub(r'\\*+[\\n\\s]+[a-z]{2}/[a-z]{2,3}\\s*$', '', text)\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = re.sub(r'[^\\w\\s.,!?\"\\'\\-]', ' ', text) # Keep word chars, whitespace, basic punc\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "# --- Main Loading Logic ---\n",
    "all_records = []\n",
    "df_model = None\n",
    "skipped_json_error = 0\n",
    "skipped_missing_key = 0\n",
    "skipped_invalid_type = 0\n",
    "skipped_empty_content = 0\n",
    "skipped_language = 0\n",
    "skipped_short = 0\n",
    "line_count = 0\n",
    "sp_file_created = False # Flag to track if SP input file was created\n",
    "\n",
    "try:\n",
    "    if not os.path.exists(DATA_PATH):\n",
    "        raise FileNotFoundError(f\"Input data file not found at {DATA_PATH}\")\n",
    "\n",
    "    print(f\"Loading and processing data from {DATA_PATH}...\")\n",
    "    with open(DATA_PATH, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            line_count = i + 1\n",
    "            if line_count % 5000 == 0: print(f\"  Processed {line_count} lines...\")\n",
    "\n",
    "            record = None\n",
    "            try:\n",
    "                record = json.loads(line)\n",
    "                if not isinstance(record, dict): raise ValueError(\"Line not dict\")\n",
    "\n",
    "                raw_text = record.get('extracted_text')\n",
    "                raw_summary = record.get('gemini_summary')\n",
    "\n",
    "                if raw_text is None or raw_summary is None: skipped_missing_key += 1; continue\n",
    "                if not (isinstance(raw_text, str) and isinstance(raw_summary, str)): skipped_invalid_type += 1; continue\n",
    "                if not raw_text.strip() or not raw_summary.strip(): skipped_empty_content += 1; continue\n",
    "\n",
    "                # Language Detection\n",
    "                try:\n",
    "                    sample_text = raw_text[:1000]\n",
    "                    if len(sample_text) < 50: lang = 'en'\n",
    "                    else: lang = detect(sample_text)\n",
    "                    if lang != 'en': skipped_language += 1; continue\n",
    "                except: skipped_language += 1; continue\n",
    "\n",
    "                cleaned_input_text = initial_preprocess(raw_text)\n",
    "                cleaned_summary = initial_preprocess(raw_summary)\n",
    "\n",
    "                input_word_count = len(cleaned_input_text.split())\n",
    "                summary_word_count = len(cleaned_summary.split())\n",
    "                if not (input_word_count >= MIN_TEXT_WORDS and summary_word_count >= MIN_SUMMARY_WORDS):\n",
    "                    skipped_short += 1; continue\n",
    "\n",
    "                all_records.append({\n",
    "                     'encoder_input_text': cleaned_input_text, # Store cleaned text\n",
    "                     'cleaned_summary': cleaned_summary,     # Store cleaned summary\n",
    "                 })\n",
    "\n",
    "            except json.JSONDecodeError: skipped_json_error += 1\n",
    "            except Exception: skipped_invalid_type += 1 # Or a more general error counter\n",
    "\n",
    "    print(f\"\\n--- Data Loading & Initial Cleaning Summary ---\")\n",
    "    # (Keep print statements for skipped counts)\n",
    "    print(f\"Total lines read: {line_count}\")\n",
    "    print(f\"Valid English records loaded & cleaned: {len(all_records)}\")\n",
    "    print(f\"Skipped due to JSON Error: {skipped_json_error}\")\n",
    "    print(f\"Skipped due to Missing Keys ('extracted_text'/'gemini_summary'): {skipped_missing_key}\")\n",
    "    print(f\"Skipped due to Invalid Data Types (not str): {skipped_invalid_type}\")\n",
    "    print(f\"Skipped due to Empty Content (after strip): {skipped_empty_content}\")\n",
    "    print(f\"Skipped non-English / lang detect fail: {skipped_language}\")\n",
    "    print(f\"Skipped records failing length filter: {skipped_short}\")\n",
    "\n",
    "\n",
    "    if not all_records: raise ValueError(\"CRITICAL: No valid data records remain.\")\n",
    "\n",
    "    # --- Convert to DataFrame ---\n",
    "    df = pd.DataFrame(all_records)\n",
    "    del all_records # Free memory\n",
    "\n",
    "    # --- Prepare text file for SentencePiece training ---\n",
    "    # This needs the cleaned columns from the DataFrame 'df'\n",
    "    print(f\"\\nPreparing text file '{SP_TEMP_INPUT_FILE}' for SentencePiece training...\")\n",
    "    lines_written_sp = 0\n",
    "    try:\n",
    "        with open(SP_TEMP_INPUT_FILE, 'w', encoding='utf-8') as f:\n",
    "            # Write encoder input text\n",
    "            for text in df['encoder_input_text']:\n",
    "                if isinstance(text, str) and text.strip():\n",
    "                     f.write(text + '\\n')\n",
    "                     lines_written_sp += 1\n",
    "            # Write cleaned summary text\n",
    "            for text in df['cleaned_summary']:\n",
    "                if isinstance(text, str) and text.strip():\n",
    "                     f.write(text + '\\n')\n",
    "                     lines_written_sp += 1\n",
    "        print(f\"SentencePiece training file created with approx {lines_written_sp} non-empty lines.\")\n",
    "        sp_file_created = True\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR creating SentencePiece training file: {e}\")\n",
    "        sp_file_created = False\n",
    "\n",
    "\n",
    "    # --- Finalize DataFrame for Modeling ---\n",
    "    # ** CORRECTED PART **\n",
    "    # Create the tagged columns FIRST, using the 'cleaned_summary' column which exists in 'df'\n",
    "    print(\"\\nAdding start/end tokens to DataFrame...\")\n",
    "    # Make sure START_TOKEN and END_TOKEN are defined (they should be from Phase 1)\n",
    "    df['decoder_input_text'] = START_TOKEN + ' ' + df['cleaned_summary']\n",
    "    df['decoder_target_text'] = df['cleaned_summary'] + ' ' + END_TOKEN\n",
    "\n",
    "    # NOW select the final columns needed for the next phases into df_model\n",
    "    # We need 'encoder_input_text' for input tokenizer/padding (Phase 4)\n",
    "    # We need 'decoder_input_text' and 'decoder_target_text' for output tokenizer/padding (Phase 4)\n",
    "    # We do NOT need 'cleaned_summary' anymore in df_model itself after creating the tagged versions\n",
    "    df_model = df[['encoder_input_text', 'decoder_input_text', 'decoder_target_text']].copy()\n",
    "    print(\"Final DataFrame 'df_model' created for modeling.\")\n",
    "    # --- END CORRECTION ---\n",
    "\n",
    "    del df # Free memory from the intermediate DataFrame\n",
    "\n",
    "    print(\"\\nInitial Preprocessing Complete.\")\n",
    "    print(f\"Final DataFrame size for modeling: {len(df_model)}\")\n",
    "    if not df_model.empty:\n",
    "        print(\"\\nSample final data (first 3 rows):\")\n",
    "        pd.set_option('display.max_colwidth', 80)\n",
    "        print(df_model.head(3))\n",
    "    else:\n",
    "        print(\"CRITICAL WARNING: DataFrame 'df_model' is empty after final selection!\")\n",
    "\n",
    "\n",
    "except FileNotFoundError: print(f\"ERROR: Data file not found at {DATA_PATH}. Please check the path.\"); df_model = None\n",
    "except ValueError as ve: print(ve); df_model = None\n",
    "except Exception as e:\n",
    "    print(f\"An critical error occurred during data loading/preprocessing: {e}\")\n",
    "    traceback.print_exc(); df_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1dd786-d3a6-4bcf-ac46-8debf687d8c3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Phase 3: SentencePiece Tokenizer Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d64a414-4725-4798-96fa-70b7d2b15e75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Phase 3: Training SentencePiece Tokenizer ---\n",
      "SentencePiece model '../models/pib_spm_70k.model' already exists. Skipping training.\n"
     ]
    }
   ],
   "source": [
    "# Phase 3: SentencePiece Tokenizer Training (Using Relative Paths)\n",
    "\n",
    "print(f\"\\n--- Phase 3: Training SentencePiece Tokenizer ---\")\n",
    "\n",
    "sp_model_path_check = f\"{SP_MODEL_PREFIX}.model\" # Path includes directory e.g., ../models/pib_spm_70k.model\n",
    "\n",
    "# Check if prerequisites are available\n",
    "sp_file_created = 'sp_file_created' in locals() and sp_file_created\n",
    "if sp_file_created and os.path.exists(SP_TEMP_INPUT_FILE):\n",
    "    try:\n",
    "        # Check if model already exists to avoid retraining unnecessarily\n",
    "        if os.path.exists(sp_model_path_check):\n",
    "             print(f\"SentencePiece model '{sp_model_path_check}' already exists. Skipping training.\")\n",
    "        else:\n",
    "            print(f\"Training SentencePiece model...\")\n",
    "            print(f\"  Input file: {SP_TEMP_INPUT_FILE}\")\n",
    "            print(f\"  Model prefix: {SP_MODEL_PREFIX} (Output directory: {os.path.dirname(SP_MODEL_PREFIX)})\")\n",
    "            print(f\"  Vocab size: {SP_VOCAB_SIZE}\")\n",
    "\n",
    "            # Ensure the output directory for the model exists\n",
    "            os.makedirs(os.path.dirname(SP_MODEL_PREFIX), exist_ok=True)\n",
    "\n",
    "            # Define SentencePiece training arguments using variables from Phase 1\n",
    "            # START_TOKEN and END_TOKEN strings are used for bos/eos piece representation\n",
    "            spm_command = (\n",
    "                f'--input={SP_TEMP_INPUT_FILE} '\n",
    "                f'--model_prefix={SP_MODEL_PREFIX} ' # Includes path to models dir\n",
    "                f'--vocab_size={SP_VOCAB_SIZE} '\n",
    "                f'--model_type=unigram '\n",
    "                f'--character_coverage=1.0 '\n",
    "                f'--unk_id=0 '\n",
    "                f'--bos_id=1 ' # This ID will be used for START_TOKEN\n",
    "                f'--eos_id=2 ' # This ID will be used for END_TOKEN\n",
    "                f'--pad_id=-1 ' # SP internal ignore ID, Keras uses 0 for padding later\n",
    "                f'--unk_piece=<unk> '\n",
    "                f'--bos_piece={START_TOKEN} ' # Visual representation in vocab file\n",
    "                f'--eos_piece={END_TOKEN} ' # Visual representation in vocab file\n",
    "                f'--remove_extra_whitespaces=true '\n",
    "                f'--normalization_rule_name=nmt_nfkc_cf'\n",
    "            )\n",
    "\n",
    "            # Train the model\n",
    "            spm.SentencePieceTrainer.train(spm_command)\n",
    "\n",
    "            print(f\"\\nSentencePiece training complete.\")\n",
    "            print(f\"Model saved to: {SP_MODEL_PREFIX}.model\")\n",
    "            print(f\"Vocabulary saved to: {SP_MODEL_PREFIX}.vocab\")\n",
    "\n",
    "        # Optional: Clean up the temporary training file now that SP model is trained\n",
    "        # try:\n",
    "        #     if os.path.exists(SP_TEMP_INPUT_FILE):\n",
    "        #          os.remove(SP_TEMP_INPUT_FILE)\n",
    "        #          print(f\"Removed temporary file: {SP_TEMP_INPUT_FILE}\")\n",
    "        # except OSError as e:\n",
    "        #     print(f\"Warning: Could not remove temporary file {SP_TEMP_INPUT_FILE}: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during SentencePiece training: {e}\")\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    if not sp_file_created:\n",
    "         print(\"Skipping SentencePiece training because the temporary input file was not created successfully.\")\n",
    "    elif not os.path.exists(SP_TEMP_INPUT_FILE):\n",
    "         print(f\"Skipping SentencePiece training because the temporary input file '{SP_TEMP_INPUT_FILE}' does not exist.\")\n",
    "    else:\n",
    "         print(\"Skipping SentencePiece training due to other missing prerequisites.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6e9459-759d-4e74-8d37-4e955c508524",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Phase 4: Tokenization & Embedding Matrix Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffa1a995-d8d7-4387-899a-43c12f51c491",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Phase 4: Tokenization using SentencePiece & Embedding Matrix Prep ---\n",
      "Loading SentencePiece model from ../models/pib_spm_70k.model...\n",
      "SentencePiece model loaded. Actual Vocabulary size: 16000\n",
      "  UNK ID: 0, BOS ID (start): 1, EOS ID (end): 2\n",
      "\n",
      "Tokenizing text data using loaded SP model...\n",
      "  Tokenizing encoder input...\n",
      "  Tokenizing decoder input...\n",
      "  Tokenizing decoder target...\n",
      "Tokenization complete.\n",
      "  Sample encoder sequence[0]: [21, 5, 615, 239, 85, 76, 229, 11785, 3143, 176, 325, 5, 74, 250, 5]...\n",
      "  Sample decoder input sequence[0]: [60, 0, 10, 0, 12, 96, 5, 3, 1159, 2660, 142, 637, 10820, 9, 3]... (Should start with BOS ID: 1)\n",
      "  Sample decoder target sequence[0]: [12, 96, 5, 3, 1159, 2660, 142, 637, 10820, 9, 3, 3313, 2217, 239, 554]... (Should end with EOS ID: 2)\n",
      "\n",
      "Padding sequences...\n",
      "Padding complete.\n",
      "  Padded Encoder Input Shape: (73935, 1000)\n",
      "  Padded Decoder Input Shape: (73935, 200)\n",
      "  Padded Decoder Target Shape: (73935, 200)\n",
      "\n",
      "Loading GloVe embeddings from: ../data/glove.6B/glove.6B.100d.txt\n",
      "  Reading GloVe file (can take a minute)...\n",
      "    Processed 100000 GloVe lines...\n",
      "    Processed 200000 GloVe lines...\n",
      "    Processed 300000 GloVe lines...\n",
      "    Processed 400000 GloVe lines...\n",
      "  Found 400000 word vectors of dimension 100 in GloVe file.\n",
      "\n",
      "Creating embedding matrix with shape (16000, 100)...\n",
      "Embedding matrix created.\n",
      "  GloVe vectors mapped (Hits): 2357\n",
      "  SP Tokens not in GloVe (Misses, incl <unk>,<s>,</s>): 13643\n",
      "  Padding index 0 vector explicitly zeroed out.\n",
      "\n",
      "Saving embedding matrix to '../models/embedding_matrix_16000v_100d.npy'...\n",
      "Embedding matrix saved.\n"
     ]
    }
   ],
   "source": [
    "# Phase 4: Tokenization using SentencePiece & Embedding Matrix Prep\n",
    "\n",
    "print(f\"\\n--- Phase 4: Tokenization using SentencePiece & Embedding Matrix Prep ---\")\n",
    "\n",
    "# Initialize variables for this phase's outputs\n",
    "encoder_input_padded, decoder_input_padded, decoder_target_padded = None, None, None\n",
    "sp = None\n",
    "embedding_matrix = None\n",
    "actual_sp_vocab_size = 0 # Use this for Embedding layer dim later\n",
    "\n",
    "# Check if prerequisites from previous phases are available\n",
    "# SP_MODEL_PATH was defined in Phase 1 using SP_MODEL_PREFIX\n",
    "if ('df_model' in locals() and df_model is not None and not df_model.empty and\n",
    "    'SP_MODEL_PATH' in locals() and os.path.exists(SP_MODEL_PATH) and\n",
    "    'GLOVE_PATH' in locals() and os.path.exists(GLOVE_PATH)):\n",
    "    try:\n",
    "        # --- 1. Load SentencePiece Model ---\n",
    "        print(f\"Loading SentencePiece model from {SP_MODEL_PATH}...\")\n",
    "        sp = spm.SentencePieceProcessor()\n",
    "        sp.load(SP_MODEL_PATH)\n",
    "        actual_sp_vocab_size = sp.get_piece_size() # Get actual vocab size from loaded model\n",
    "        if actual_sp_vocab_size <= 0:\n",
    "             raise ValueError(\"Loaded SentencePiece model has zero or negative vocabulary size!\")\n",
    "        print(f\"SentencePiece model loaded. Actual Vocabulary size: {actual_sp_vocab_size}\")\n",
    "        print(f\"  UNK ID: {sp.unk_id()}, BOS ID (start): {sp.bos_id()}, EOS ID (end): {sp.eos_id()}\")\n",
    "\n",
    "        # --- 2. Tokenize Data using loaded SP model ---\n",
    "        print(\"\\nTokenizing text data using loaded SP model...\")\n",
    "        # Check if the required columns exist in df_model (created at the end of Phase 2)\n",
    "        required_cols = ['encoder_input_text', 'decoder_input_text', 'decoder_target_text']\n",
    "        if not all(col in df_model.columns for col in required_cols):\n",
    "             raise KeyError(f\"Missing one or more required columns in df_model: {required_cols}\")\n",
    "\n",
    "        # Use SentencePiece to encode the text columns into integer IDs\n",
    "        # Using list comprehension for potentially better performance than pandas apply\n",
    "        print(\"  Tokenizing encoder input...\")\n",
    "        encoder_input_sequences = [sp.encode(text, out_type=int) for text in df_model['encoder_input_text']]\n",
    "\n",
    "        print(\"  Tokenizing decoder input...\")\n",
    "        # The 'decoder_input_text' column already has START_TOKEN prepended as a string\n",
    "        # SentencePiece encoding will handle converting START_TOKEN ('<s>') to its BOS ID (1)\n",
    "        decoder_input_sequences = [sp.encode(text, out_type=int) for text in df_model['decoder_input_text']]\n",
    "\n",
    "        print(\"  Tokenizing decoder target...\")\n",
    "        # The 'decoder_target_text' column already has END_TOKEN appended as a string\n",
    "        # SentencePiece encoding will handle converting END_TOKEN ('</s>') to its EOS ID (2)\n",
    "        decoder_target_sequences = [sp.encode(text, out_type=int) for text in df_model['decoder_target_text']]\n",
    "\n",
    "        print(\"Tokenization complete.\")\n",
    "        # Print samples to verify tokenization, especially start/end IDs\n",
    "        if encoder_input_sequences: print(f\"  Sample encoder sequence[0]: {encoder_input_sequences[0][:15]}...\")\n",
    "        if decoder_input_sequences: print(f\"  Sample decoder input sequence[0]: {decoder_input_sequences[0][:15]}... (Should start with BOS ID: {sp.bos_id()})\")\n",
    "        if decoder_target_sequences: print(f\"  Sample decoder target sequence[0]: {decoder_target_sequences[0][:15]}... (Should end with EOS ID: {sp.eos_id()})\")\n",
    "\n",
    "\n",
    "        # --- 3. Padding ---\n",
    "        print(\"\\nPadding sequences...\")\n",
    "        # Pad sequences using 0. This is the standard value Keras Embedding expects for masking.\n",
    "        # SentencePiece uses ID 0 for <unk>, ID 1 for <s> (BOS), ID 2 for </s> (EOS).\n",
    "        # So, padding with 0 means we are padding with the <unk> ID, which is acceptable,\n",
    "        # as the Embedding layer's mask_zero=True will ignore these 0s anyway.\n",
    "        encoder_input_padded = pad_sequences(encoder_input_sequences, maxlen=MAXLEN_INPUT, padding='post', truncating='post', value=0)\n",
    "        decoder_input_padded = pad_sequences(decoder_input_sequences, maxlen=MAXLEN_OUTPUT, padding='post', truncating='post', value=0)\n",
    "        decoder_target_padded = pad_sequences(decoder_target_sequences, maxlen=MAXLEN_OUTPUT, padding='post', truncating='post', value=0)\n",
    "        # Clear large intermediate lists to free memory\n",
    "        del encoder_input_sequences, decoder_input_sequences, decoder_target_sequences\n",
    "        print(\"Padding complete.\")\n",
    "        print(f\"  Padded Encoder Input Shape: {encoder_input_padded.shape}\")\n",
    "        print(f\"  Padded Decoder Input Shape: {decoder_input_padded.shape}\")\n",
    "        print(f\"  Padded Decoder Target Shape: {decoder_target_padded.shape}\")\n",
    "\n",
    "\n",
    "        # --- 4. Prepare GloVe Embedding Matrix ---\n",
    "        print(f\"\\nLoading GloVe embeddings from: {GLOVE_PATH}\")\n",
    "        embeddings_index = {}\n",
    "        loaded_vector_count = 0\n",
    "        print(\"  Reading GloVe file (can take a minute)...\")\n",
    "        try:\n",
    "            with open(GLOVE_PATH, 'r', encoding='utf-8') as f:\n",
    "                for line_num, line in enumerate(f):\n",
    "                     # Print progress occasionally for large GloVe files\n",
    "                     if (line_num + 1) % 100000 == 0: print(f\"    Processed {line_num+1} GloVe lines...\")\n",
    "                     values = line.split()\n",
    "                     word = values[0]\n",
    "                     try:\n",
    "                         coefs = np.asarray(values[1:], dtype='float32')\n",
    "                         # Check dimension ONLY if word is likely relevant\n",
    "                         if len(coefs) == EMBEDDING_DIM:\n",
    "                              embeddings_index[word] = coefs\n",
    "                              loaded_vector_count += 1\n",
    "                     except ValueError: pass # Ignore lines with parsing errors\n",
    "            print(f\"  Found {loaded_vector_count} word vectors of dimension {EMBEDDING_DIM} in GloVe file.\")\n",
    "            if loaded_vector_count == 0:\n",
    "                 raise ValueError(\"No vectors loaded from GloVe file. Check file path, format, and EMBEDDING_DIM.\")\n",
    "        except Exception as e:\n",
    "             print(f\"ERROR loading or processing GloVe file: {e}\")\n",
    "             raise\n",
    "\n",
    "        print(f\"\\nCreating embedding matrix with shape ({actual_sp_vocab_size}, {EMBEDDING_DIM})...\")\n",
    "        # Initialize matrix (e.g., with small random values)\n",
    "        # Using float32 for compatibility with TensorFlow/Keras layers\n",
    "        embedding_matrix = np.random.uniform(-0.05, 0.05, (actual_sp_vocab_size, EMBEDDING_DIM)).astype(np.float32)\n",
    "\n",
    "        hits = 0\n",
    "        misses = 0\n",
    "        # Iterate through the SentencePiece vocabulary (IDs 0 to vocab_size-1)\n",
    "        for i in range(actual_sp_vocab_size):\n",
    "             piece = sp.id_to_piece(i) # Get the actual word/subword piece string\n",
    "             embedding_vector = embeddings_index.get(piece) # Look it up in GloVe\n",
    "             if embedding_vector is not None:\n",
    "                 # If found in GloVe, use the GloVe vector\n",
    "                 embedding_matrix[i] = embedding_vector\n",
    "                 hits += 1\n",
    "             else:\n",
    "                 # If not found, keep the random initialization (model will learn these)\n",
    "                 misses += 1\n",
    "\n",
    "        # Crucial Step: Ensure the padding index (0) has a zero vector for Keras masking\n",
    "        # Regardless of what SP assigns to ID 0 (<unk>), Keras mask_zero=True needs the\n",
    "        # vector at index 0 to be all zeros when the input value is 0.\n",
    "        embedding_matrix[0] = np.zeros((EMBEDDING_DIM,), dtype=np.float32)\n",
    "\n",
    "        print(\"Embedding matrix created.\")\n",
    "        print(f\"  GloVe vectors mapped (Hits): {hits}\")\n",
    "        print(f\"  SP Tokens not in GloVe (Misses, incl <unk>,<s>,</s>): {misses}\")\n",
    "        print(f\"  Padding index 0 vector explicitly zeroed out.\")\n",
    "\n",
    "\n",
    "        # Save the embedding matrix using the path defined in Phase 1\n",
    "        print(f\"\\nSaving embedding matrix to '{EMBEDDING_MATRIX_PATH}'...\")\n",
    "        # Ensure directory exists\n",
    "        os.makedirs(os.path.dirname(EMBEDDING_MATRIX_PATH), exist_ok=True)\n",
    "        np.save(EMBEDDING_MATRIX_PATH, embedding_matrix)\n",
    "        print(\"Embedding matrix saved.\")\n",
    "\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "         print(f\"ERROR: A required file was not found: {e}\")\n",
    "         encoder_input_padded, decoder_input_padded, decoder_target_padded = None, None, None\n",
    "         sp = None; embedding_matrix = None; actual_sp_vocab_size = 0\n",
    "    except KeyError as e:\n",
    "         print(f\"ERROR: Missing expected column in DataFrame during tokenization: {e}\")\n",
    "         encoder_input_padded, decoder_input_padded, decoder_target_padded = None, None, None\n",
    "         sp = None; embedding_matrix = None; actual_sp_vocab_size = 0\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during Phase 4: {e}\")\n",
    "        traceback.print_exc()\n",
    "        encoder_input_padded, decoder_input_padded, decoder_target_padded = None, None, None\n",
    "        sp = None; embedding_matrix = None; actual_sp_vocab_size = 0\n",
    "else:\n",
    "    print(\"Skipping Phase 4 due to missing prerequisites (df_model, SP model file, or GloVe file).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c996de35-8473-4b64-be22-3547b8b59d2d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Phase 4a: Splitting Padded Data into Training and Validation sets ---\n",
      "Original dataset size before split: 73935 samples\n",
      "Using validation split: 15.0%\n",
      "\n",
      "Data split successful. Shapes:\n",
      "Encoder Train:      (62844, 1000)\n",
      "Decoder Input Train:(62844, 200)\n",
      "Decoder Target Train:(62844, 200)\n",
      "---\n",
      "Encoder Validation: (11091, 1000)\n",
      "Decoder Input Val:  (11091, 200)\n",
      "Decoder Target Val: (11091, 200)\n",
      "\n",
      "Split sizes verified.\n"
     ]
    }
   ],
   "source": [
    "# Phase 4a: Data Splitting\n",
    "\n",
    "print(f\"\\n--- Phase 4a: Splitting Padded Data into Training and Validation sets ---\")\n",
    "\n",
    "# Initialize split variables to None\n",
    "encoder_input_train, decoder_input_train, decoder_target_train = None, None, None\n",
    "encoder_input_val, decoder_input_val, decoder_target_val = None, None, None\n",
    "\n",
    "# Check if the padded data from Phase 4 exists\n",
    "if ('encoder_input_padded' in locals() and encoder_input_padded is not None and\n",
    "    'decoder_input_padded' in locals() and decoder_input_padded is not None and\n",
    "    'decoder_target_padded' in locals() and decoder_target_padded is not None):\n",
    "\n",
    "    print(f\"Original dataset size before split: {len(encoder_input_padded)} samples\")\n",
    "    print(f\"Using validation split: {VALIDATION_SPLIT*100:.1f}%\")\n",
    "\n",
    "    try:\n",
    "        # Perform the split\n",
    "        encoder_input_train, encoder_input_val, \\\n",
    "        decoder_input_train, decoder_input_val, \\\n",
    "        decoder_target_train, decoder_target_val = train_test_split(\n",
    "            encoder_input_padded,      # Input texts for encoder\n",
    "            decoder_input_padded,      # Input summaries for decoder (<start> + summary)\n",
    "            decoder_target_padded,     # Target summaries for decoder (summary + <end>)\n",
    "            test_size=VALIDATION_SPLIT, # Fraction for validation set\n",
    "            random_state=42             # Reproducibility\n",
    "        )\n",
    "\n",
    "        print(\"\\nData split successful. Shapes:\")\n",
    "        print(f\"Encoder Train:      {encoder_input_train.shape}\")\n",
    "        print(f\"Decoder Input Train:{decoder_input_train.shape}\")\n",
    "        print(f\"Decoder Target Train:{decoder_target_train.shape}\")\n",
    "        print(\"---\")\n",
    "        print(f\"Encoder Validation: {encoder_input_val.shape}\")\n",
    "        print(f\"Decoder Input Val:  {decoder_input_val.shape}\")\n",
    "        print(f\"Decoder Target Val: {decoder_target_val.shape}\")\n",
    "\n",
    "        # Sanity check sizes\n",
    "        assert len(encoder_input_train) + len(encoder_input_val) == len(encoder_input_padded)\n",
    "        print(\"\\nSplit sizes verified.\")\n",
    "\n",
    "        # Optional: Delete the large padded arrays if memory is tight,\n",
    "        # as they are now stored in the train/val splits\n",
    "        # del encoder_input_padded, decoder_input_padded, decoder_target_padded\n",
    "        # print(\"Original padded arrays deleted to free memory.\")\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during data splitting: {e}\")\n",
    "        traceback.print_exc()\n",
    "        # Ensure subsequent cells know the split failed\n",
    "        encoder_input_train, decoder_input_train, decoder_target_train = None, None, None\n",
    "\n",
    "else:\n",
    "    print(\"Skipping data split due to missing padded data from Phase 4.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e8aae4-61f5-496b-91f1-2afd8d9badd8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Phase 5: Model Building (BiLSTM Encoder, Pre-trained Embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b643917f-ac3a-4834-8f85-de02d3245ec9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Phase 5: Building the Seq2Seq model (BiLSTM Encoder - Combined States, GloVe Embeddings) ---\n",
      "Shared Embedding layer created. Trainable: False\n",
      "Building Encoder...\n",
      "Encoder configured to pass COMBINED & PROJECTED states (h:(None, 256), c:(None, 256)) to decoder.\n",
      "\n",
      "Building Decoder...\n",
      "\n",
      "Defining Keras Model...\n",
      "\n",
      "Model Architecture Summary:\n",
      "Model: \"seq2seq_bilstm_proj_glove_no_attention\"\n",
      "________________________________________________________________________________________________________________________\n",
      " Layer (type)                          Output Shape               Param #       Connected to                            \n",
      "========================================================================================================================\n",
      " decoder_input (InputLayer)            [(None, 200)]              0             []                                      \n",
      "                                                                                                                        \n",
      " encoder_input (InputLayer)            [(None, 1000)]             0             []                                      \n",
      "                                                                                                                        \n",
      " shared_glove_embedding (Embedding)    multiple                   1600000       ['encoder_input[0][0]',                 \n",
      "                                                                                 'decoder_input[0][0]']                 \n",
      "                                                                                                                        \n",
      " bidirectional_encoder_lstm (Bidirecti  [(None, 512),             731136        ['shared_glove_embedding[0][0]']        \n",
      " onal)                                  (None, 256),                                                                    \n",
      "                                        (None, 256),                                                                    \n",
      "                                        (None, 256),                                                                    \n",
      "                                        (None, 256)]                                                                    \n",
      "                                                                                                                        \n",
      " concat_h (Concatenate)                (None, 512)                0             ['bidirectional_encoder_lstm[0][1]',    \n",
      "                                                                                 'bidirectional_encoder_lstm[0][3]']    \n",
      "                                                                                                                        \n",
      " concat_c (Concatenate)                (None, 512)                0             ['bidirectional_encoder_lstm[0][2]',    \n",
      "                                                                                 'bidirectional_encoder_lstm[0][4]']    \n",
      "                                                                                                                        \n",
      " project_state_h (Dense)               (None, 256)                131328        ['concat_h[0][0]']                      \n",
      "                                                                                                                        \n",
      " project_state_c (Dense)               (None, 256)                131328        ['concat_c[0][0]']                      \n",
      "                                                                                                                        \n",
      " decoder_lstm (LSTM)                   [(None, 200, 256),         365568        ['shared_glove_embedding[1][0]',        \n",
      "                                        (None, 256),                             'project_state_h[0][0]',               \n",
      "                                        (None, 256)]                             'project_state_c[0][0]']               \n",
      "                                                                                                                        \n",
      " output_dense (Dense)                  (None, 200, 16000)         4112000       ['decoder_lstm[0][0]']                  \n",
      "                                                                                                                        \n",
      "========================================================================================================================\n",
      "Total params: 7,071,360\n",
      "Trainable params: 5,471,360\n",
      "Non-trainable params: 1,600,000\n",
      "________________________________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Phase 5 - Model Building (Revised for Full BiLSTM State Usage)\n",
    "\n",
    "print(f\"\\n--- Phase 5: Building the Seq2Seq model (BiLSTM Encoder - Combined States, GloVe Embeddings) ---\")\n",
    "\n",
    "model = None # Initialize model variable\n",
    "\n",
    "# Check prerequisites\n",
    "if ('encoder_input_train' in locals() and encoder_input_train is not None and # Using existence of train data as proxy\n",
    "    'embedding_matrix' in locals() and embedding_matrix is not None and\n",
    "    'actual_sp_vocab_size' in locals() and actual_sp_vocab_size > 0):\n",
    "\n",
    "    try:\n",
    "        # --- Shared Embedding Layer ---\n",
    "        # (Remains the same as before)\n",
    "        shared_embedding_layer = Embedding(\n",
    "            input_dim=actual_sp_vocab_size, output_dim=EMBEDDING_DIM,\n",
    "            weights=[embedding_matrix], trainable=False, mask_zero=True,\n",
    "            name='shared_glove_embedding'\n",
    "        )\n",
    "        print(f\"Shared Embedding layer created. Trainable: {shared_embedding_layer.trainable}\")\n",
    "\n",
    "        # --- Encoder ---\n",
    "        print(\"Building Encoder...\")\n",
    "        encoder_inputs = Input(shape=(MAXLEN_INPUT,), name='encoder_input')\n",
    "        encoder_embedding = shared_embedding_layer(encoder_inputs)\n",
    "        # Bidirectional LSTM Layer\n",
    "        encoder_bilstm = Bidirectional(\n",
    "            LSTM(LSTM_UNITS, return_state=True, name='encoder_lstm'), # Core LSTM\n",
    "            name='bidirectional_encoder_lstm',\n",
    "            merge_mode='concat' # Although merge_mode affects sequence output (which we discard),\n",
    "                               # return_state=True always returns separate fwd/bwd states\n",
    "        )\n",
    "        # Output: [combined_sequence (discarded)], h_fwd, c_fwd, h_bwd, c_bwd\n",
    "        _, state_h_fwd, state_c_fwd, state_h_bwd, state_c_bwd = encoder_bilstm(encoder_embedding)\n",
    "\n",
    "        # --- Combine Forward and Backward States ---\n",
    "        # Concatenate hidden states (axis=-1 merges the feature dimension)\n",
    "        state_h_combined = tf.keras.layers.Concatenate(name='concat_h')([state_h_fwd, state_h_bwd])\n",
    "        # Concatenate cell states\n",
    "        state_c_combined = tf.keras.layers.Concatenate(name='concat_c')([state_c_fwd, state_c_bwd])\n",
    "\n",
    "        # --- Project Combined States (Optional but Recommended) ---\n",
    "        # Add Dense layers to map concatenated states (2*LSTM_UNITS) back to LSTM_UNITS\n",
    "        # This allows the decoder LSTM to keep the original LSTM_UNITS size.\n",
    "        # Use activation like 'tanh' which is common for hidden states\n",
    "        decoder_init_state_h = Dense(LSTM_UNITS, activation='tanh', name='project_state_h')(state_h_combined)\n",
    "        decoder_init_state_c = Dense(LSTM_UNITS, activation='tanh', name='project_state_c')(state_c_combined)\n",
    "\n",
    "        # The final encoder states passed to the decoder\n",
    "        encoder_states = [decoder_init_state_h, decoder_init_state_c]\n",
    "        print(f\"Encoder configured to pass COMBINED & PROJECTED states (h:{decoder_init_state_h.shape}, c:{decoder_init_state_c.shape}) to decoder.\")\n",
    "\n",
    "\n",
    "        # --- Decoder ---\n",
    "        # (Decoder definition remains the same, but now receives the projected states)\n",
    "        print(\"\\nBuilding Decoder...\")\n",
    "        decoder_inputs = Input(shape=(MAXLEN_OUTPUT,), name='decoder_input')\n",
    "        decoder_embedding = shared_embedding_layer(decoder_inputs) # Reuse shared embedding\n",
    "        decoder_lstm = LSTM(LSTM_UNITS, return_sequences=True, return_state=True, name='decoder_lstm')\n",
    "        # Initialize with the NEW projected encoder_states\n",
    "        decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "        decoder_dense = Dense(actual_sp_vocab_size, activation='softmax', name='output_dense')\n",
    "        decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "        # --- Define the Training Model ---\n",
    "        print(\"\\nDefining Keras Model...\")\n",
    "        model = Model(inputs=[encoder_inputs, decoder_inputs],\n",
    "                      outputs=decoder_outputs,\n",
    "                      name='seq2seq_bilstm_proj_glove_no_attention') # Updated model name\n",
    "\n",
    "        print(\"\\nModel Architecture Summary:\")\n",
    "        model.summary(line_length=120) # Slightly wider print for new layers\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during model building: {e}\")\n",
    "        traceback.print_exc()\n",
    "        model = None\n",
    "else:\n",
    "    # Refine the prerequisite check message (same as before)\n",
    "    missing_prereqs = []\n",
    "    # ... (add checks as before) ...\n",
    "    if 'embedding_matrix' not in locals() or embedding_matrix is None: missing_prereqs.append(\"Embedding Matrix\")\n",
    "    # ... etc ...\n",
    "    print(f\"Skipping model building due to missing prerequisites: {', '.join(missing_prereqs)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd4d3b4-d3e5-4f65-bf90-3296edd39c4c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Phase 6: Model Compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3384be11-d89c-41b9-a401-de7f82c148c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Phase 6: Compiling the model ---\n",
      "Model compiled successfully with Adam optimizer (LR=0.001) and sparse categorical crossentropy loss.\n",
      "Accuracy will be monitored during training.\n"
     ]
    }
   ],
   "source": [
    "# Phase 6: Model Compilation\n",
    "\n",
    "# Check if the model object exists from the previous phase\n",
    "if 'model' in locals() and model is not None:\n",
    "    print(\"\\n--- Phase 6: Compiling the model ---\")\n",
    "    try:\n",
    "        # Define the optimizer with the specified learning rate\n",
    "        optimizer = Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "        # Configure the model for training\n",
    "        model.compile(\n",
    "            optimizer=optimizer,                   # Use Adam optimizer with specified LR\n",
    "            loss='sparse_categorical_crossentropy', # Correct loss for integer targets\n",
    "            metrics=['accuracy']                   # Monitor basic accuracy\n",
    "            )\n",
    "        print(f\"Model compiled successfully with Adam optimizer (LR={LEARNING_RATE}) and sparse categorical crossentropy loss.\")\n",
    "        print(\"Accuracy will be monitored during training.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during model compilation: {e}\")\n",
    "        traceback.print_exc()\n",
    "        # Invalidate model if compilation fails\n",
    "        model = None\n",
    "else:\n",
    "    print(\"Skipping model compilation as the model object ('model') was not created successfully in Phase 5.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301538f0-c55c-499a-963d-2fd613421896",
   "metadata": {
    "tags": []
   },
   "source": [
    "##  Phase 7: Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eaaf5afa-fbe5-4ca8-ad31-860fde9fa4af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Phase 7: Custom Training Loop ---\n",
      "Setting up training components...\n",
      "Building inference models for training step...\n",
      "  Using projected encoder states for decoder init.\n",
      "Inference Encoder built.\n",
      "Inference Decoder built.\n",
      "Warning: rouge-score library not available. Skipping ROUGE calculation.\n",
      "\n",
      "--- Starting Custom Training Loop ---\n",
      "Creating TensorFlow Datasets for efficient batching...\n",
      "Datasets created.\n",
      "Train steps per epoch: 982, Val steps per epoch: 174\n",
      "\n",
      "Epoch 1/20\n",
      "  Scheduled Sampling Probability (Teacher Forcing): 0.9091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-23 21:04:52.003699: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x55f0a24018a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-04-23 21:04:52.003731: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA L4, Compute Capability 8.9\n",
      "2025-04-23 21:04:52.041755: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-04-23 21:04:52.491709: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7fe9a804b400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7fe9a804b400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "  Batch 982/982, Loss: 6.6794, Accuracy: 2.4836\n",
      "Epoch 1 Training ---- Loss: 6.6794, Accuracy: 2.4836\n",
      "  Running Validation...\n",
      "Epoch 1 Validation -- Loss: 6.9219, Acc: 0.0527 (ROUGE not calculated or scorer unavailable)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'MODEL_WEIGHTS_SAVE_PATH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 361\u001b[0m\n\u001b[1;32m    357\u001b[0m     is_better \u001b[38;5;241m=\u001b[39m current_val_metric \u001b[38;5;241m>\u001b[39m best_val_metric \u001b[38;5;66;03m# Higher neg loss (lower actual loss) is better\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_better:\n\u001b[0;32m--> 361\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmonitor_metric_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m improved from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_val_metric\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_val_metric\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Saving model weights to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mMODEL_WEIGHTS_SAVE_PATH\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    362\u001b[0m     best_val_metric \u001b[38;5;241m=\u001b[39m current_val_metric\n\u001b[1;32m    363\u001b[0m     epochs_no_improve \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MODEL_WEIGHTS_SAVE_PATH' is not defined"
     ]
    }
   ],
   "source": [
    "# Cell 8: Phase 7 - Model Training (Custom Loop)\n",
    "\n",
    "print(f\"\\n--- Phase 7: Custom Training Loop ---\")\n",
    "\n",
    "# Ensure rouge_scorer class exists (from Phase 1 import/install)\n",
    "# We will instantiate the scorer object later inside the main check\n",
    "rouge_scorer_class_available = 'rouge_scorer' in globals() and rouge_scorer is not None\n",
    "\n",
    "history_custom = None # Initialize history\n",
    "\n",
    "# Check main prerequisites: compiled model, split training data, SP processor\n",
    "if ('model' in locals() and model is not None and\n",
    "    'encoder_input_train' in locals() and encoder_input_train is not None and\n",
    "    'decoder_input_train' in locals() and decoder_input_train is not None and\n",
    "    'decoder_target_train' in locals() and decoder_target_train is not None and\n",
    "    'encoder_input_val' in locals() and encoder_input_val is not None and\n",
    "    'decoder_input_val' in locals() and decoder_input_val is not None and\n",
    "    'decoder_target_val' in locals() and decoder_target_val is not None and\n",
    "    'sp' in locals() and sp is not None ):\n",
    "\n",
    "    print(\"Setting up training components...\")\n",
    "\n",
    "    # --- Optimizer, Loss, Metrics ---\n",
    "    optimizer = Adam(learning_rate=LEARNING_RATE)\n",
    "    loss_object = SparseCategoricalCrossentropy(from_logits=False, reduction='none')\n",
    "\n",
    "    def masked_loss(real, pred):\n",
    "        mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "        loss_ = loss_object(real, pred)\n",
    "        mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "        loss_ *= mask\n",
    "        return tf.reduce_sum(loss_) / (tf.reduce_sum(mask) + 1e-9)\n",
    "\n",
    "    def masked_accuracy(real, pred):\n",
    "        pred_ids = tf.argmax(pred, axis=-1, output_type=real.dtype)\n",
    "        match = tf.cast(tf.equal(real, pred_ids), dtype=tf.float32)\n",
    "        mask = tf.cast(tf.math.logical_not(tf.math.equal(real, 0)), dtype=tf.float32)\n",
    "        match *= mask\n",
    "        return tf.reduce_sum(match) / (tf.reduce_sum(mask) + 1e-9)\n",
    "\n",
    "    train_loss_metric = tf.keras.metrics.Mean(name='train_loss')\n",
    "    train_accuracy_metric = tf.keras.metrics.Mean(name='train_accuracy')\n",
    "    val_loss_metric = tf.keras.metrics.Mean(name='val_loss')\n",
    "    val_accuracy_metric = tf.keras.metrics.Mean(name='val_accuracy')\n",
    "\n",
    "    # --- Scheduled Sampling Probability Function ---\n",
    "    def get_sampling_prob(epoch, k=SCHEDULED_SAMPLING_K):\n",
    "        # Inverse sigmoid decay: p decreases as epoch increases\n",
    "        prob = k / (k + tf.exp(epoch / k))\n",
    "        return float(prob)\n",
    "\n",
    "    # --- Rebuild Inference Models (Needed for train_step and greedy_decode) ---\n",
    "    encoder_model_inf = None\n",
    "    decoder_model_inf = None\n",
    "    try:\n",
    "        print(\"Building inference models for training step...\")\n",
    "        # --- Inference Encoder ---\n",
    "        encoder_inputs_inf = model.get_layer('encoder_input').input\n",
    "        # Access states from the Bidirectional layer output\n",
    "        # Output: [combined_sequence], state_h_fwd, state_c_fwd, state_h_bwd, state_c_bwd\n",
    "        _enc_seq_out, state_h_fwd, state_c_fwd, state_h_bwd, state_c_bwd = model.get_layer('bidirectional_encoder_lstm').output\n",
    "\n",
    "        # Use the *projected* states if they exist (from the corrected Phase 5)\n",
    "        if 'project_state_h' in [l.name for l in model.layers]:\n",
    "             print(\"  Using projected encoder states for decoder init.\")\n",
    "             state_h_inf = model.get_layer('project_state_h').output\n",
    "             state_c_inf = model.get_layer('project_state_c').output\n",
    "        else:\n",
    "             # Fallback to using forward states if projection layers weren't added (old Phase 5)\n",
    "             print(\"  Warning: Projection layers not found, using only FORWARD encoder states.\")\n",
    "             state_h_inf = state_h_fwd\n",
    "             state_c_inf = state_c_fwd\n",
    "\n",
    "        encoder_states_inf = [state_h_inf, state_c_inf]\n",
    "        encoder_model_inf = Model(inputs=encoder_inputs_inf, outputs=encoder_states_inf, name=\"train_inference_encoder\")\n",
    "        print(\"Inference Encoder built.\")\n",
    "\n",
    "        # --- Inference Decoder ---\n",
    "        decoder_state_input_h = Input(shape=(LSTM_UNITS,), name='train_inf_dec_state_h')\n",
    "        decoder_state_input_c = Input(shape=(LSTM_UNITS,), name='train_inf_dec_state_c')\n",
    "        decoder_states_inputs_inf = [decoder_state_input_h, decoder_state_input_c]\n",
    "        decoder_inputs_inf_single = Input(shape=(1,), name='train_inf_dec_input_single')\n",
    "\n",
    "        # Reuse layers from the main 'model'\n",
    "        decoder_embedding_layer_inf = model.get_layer('shared_glove_embedding')\n",
    "        decoder_lstm_inf_layer = model.get_layer('decoder_lstm')\n",
    "        decoder_dense_inf_layer = model.get_layer('output_dense')\n",
    "\n",
    "        # Connect inference decoder layers\n",
    "        decoder_embedding_inf = decoder_embedding_layer_inf(decoder_inputs_inf_single)\n",
    "        decoder_lstm_outputs_inf, state_h_out_inf, state_c_out_inf = decoder_lstm_inf_layer(decoder_embedding_inf, initial_state=decoder_states_inputs_inf)\n",
    "        decoder_states_out_inf = [state_h_out_inf, state_c_out_inf]\n",
    "        decoder_pred_inf = decoder_dense_inf_layer(decoder_lstm_outputs_inf)\n",
    "\n",
    "        # Define the inference decoder model\n",
    "        decoder_model_inf = Model(inputs=[decoder_inputs_inf_single] + decoder_states_inputs_inf,\n",
    "                                  outputs=[decoder_pred_inf] + decoder_states_out_inf,\n",
    "                                  name=\"train_inference_decoder\")\n",
    "        print(\"Inference Decoder built.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to build inference models for training step: {e}\")\n",
    "        traceback.print_exc()\n",
    "        encoder_model_inf = None; decoder_model_inf = None # Ensure they are None\n",
    "\n",
    "\n",
    "    # --- Custom Training Step Function ---\n",
    "    # Only define if inference models were built successfully\n",
    "    if encoder_model_inf and decoder_model_inf:\n",
    "        # @tf.function # Compile to graph for speed\n",
    "        def train_step(enc_input_batch, dec_input_batch, dec_target_batch, sampling_prob_tensor):\n",
    "            batch_loss = tf.constant(0.0, dtype=tf.float32)\n",
    "            batch_acc_match = tf.constant(0.0, dtype=tf.float32)\n",
    "            batch_acc_total = tf.constant(0.0, dtype=tf.float32)\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Get initial states from the inference encoder model\n",
    "                enc_states = encoder_model_inf(enc_input_batch, training=True)\n",
    "                dec_state = enc_states # Initialize decoder state\n",
    "\n",
    "                # Start token is the first token in the decoder input sequence\n",
    "                dec_input_token = tf.expand_dims(dec_input_batch[:, 0], 1)\n",
    "\n",
    "                # Loop through the target sequence (ignoring the first token, typically <start>)\n",
    "                for t in range(1, dec_target_batch.shape[1]):\n",
    "                    # Run the inference decoder model for one step\n",
    "                    predictions, state_h, state_c = decoder_model_inf([dec_input_token] + dec_state, training=True)\n",
    "                    dec_state = [state_h, state_c] # Update the state for the next step\n",
    "\n",
    "                    # Calculate loss against the actual target token for this timestep\n",
    "                    real_target_token = dec_target_batch[:, t]\n",
    "                    loss_t = masked_loss(real_target_token, predictions)\n",
    "                    batch_loss += loss_t # Accumulate loss over the sequence\n",
    "\n",
    "                    # Calculate accuracy for this timestep\n",
    "                    pred_ids_t = tf.argmax(predictions, axis=-1, output_type=real_target_token.dtype)\n",
    "                    mask_t = tf.math.logical_not(tf.math.equal(real_target_token, 0))\n",
    "                    match_t = tf.cast(tf.equal(real_target_token, pred_ids_t), dtype=tf.float32) * tf.cast(mask_t, dtype=tf.float32)\n",
    "                    batch_acc_match += tf.reduce_sum(match_t)\n",
    "                    batch_acc_total += tf.reduce_sum(tf.cast(mask_t, dtype=tf.float32))\n",
    "\n",
    "                    # --- Scheduled Sampling Decision ---\n",
    "                    # Decide whether to use the ground truth or the model's prediction as the next input\n",
    "                    use_teacher_forcing = tf.random.uniform(()) < sampling_prob_tensor\n",
    "\n",
    "                    if use_teacher_forcing:\n",
    "                        # Use the actual target token from the *current* step as the *next* input\n",
    "                        dec_input_token = tf.expand_dims(real_target_token, 1)\n",
    "                    else:\n",
    "                        # Use the model's prediction (highest probability token ID) as the *next* input\n",
    "                        dec_input_token = tf.expand_dims(pred_ids_t, 1)\n",
    "\n",
    "            # Calculate gradients based on the total loss for the sequence\n",
    "            variables = model.trainable_variables\n",
    "            gradients = tape.gradient(batch_loss, variables)\n",
    "             # Clip gradients to prevent exploding gradients (optional but often helpful)\n",
    "            gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "            # Apply gradients to update model weights\n",
    "            optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "            # Calculate average loss & accuracy for the batch (over non-padded tokens)\n",
    "            avg_batch_loss = batch_loss / batch_acc_total if batch_acc_total > 0 else 0.0\n",
    "            avg_batch_acc = batch_acc_match / batch_acc_total if batch_acc_total > 0 else 0.0\n",
    "\n",
    "            return avg_batch_loss, avg_batch_acc\n",
    "    else:\n",
    "        print(\"ERROR: Cannot define train_step because inference model building failed.\")\n",
    "        encoder_input_train = None # Prevent training loop start\n",
    "\n",
    "\n",
    "    # --- Greedy Decoding Function (for validation ROUGE check) ---\n",
    "    if encoder_model_inf and decoder_model_inf:\n",
    "        # Use numpy for greedy decoding loop as it runs outside tf.function\n",
    "        def greedy_decode_sequence(input_seq_padded, maxlen_out, start_token_id, end_token_id):\n",
    "            # Ensure input has batch dimension\n",
    "            if input_seq_padded.ndim == 1:\n",
    "                 input_seq_padded = np.expand_dims(input_seq_padded, 0)\n",
    "            # Predict initial states\n",
    "            states_value = encoder_model_inf.predict(input_seq_padded, verbose=0)\n",
    "            # Start sequence with BOS token ID\n",
    "            target_seq = np.array([[start_token_id]])\n",
    "            decoded_ids = []\n",
    "            # Loop until max length or EOS token\n",
    "            for _ in range(maxlen_out):\n",
    "                # Predict next token and states\n",
    "                output_tokens_dist, h, c = decoder_model_inf.predict([target_seq] + states_value, verbose=0)\n",
    "                # Get the token ID with highest probability (greedy choice)\n",
    "                sampled_token_id = np.argmax(output_tokens_dist[0, -1, :])\n",
    "                # Stop if EOS or padding token is predicted\n",
    "                if sampled_token_id == end_token_id or sampled_token_id == 0: break\n",
    "                decoded_ids.append(sampled_token_id)\n",
    "                # Update the target sequence for the next step\n",
    "                target_seq = np.array([[sampled_token_id]])\n",
    "                # Update the states\n",
    "                states_value = [h, c]\n",
    "            return decoded_ids\n",
    "    else:\n",
    "        print(\"ERROR: Cannot define greedy_decode_sequence because inference models failed.\")\n",
    "\n",
    "\n",
    "    # --- ROUGE Scorer Initialization ---\n",
    "    scorer = None # Initialize\n",
    "    if rouge_scorer_class_available:\n",
    "        try:\n",
    "            rouge_types = ['rouge1', 'rouge2', 'rougeL']\n",
    "            scorer = rouge_scorer.RougeScorer(rouge_types=rouge_types, use_stemmer=True)\n",
    "            print(\"ROUGE scorer initialized.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Failed to initialize ROUGE scorer: {e}. Skipping ROUGE calculation.\")\n",
    "            scorer = None\n",
    "    else:\n",
    "        print(\"Warning: rouge-score library not available. Skipping ROUGE calculation.\")\n",
    "\n",
    "\n",
    "    # --- Training Loop ---\n",
    "    # Final check before starting the potentially long loop\n",
    "    if encoder_model_inf and decoder_model_inf and encoder_input_train is not None:\n",
    "        print(\"\\n--- Starting Custom Training Loop ---\")\n",
    "        start_train_time = time.time()\n",
    "        best_val_metric = -np.inf # Initialize for maximizing ROUGE or -Loss\n",
    "        metric_to_monitor = 'ROUGE-L F1' if scorer else 'Neg Validation Loss'\n",
    "        epochs_no_improve = 0\n",
    "        # Store history manually\n",
    "        history_custom = {'loss': [], 'accuracy': [], 'val_loss': [], 'val_accuracy': [],\n",
    "                          'val_rouge1': [], 'val_rouge2': [], 'val_rougeL': []}\n",
    "\n",
    "        # --- Create TensorFlow Datasets ---\n",
    "        print(\"Creating TensorFlow Datasets for efficient batching...\")\n",
    "        try:\n",
    "            buffer_size = len(encoder_input_train)\n",
    "            # Training dataset: shuffle, batch, prefetch\n",
    "            train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "                (encoder_input_train, decoder_input_train, decoder_target_train)\n",
    "            )\n",
    "            train_dataset = train_dataset.shuffle(buffer_size).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "            # Validation dataset: batch, prefetch (no shuffle)\n",
    "            val_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "                (encoder_input_val, decoder_input_val, decoder_target_val)\n",
    "            )\n",
    "            val_dataset = val_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "            print(\"Datasets created.\")\n",
    "            # Get number of batches for progress display\n",
    "            train_steps_per_epoch = tf.data.experimental.cardinality(train_dataset).numpy()\n",
    "            val_steps_per_epoch = tf.data.experimental.cardinality(val_dataset).numpy()\n",
    "            print(f\"Train steps per epoch: {train_steps_per_epoch}, Val steps per epoch: {val_steps_per_epoch}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR creating TensorFlow Datasets: {e}\")\n",
    "            traceback.print_exc()\n",
    "            encoder_input_train = None # Prevent loop start\n",
    "\n",
    "        # --- Epoch Loop ---\n",
    "        if encoder_input_train is not None: # Check dataset creation worked\n",
    "            for epoch in range(EPOCHS):\n",
    "                epoch_start_time = time.time()\n",
    "                print(f\"\\nEpoch {epoch + 1}/{EPOCHS}\")\n",
    "\n",
    "                # Reset metrics at the start of each epoch\n",
    "                train_loss_metric.reset_states(); train_accuracy_metric.reset_states()\n",
    "                val_loss_metric.reset_states(); val_accuracy_metric.reset_states()\n",
    "\n",
    "                # --- Training Batch Loop ---\n",
    "                current_sampling_prob = get_sampling_prob(epoch, SCHEDULED_SAMPLING_K)\n",
    "                sampling_prob_tensor = tf.constant(current_sampling_prob, dtype=tf.float32)\n",
    "                print(f\"  Scheduled Sampling Probability (Teacher Forcing): {current_sampling_prob:.4f}\")\n",
    "\n",
    "                for batch, (enc_in, dec_in, dec_target) in enumerate(train_dataset):\n",
    "                    batch_loss, batch_acc = train_step(enc_in, dec_in, dec_target, sampling_prob_tensor)\n",
    "                    train_loss_metric(batch_loss)\n",
    "                    train_accuracy_metric(batch_acc)\n",
    "                    # Print progress every N batches\n",
    "                    if (batch + 1) % 100 == 0 or (batch + 1) == train_steps_per_epoch:\n",
    "                        print(f'  Batch {batch + 1}/{train_steps_per_epoch}, Loss: {train_loss_metric.result():.4f}, Accuracy: {train_accuracy_metric.result():.4f}', end='\\r')\n",
    "\n",
    "                # End of Training Epoch\n",
    "                epoch_train_loss = train_loss_metric.result().numpy()\n",
    "                epoch_train_acc = train_accuracy_metric.result().numpy()\n",
    "                history_custom['loss'].append(epoch_train_loss)\n",
    "                history_custom['accuracy'].append(epoch_train_acc)\n",
    "                print(f'\\nEpoch {epoch + 1} Training ---- Loss: {epoch_train_loss:.4f}, Accuracy: {epoch_train_acc:.4f}')\n",
    "\n",
    "                # --- Validation Loop ---\n",
    "                print(\"  Running Validation...\")\n",
    "                all_preds_text = []\n",
    "                all_reals_text = []\n",
    "                # Limit ROUGE calculation for speed (e.g., first N batches or N samples)\n",
    "                num_val_samples_for_rouge = BATCH_SIZE * 2 # Example: first 2 batches\n",
    "                samples_evaluated_rouge = 0\n",
    "\n",
    "                for val_batch_num, (enc_in_val, dec_in_val, dec_target_val) in enumerate(val_dataset):\n",
    "                    # Calculate standard val loss/accuracy on ALL validation data\n",
    "                    val_predictions = model([enc_in_val, dec_in_val], training=False) # Use main model\n",
    "                    batch_val_loss = masked_loss(dec_target_val, val_predictions)\n",
    "                    batch_val_acc = masked_accuracy(dec_target_val, val_predictions)\n",
    "                    val_loss_metric(batch_val_loss)\n",
    "                    val_accuracy_metric(batch_val_acc)\n",
    "\n",
    "                    # Generate summaries & collect text for ROUGE on a SUBSET\n",
    "                    if scorer and samples_evaluated_rouge < num_val_samples_for_rouge:\n",
    "                        if val_batch_num == 0: print(f\"    Generating summaries for ROUGE (approx {num_val_samples_for_rouge} samples)...\")\n",
    "                        for i in range(enc_in_val.shape[0]):\n",
    "                             if samples_evaluated_rouge < num_val_samples_for_rouge:\n",
    "                                 single_enc_input = tf.expand_dims(enc_in_val[i], 0)\n",
    "                                 decoded_ids = greedy_decode_sequence(single_enc_input, MAXLEN_OUTPUT, sp.bos_id(), sp.eos_id())\n",
    "                                 pred_text = sp.decode(decoded_ids)\n",
    "                                 # Get non-padding target tokens for reference text\n",
    "                                 real_target_tokens = [int(t) for t in dec_target_val[i].numpy() if t != 0 ]\n",
    "                                 real_text = sp.decode(real_target_tokens)\n",
    "                                 all_preds_text.append(pred_text)\n",
    "                                 all_reals_text.append(real_text)\n",
    "                                 samples_evaluated_rouge += 1\n",
    "\n",
    "                epoch_val_loss = val_loss_metric.result().numpy()\n",
    "                epoch_val_acc = val_accuracy_metric.result().numpy()\n",
    "                history_custom['val_loss'].append(epoch_val_loss)\n",
    "                history_custom['val_accuracy'].append(epoch_val_acc)\n",
    "\n",
    "                # --- Calculate and Record ROUGE ---\n",
    "                val_rouge1, val_rouge2, val_rougeL = np.nan, np.nan, np.nan # Default to NaN\n",
    "                if scorer and all_preds_text:\n",
    "                    try:\n",
    "                        aggregator = rouge_scorer.scoring.BootstrapAggregator()\n",
    "                        print(f\"    Calculating ROUGE on {len(all_preds_text)} generated summaries...\")\n",
    "                        for pred, real in zip(all_preds_text, all_reals_text):\n",
    "                            pred = pred if pred else \" \" # Handle empty strings\n",
    "                            real = real if real else \" \"\n",
    "                            scores = scorer.score(target=real, prediction=pred)\n",
    "                            aggregator.add_scores(scores)\n",
    "                        result = aggregator.aggregate()\n",
    "                        val_rouge1 = result['rouge1'].mid.fmeasure * 100\n",
    "                        val_rouge2 = result['rouge2'].mid.fmeasure * 100\n",
    "                        val_rougeL = result['rougeL'].mid.fmeasure * 100\n",
    "                        print(f'Epoch {epoch + 1} Validation -- Loss: {epoch_val_loss:.4f}, Acc: {epoch_val_acc:.4f}, ROUGE-L: {val_rougeL:.2f}')\n",
    "                    except Exception as rouge_e:\n",
    "                        print(f\"Warning: Error calculating ROUGE scores: {rouge_e}\")\n",
    "                        # Keep ROUGE scores as NaN\n",
    "                else:\n",
    "                    print(f'Epoch {epoch + 1} Validation -- Loss: {epoch_val_loss:.4f}, Acc: {epoch_val_acc:.4f} (ROUGE not calculated or scorer unavailable)')\n",
    "\n",
    "                # Append scores (will append NaN if not calculated)\n",
    "                history_custom['val_rouge1'].append(val_rouge1)\n",
    "                history_custom['val_rouge2'].append(val_rouge2)\n",
    "                history_custom['val_rougeL'].append(val_rougeL)\n",
    "\n",
    "\n",
    "                # --- Early Stopping & Model Saving Logic ---\n",
    "                # Monitor ROUGE-L F1 score if available and not NaN, otherwise monitor inverted validation loss\n",
    "                current_val_metric = np.nan # Default to NaN\n",
    "                if scorer and not np.isnan(val_rougeL):\n",
    "                    current_val_metric = val_rougeL\n",
    "                    monitor_metric_name = 'ROUGE-L F1'\n",
    "                    is_better = current_val_metric > best_val_metric # Higher ROUGE is better\n",
    "                else:\n",
    "                    current_val_metric = -epoch_val_loss # Use negative loss (higher is better)\n",
    "                    monitor_metric_name = 'Neg Validation Loss'\n",
    "                    is_better = current_val_metric > best_val_metric # Higher neg loss (lower actual loss) is better\n",
    "\n",
    "\n",
    "                if is_better:\n",
    "                    print(f'Validation {monitor_metric_name} improved from {best_val_metric:.4f} to {current_val_metric:.4f}. Saving model weights to {MODEL_WEIGHTS_SAVE_PATH}...')\n",
    "                    best_val_metric = current_val_metric\n",
    "                    epochs_no_improve = 0\n",
    "                    # Save only weights\n",
    "                    model.save_weights(MODEL_WEIGHTS_SAVE_PATH)\n",
    "                else:\n",
    "                    epochs_no_improve += 1\n",
    "                    print(f'Validation {monitor_metric_name} did not improve ({current_val_metric:.4f} vs best {best_val_metric:.4f}). Patience: {epochs_no_improve}/{EARLY_STOPPING_PATIENCE}')\n",
    "\n",
    "                if epochs_no_improve >= EARLY_STOPPING_PATIENCE:\n",
    "                    print(f\"\\nEarly stopping triggered after epoch {epoch + 1}.\")\n",
    "                    break # Exit epoch loop\n",
    "\n",
    "                print(f\"Epoch {epoch + 1} Time: {time.time() - epoch_start_time:.2f} sec\")\n",
    "\n",
    "\n",
    "            # --- End of Epoch Loop ---\n",
    "            total_train_time = time.time() - start_train_time\n",
    "            print(f\"\\n--- Custom Training Loop Finished ---\")\n",
    "            print(f\"Total training time: {total_train_time:.2f} seconds ({total_train_time / 60:.2f} minutes)\")\n",
    "            print(f\"Best Validation Metric ({monitor_metric_name}): {best_val_metric:.4f}\")\n",
    "            if os.path.exists(MODEL_WEIGHTS_SAVE_PATH):\n",
    "                 print(f\"Model weights for best epoch saved to '{MODEL_WEIGHTS_SAVE_PATH}'\")\n",
    "            else:\n",
    "                 print(\"Warning: Best model weights file not found (Training might have stopped before improvement).\")\n",
    "\n",
    "\n",
    "        else: # Handle dataset creation failure\n",
    "             print(\"ERROR: Cannot start training loop because TensorFlow dataset creation failed.\")\n",
    "             history_custom = None\n",
    "\n",
    "    else: # Handle failure to build inference models\n",
    "        print(\"ERROR: Cannot start training loop because inference model building failed.\")\n",
    "        history_custom = None\n",
    "\n",
    "else:\n",
    "    print(\"Skipping custom training loop due to missing prerequisites (check previous cell outputs for errors).\")\n",
    "    history_custom = None # Ensure history is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b735c830-a94a-46ec-9c6d-290bb77f1302",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-tensorflow-tensorflow",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "TensorFlow 2-11 (Local)",
   "language": "python",
   "name": "conda-env-tensorflow-tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
