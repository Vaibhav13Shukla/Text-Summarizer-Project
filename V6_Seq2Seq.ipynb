{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63f303c5-b3af-467a-a1c6-68317248c38c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c194b622-d6bc-4088-86cf-58453c354ad9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 18:30:30.952820: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-24 18:30:31.078033: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-24 18:30:31.911768: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/lib/x86_64-linux-gnu/:/opt/conda/lib\n",
      "2025-04-24 18:30:31.911866: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/lib/x86_64-linux-gnu/:/opt/conda/lib\n",
      "2025-04-24 18:30:31.911875: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2025-04-24 18:30:33,205 - INFO - --- Configuration (Attention, SS, Beam Search) ---\n",
      "2025-04-24 18:30:33,206 - INFO - TensorFlow Version: 2.11.0\n",
      "2025-04-24 18:30:33,207 - INFO - SentencePiece Version: 0.2.0\n",
      "2025-04-24 18:30:33,209 - INFO - Langdetect Version: 1.0.9\n",
      "2025-04-24 18:30:33,283 - INFO - GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "2025-04-24 18:30:33,284 - INFO - Input JSONL: mergedt02.jsonl\n",
      "2025-04-24 18:30:33,284 - INFO - Output Parquet Cache: model_attention_files/processed_dataframe.parquet\n",
      "2025-04-24 18:30:33,285 - INFO - Tokenizer Model Prefix: model_attention_files/pib_summarizer_spm_50k\n",
      "2025-04-24 18:30:33,286 - INFO - Model Checkpoint Dir: model_attention_files/training_checkpoints\n",
      "2025-04-24 18:30:33,287 - INFO - Best Model Save Path: model_attention_files/pib_summarizer_attention_best.keras\n",
      "2025-04-24 18:30:33,287 - INFO - TensorBoard Log Dir: model_attention_files/logs/custom_train/20250424-183033\n",
      "2025-04-24 18:30:33,288 - INFO - Vocab Size: 30000\n",
      "2025-04-24 18:30:33,288 - INFO - Embedding Dim: 100\n",
      "2025-04-24 18:30:33,289 - INFO - Encoder LSTM Units (per dir): 128\n",
      "2025-04-24 18:30:33,290 - INFO - Decoder LSTM Units: 256\n",
      "2025-04-24 18:30:33,290 - INFO - Attention Units: 512\n",
      "2025-04-24 18:30:33,291 - INFO - Encoder Layers: 1\n",
      "2025-04-24 18:30:33,292 - INFO - Decoder Layers: 1\n",
      "2025-04-24 18:30:33,292 - INFO - Max Input Length: 1024\n",
      "2025-04-24 18:30:33,293 - INFO - Max Summary Length: 150\n",
      "2025-04-24 18:30:33,294 - INFO - Batch Size: 32\n",
      "2025-04-24 18:30:33,294 - INFO - Max Epochs: 30\n",
      "2025-04-24 18:30:33,295 - INFO - Learning Rate: 0.001\n",
      "2025-04-24 18:30:33,295 - INFO - Initial Sampling Prob: 1.0\n",
      "2025-04-24 18:30:33,296 - INFO - Sampling Prob Decay: 0.99\n",
      "2025-04-24 18:30:33,297 - INFO - Manual Early Stopping Patience: 5\n",
      "2025-04-24 18:30:33,297 - INFO - Manual Reduce LR Patience: 3\n",
      "2025-04-24 18:30:33,298 - INFO - Manual Reduce LR Factor: 0.2\n",
      "2025-04-24 18:30:33,299 - INFO - Beam Width: 4\n",
      "2025-04-24 18:30:33,299 - INFO - ------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensuring output directory exists: /home/jupyter/model_attention_files\n",
      "Logging setup complete. Log file: model_attention_files/training_log_attention.log\n",
      "\n",
      "--- Configuration (Attention, SS, Beam Search) ---\n",
      "TensorFlow Version: 2.11.0\n",
      "SentencePiece Version: 0.2.0\n",
      "Langdetect Version: 1.0.9\n",
      "GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Input JSONL: mergedt02.jsonl\n",
      "Output Parquet Cache: model_attention_files/processed_dataframe.parquet\n",
      "Tokenizer Model Prefix: model_attention_files/pib_summarizer_spm_50k\n",
      "Model Checkpoint Dir: model_attention_files/training_checkpoints\n",
      "Best Model Save Path: model_attention_files/pib_summarizer_attention_best.keras\n",
      "TensorBoard Log Dir: model_attention_files/logs/custom_train/20250424-183033\n",
      "Vocab Size: 30000\n",
      "Embedding Dim: 100\n",
      "Encoder LSTM Units (per dir): 128\n",
      "Decoder LSTM Units: 256\n",
      "Attention Units: 512\n",
      "Encoder Layers: 1\n",
      "Decoder Layers: 1\n",
      "Max Input Length: 1024\n",
      "Max Summary Length: 150\n",
      "Batch Size: 32\n",
      "Max Epochs: 30\n",
      "Learning Rate: 0.001\n",
      "Initial Sampling Prob: 1.0\n",
      "Sampling Prob Decay: 0.99\n",
      "Manual Early Stopping Patience: 5\n",
      "Manual Reduce LR Patience: 3\n",
      "Manual Reduce LR Factor: 0.2\n",
      "Beam Width: 4\n",
      "------------------------------\n",
      "Checkpoint directory (for custom loop): model_attention_files/training_checkpoints\n",
      "Best model save path (for custom loop): model_attention_files/pib_summarizer_attention_best.keras\n",
      "TensorBoard Log Directory (for custom loop): model_attention_files/logs/custom_train/20250424-183033\n"
     ]
    }
   ],
   "source": [
    "# Block 1: Setup and Configuration (Corrected with Definitions)\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import importlib.metadata # Use for getting package versions (Python 3.8+)\n",
    "import gc # For garbage collection\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "# Callbacks might be replaced by custom loop logic later, but keep imports for now\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
    "\n",
    "# Check if SentencePiece is available\n",
    "try:\n",
    "    import sentencepiece as spm\n",
    "except ImportError:\n",
    "    print(\"SentencePiece not found. You might need to install it (`pip install sentencepiece`)\")\n",
    "    spm = None\n",
    "\n",
    "# Check if langdetect is available\n",
    "try:\n",
    "    from langdetect import detect, DetectorFactory\n",
    "    from langdetect.lang_detect_exception import LangDetectException\n",
    "    DetectorFactory.seed = 0\n",
    "    _langdetect_installed = True\n",
    "except ImportError:\n",
    "    print(\"langdetect not found. You might need to install it (`pip install langdetect`)\")\n",
    "    _langdetect_installed = False\n",
    "\n",
    "\n",
    "# Check if rouge-score is available (for evaluation later)\n",
    "try:\n",
    "    from rouge_score import rouge_scorer, scoring\n",
    "except ImportError:\n",
    "    print(\"rouge-score not found. You might need to install it (`pip install rouge-score nltk`)\")\n",
    "    try:\n",
    "        import nltk\n",
    "        nltk.download('punkt', quiet=True)\n",
    "    except ImportError:\n",
    "        print(\"NLTK not found, which might be needed for rouge-score.\")\n",
    "\n",
    "\n",
    "from tqdm.notebook import tqdm # Use tqdm.notebook for Jupyter/Vertex AI Notebooks\n",
    "\n",
    "# --- Configuration ---\n",
    "OUTPUT_DIR = 'model_attention_files' # Changed directory name\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"Ensuring output directory exists: {os.path.abspath(OUTPUT_DIR)}\")\n",
    "\n",
    "# File Paths (relative to OUTPUT_DIR)\n",
    "INPUT_JSONL = 'mergedt02.jsonl' # Input dataset (Make sure this is correct)\n",
    "OUTPUT_PARQUET = os.path.join(OUTPUT_DIR, 'processed_dataframe.parquet') # Cached processed data\n",
    "TOKENIZER_MODEL_PREFIX = os.path.join(OUTPUT_DIR, 'pib_summarizer_spm_50k') # Prefix for SentencePiece model\n",
    "TOKENIZER_MODEL_FILE = f'{TOKENIZER_MODEL_PREFIX}.model'\n",
    "# LOG_DIR will be used for TensorBoard path in custom loop\n",
    "LOG_DIR = os.path.join(OUTPUT_DIR, \"logs\", \"custom_train\", datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "# MODEL_SAVE_PATH is now used for the *best* model path in custom loop\n",
    "BEST_MODEL_SAVE_PATH = os.path.join(OUTPUT_DIR, 'pib_summarizer_attention_best.keras')\n",
    "# Checkpoint directory for intermediate saves\n",
    "CHECKPOINT_DIR = os.path.join(OUTPUT_DIR, 'training_checkpoints')\n",
    "\n",
    "\n",
    "# Data Processing Params (Keep Same)\n",
    "MIN_INPUT_WORDS = 20\n",
    "MIN_SUMMARY_WORDS = 5\n",
    "LANG_DETECT_THRESHOLD = 0.90\n",
    "\n",
    "# Tokenizer Params (Keep Same)\n",
    "VOCAB_SIZE = 30000 # Target vocabulary size\n",
    "PAD_ID = 0\n",
    "UNK_ID = 1\n",
    "START_ID = 2\n",
    "END_ID = 3\n",
    "\n",
    "# Model Hyperparameters (Adjust as needed, add Attention Units)\n",
    "EMBEDDING_DIM = 100 # Adjusted based on previous log output\n",
    "LSTM_UNITS = 128   # Encoder units per direction (Adjusted based on previous log)\n",
    "DECODER_LSTM_UNITS = LSTM_UNITS * 2 # 512 (Adjusted based on previous log)\n",
    "NUM_ENCODER_LAYERS = 1\n",
    "NUM_DECODER_LAYERS = 1\n",
    "DROPOUT_RATE = 0.2\n",
    "MAX_LEN_INPUT = 1024\n",
    "MAX_LEN_SUMMARY = 150\n",
    "ATTENTION_UNITS = 512 # Size of the dense layer in attention calculation (Adjusted based on previous log)\n",
    "\n",
    "# Training Params (Adjust Batch Size, Epochs may be indicative for custom loop)\n",
    "BATCH_SIZE = 32 # Adjust based on GPU memory with attention\n",
    "EPOCHS = 30 # Max epochs for custom loop, early stopping will be manual\n",
    "LEARNING_RATE = 0.001\n",
    "# <<< NEW: Scheduled Sampling Params (Example values) >>>\n",
    "INITIAL_SAMPLING_PROB = 1.0 # Start with 100% teacher forcing\n",
    "SAMPLING_PROB_DECAY_RATE = 0.99 # Example: Multiplicative decay per epoch/step\n",
    "MIN_SAMPLING_PROB = 0.1 # Minimum probability to use ground truth\n",
    "\n",
    "# <<< NEW: Manual Early Stopping/LR Reduction Params (Example values) >>>\n",
    "# --- ADDED THESE THREE LINES ---\n",
    "EARLY_STOPPING_PATIENCE_MANUAL = 5 # Epochs to wait for val_loss improvement\n",
    "REDUCE_LR_PATIENCE_MANUAL = 3    # Epochs to wait for val_loss improvement before reducing LR\n",
    "REDUCE_LR_FACTOR_MANUAL = 0.2    # Factor to reduce LR by\n",
    "# --- END OF ADDED LINES ---\n",
    "\n",
    "# Inference Params\n",
    "# <<< NEW: Beam Search Width >>>\n",
    "BEAM_WIDTH = 4 # Set desired beam width\n",
    "\n",
    "# --- Setup Logging ---\n",
    "LOG_FILE_PATH = os.path.join(OUTPUT_DIR, 'training_log_attention.log') # Changed log name\n",
    "# Ensure logging handlers are cleared if re-running the cell in the same kernel session\n",
    "root_logger = logging.getLogger()\n",
    "if root_logger.hasHandlers():\n",
    "    root_logger.handlers.clear()\n",
    "\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                    handlers=[\n",
    "                        logging.FileHandler(LOG_FILE_PATH),\n",
    "                        logging.StreamHandler()\n",
    "                    ])\n",
    "print(f\"Logging setup complete. Log file: {LOG_FILE_PATH}\")\n",
    "\n",
    "\n",
    "# --- Debugging Info ---\n",
    "print(\"\\n--- Configuration (Attention, SS, Beam Search) ---\")\n",
    "logging.info(\"--- Configuration (Attention, SS, Beam Search) ---\")\n",
    "print(f\"TensorFlow Version: {tf.__version__}\"); logging.info(f\"TensorFlow Version: {tf.__version__}\")\n",
    "if spm: print(f\"SentencePiece Version: {spm.__version__}\"); logging.info(f\"SentencePiece Version: {spm.__version__}\")\n",
    "else: print(\"SentencePiece not imported.\"); logging.warning(\"SentencePiece not imported.\")\n",
    "\n",
    "if _langdetect_installed:\n",
    "    try:\n",
    "        langdetect_version = importlib.metadata.version(\"langdetect\")\n",
    "        print(f\"Langdetect Version: {langdetect_version}\"); logging.info(f\"Langdetect Version: {langdetect_version}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Langdetect Version: Could not determine version ({e})\"); logging.warning(f\"Langdetect Version: Could not determine version ({e})\")\n",
    "else:\n",
    "     print(\"Langdetect: Not installed or failed to import.\"); logging.warning(\"Langdetect: Not installed or failed to import.\")\n",
    "\n",
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "print(f\"GPU Available: {gpu_devices}\"); logging.info(f\"GPU Available: {gpu_devices}\")\n",
    "print(f\"Input JSONL: {INPUT_JSONL}\"); logging.info(f\"Input JSONL: {INPUT_JSONL}\")\n",
    "print(f\"Output Parquet Cache: {OUTPUT_PARQUET}\"); logging.info(f\"Output Parquet Cache: {OUTPUT_PARQUET}\")\n",
    "print(f\"Tokenizer Model Prefix: {TOKENIZER_MODEL_PREFIX}\"); logging.info(f\"Tokenizer Model Prefix: {TOKENIZER_MODEL_PREFIX}\")\n",
    "print(f\"Model Checkpoint Dir: {CHECKPOINT_DIR}\"); logging.info(f\"Model Checkpoint Dir: {CHECKPOINT_DIR}\")\n",
    "print(f\"Best Model Save Path: {BEST_MODEL_SAVE_PATH}\"); logging.info(f\"Best Model Save Path: {BEST_MODEL_SAVE_PATH}\")\n",
    "print(f\"TensorBoard Log Dir: {LOG_DIR}\"); logging.info(f\"TensorBoard Log Dir: {LOG_DIR}\")\n",
    "print(f\"Vocab Size: {VOCAB_SIZE}\"); logging.info(f\"Vocab Size: {VOCAB_SIZE}\")\n",
    "print(f\"Embedding Dim: {EMBEDDING_DIM}\"); logging.info(f\"Embedding Dim: {EMBEDDING_DIM}\")\n",
    "print(f\"Encoder LSTM Units (per dir): {LSTM_UNITS}\"); logging.info(f\"Encoder LSTM Units (per dir): {LSTM_UNITS}\")\n",
    "print(f\"Decoder LSTM Units: {DECODER_LSTM_UNITS}\"); logging.info(f\"Decoder LSTM Units: {DECODER_LSTM_UNITS}\")\n",
    "print(f\"Attention Units: {ATTENTION_UNITS}\"); logging.info(f\"Attention Units: {ATTENTION_UNITS}\")\n",
    "print(f\"Encoder Layers: {NUM_ENCODER_LAYERS}\"); logging.info(f\"Encoder Layers: {NUM_ENCODER_LAYERS}\")\n",
    "print(f\"Decoder Layers: {NUM_DECODER_LAYERS}\"); logging.info(f\"Decoder Layers: {NUM_DECODER_LAYERS}\")\n",
    "print(f\"Max Input Length: {MAX_LEN_INPUT}\"); logging.info(f\"Max Input Length: {MAX_LEN_INPUT}\")\n",
    "print(f\"Max Summary Length: {MAX_LEN_SUMMARY}\"); logging.info(f\"Max Summary Length: {MAX_LEN_SUMMARY}\")\n",
    "print(f\"Batch Size: {BATCH_SIZE}\"); logging.info(f\"Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"Max Epochs: {EPOCHS}\"); logging.info(f\"Max Epochs: {EPOCHS}\")\n",
    "print(f\"Learning Rate: {LEARNING_RATE}\"); logging.info(f\"Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"Initial Sampling Prob: {INITIAL_SAMPLING_PROB}\"); logging.info(f\"Initial Sampling Prob: {INITIAL_SAMPLING_PROB}\")\n",
    "print(f\"Sampling Prob Decay: {SAMPLING_PROB_DECAY_RATE}\"); logging.info(f\"Sampling Prob Decay: {SAMPLING_PROB_DECAY_RATE}\")\n",
    "# Print the newly added manual parameters\n",
    "print(f\"Manual Early Stopping Patience: {EARLY_STOPPING_PATIENCE_MANUAL}\"); logging.info(f\"Manual Early Stopping Patience: {EARLY_STOPPING_PATIENCE_MANUAL}\")\n",
    "print(f\"Manual Reduce LR Patience: {REDUCE_LR_PATIENCE_MANUAL}\"); logging.info(f\"Manual Reduce LR Patience: {REDUCE_LR_PATIENCE_MANUAL}\")\n",
    "print(f\"Manual Reduce LR Factor: {REDUCE_LR_FACTOR_MANUAL}\"); logging.info(f\"Manual Reduce LR Factor: {REDUCE_LR_FACTOR_MANUAL}\")\n",
    "print(f\"Beam Width: {BEAM_WIDTH}\"); logging.info(f\"Beam Width: {BEAM_WIDTH}\")\n",
    "print(\"-\" * 30); logging.info(\"-\" * 30)\n",
    "\n",
    "# Also print the paths used in Block 8 for confirmation\n",
    "print(f\"Checkpoint directory (for custom loop): {CHECKPOINT_DIR}\")\n",
    "print(f\"Best model save path (for custom loop): {BEST_MODEL_SAVE_PATH}\")\n",
    "print(f\"TensorBoard Log Directory (for custom loop): {LOG_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6adc0e7-ccb9-4176-8bfe-ff0068378d17",
   "metadata": {},
   "source": [
    "## Data Loading and Initial Cleaning Functions + Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6331f0c-d255-49c7-8141-08fd87030b6f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 18:30:33,319 - INFO - Attempting to load data from: mergedt02.jsonl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cleaning Function Test (Block 2) ---\n",
      "Original:\n",
      "\n",
      "Press Information Bureau\n",
      "Government of India\n",
      "Ministry of Finance\n",
      "Posted on: 25 JUL 2024 6:00PM by PIB Delhi\n",
      "This is a [ 1] test document from file:///path/to/doc.pdf. Check www.example.com.\n",
      "It has “quotes” and ‘apostrophes’.   Extra spaces. And some !?.,'\"- punctuation.\n",
      "Bad chars: #$%^&*()_+={}[]|\\:;<>~/\n",
      "***DS/AK***\n",
      "(Release ID: 12345)\n",
      "\n",
      "\n",
      "Cleaned:\n",
      "25 jul 2024 600pm by this is a test document from . check it has \"quotes\" and 'apostrophes'. extra spaces. and some !?.,'\"- punctuation. bad chars\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 18:30:34,447 - INFO - Loaded 37392 records successfully out of 37392 lines (0 failed/skipped).\n",
      "2025-04-24 18:30:34,465 - INFO - Block 2 completed. `raw_df` created successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Data Loading ---\n",
      "Processed 37392 lines from mergedt02.jsonl.\n",
      "Successfully loaded 37392 records.\n",
      "Skipped/failed 0 lines.\n",
      "Columns: ['pdf_filename', 'extracted_text', 'gemini_summary', 'gemini_topics', 'preserve_case', 'stopword_removed']\n",
      "Data Types:\n",
      " pdf_filename        object\n",
      "extracted_text      object\n",
      "gemini_summary      object\n",
      "gemini_topics       object\n",
      "preserve_case         bool\n",
      "stopword_removed      bool\n",
      "dtype: object\n",
      "Sample record (first 5 rows raw_df):\n",
      "                 pdf_filename  \\\n",
      "0  PIB_115363_2015_02_11.pdf   \n",
      "1  PIB_115365_2015_02_11.pdf   \n",
      "2  PIB_115367_2015_02_12.pdf   \n",
      "3  PIB_115368_2015_02_12.pdf   \n",
      "4  PIB_115369_2015_02_12.pdf   \n",
      "\n",
      "                                      extracted_text  \\\n",
      "0  States can flexibly use of central assistance ...   \n",
      "1  Consultative Committee of the Ministry of Tour...   \n",
      "2  Prime Minister's Office\\nPM pays tributes to S...   \n",
      "3  Prime Minister's Office\\nPM appalled at the ne...   \n",
      "4  Shri Bandaru Dattatreya Chairs Tripartite Meet...   \n",
      "\n",
      "                                      gemini_summary  \\\n",
      "0  A meeting of the Parliamentary Consultative Co...   \n",
      "1  A meeting of the Parliamentary Consultative Co...   \n",
      "2  Prime Minister Narendra Modi paid tributes to ...   \n",
      "3  Prime Minister Narendra Modi expressed his str...   \n",
      "4  This press release summarizes a tripartite mee...   \n",
      "\n",
      "                                       gemini_topics  preserve_case  \\\n",
      "0                                               None           True   \n",
      "1  [{'main_topic': 'Parliamentary Consultative Co...           True   \n",
      "2  [{'main_topic': 'Swami Dayananda Saraswati'}, ...           True   \n",
      "3  [{'main_topic': 'Prime Minister Narendra Modi'...           True   \n",
      "4  [{'main_topic': 'Amendments to Employees’ Prov...           True   \n",
      "\n",
      "   stopword_removed  \n",
      "0             False  \n",
      "1             False  \n",
      "2             False  \n",
      "3             False  \n",
      "4             False  \n",
      "------------------------------\n",
      "Block 2 completed. `raw_df` created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Block 2: Data Loading and Initial Cleaning Functions + Execution\n",
    "# (This block remains unchanged from the original working code)\n",
    "\n",
    "# --- Function Definitions ---\n",
    "def load_data(jsonl_path):\n",
    "    \"\"\"Loads data from a JSONL file.\"\"\"\n",
    "    logging.info(f\"Attempting to load data from: {jsonl_path}\")\n",
    "    data = []\n",
    "    lines_processed = 0\n",
    "    lines_failed = 0\n",
    "    if not os.path.exists(jsonl_path):\n",
    "        logging.error(f\"Input file not found: {jsonl_path}\")\n",
    "        print(f\"\\n--- Data Loading Error ---\")\n",
    "        print(f\"Error: Input file not found at {jsonl_path}\")\n",
    "        print(\"Please ensure the file exists and the path is correct.\")\n",
    "        print(\"-\" * 30)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    try:\n",
    "        with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                lines_processed += 1\n",
    "                try:\n",
    "                    # Skip empty lines\n",
    "                    if not line.strip():\n",
    "                        logging.warning(f\"Skipping empty line: {i+1}\")\n",
    "                        lines_failed +=1\n",
    "                        continue\n",
    "                    data.append(json.loads(line))\n",
    "                except json.JSONDecodeError as e:\n",
    "                    logging.warning(f\"Skipping malformed JSON line: {i+1}. Error: {e}\")\n",
    "                    lines_failed += 1\n",
    "                    continue\n",
    "        df = pd.DataFrame(data)\n",
    "        logging.info(f\"Loaded {len(df)} records successfully out of {lines_processed} lines ({lines_failed} failed/skipped).\")\n",
    "        # --- Debugging Info ---\n",
    "        print(\"\\n--- Data Loading ---\")\n",
    "        print(f\"Processed {lines_processed} lines from {jsonl_path}.\")\n",
    "        print(f\"Successfully loaded {len(df)} records.\")\n",
    "        print(f\"Skipped/failed {lines_failed} lines.\")\n",
    "        if not df.empty:\n",
    "            print(\"Columns:\", df.columns.tolist())\n",
    "            print(\"Data Types:\\n\", df.dtypes)\n",
    "            print(\"Sample record (first 5 rows raw_df):\\n\", df.head())\n",
    "        else:\n",
    "            print(\"Loaded DataFrame (raw_df) is empty. Check input file content and format.\")\n",
    "            logging.warning(\"Loaded DataFrame (raw_df) is empty after processing the file.\")\n",
    "        print(\"-\" * 30)\n",
    "        return df\n",
    "    except FileNotFoundError: # This case is already handled above, but keep for robustness\n",
    "        logging.error(f\"Error: Input file not found at {jsonl_path}\")\n",
    "        print(f\"\\n--- Data Loading Error ---\")\n",
    "        print(f\"Error: Input file not found at {jsonl_path}\")\n",
    "        print(\"-\" * 30)\n",
    "        return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred during data loading: {e}\", exc_info=True)\n",
    "        print(f\"\\n--- Data Loading Error ---\")\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        print(\"-\" * 30)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Applies cleaning steps to a single text string.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    text = text.lower()\n",
    "    text = text.replace('“', '\"').replace('”', '\"').replace(\"‘\", \"'\").replace(\"’\", \"'\")\n",
    "    text = re.sub(r'file:///[^ ]+\\.pdf', '', text)\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'\\[\\s*\\d+\\s*\\]', '', text)\n",
    "    headers_footers = [\n",
    "        r\"press information bureau\", r\"government of india\", r\"ministry of [\\w\\s]+\",\n",
    "        r\"posted on:\\s*\\d{1,2}\\s+\\w{3,}\\s+\\d{4}\\s+\\d{1,2}:\\d{2}\\s*[ap]m\\s*(by pib \\w+)?\",\n",
    "        r\"release id: \\d+\",\n",
    "        r\"\\(release id.*?\\)\",\"pib \\w+\",\n",
    "        r\"\\*{3,}\\s*[a-z\\\\/]+\\s*\\*{3,}\", # Footer pattern (e.g., ***DS/AK***) - Need double backslash for literal \\\n",
    "    ]\n",
    "    for pattern in headers_footers:\n",
    "       text = re.sub(pattern, '', text, flags=re.IGNORECASE)\n",
    "\n",
    "    text = text.replace('\\\\n', ' ') # Replace literal '\\n' strings if they exist\n",
    "    text = text.replace('\\n', ' ') # Replace actual newline characters\n",
    "    text = re.sub(r'\\s+', ' ', text) # Collapse multiple whitespace characters\n",
    "    # Keep basic punctuation useful for summarization\n",
    "    text = re.sub(r\"[^a-z0-9\\s.,!?'\\\"-]\", \"\", text) # Allow letters, numbers, space, and .,!?'\"-\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "# --- Execution for Block 2 ---\n",
    "\n",
    "# --- Debugging: Test cleaning function ---\n",
    "print(\"\\n--- Cleaning Function Test (Block 2) ---\")\n",
    "test_text = \"\"\"\n",
    "Press Information Bureau\\nGovernment of India\\nMinistry of Finance\\nPosted on: 25 JUL 2024 6:00PM by PIB Delhi\n",
    "This is a [ 1] test document from file:///path/to/doc.pdf. Check www.example.com.\n",
    "It has “quotes” and ‘apostrophes’.   Extra spaces. And some !?.,'\\\"- punctuation.\n",
    "Bad chars: #$%^&*()_+={}[]|\\\\:;<>~/\n",
    "***DS/AK***\n",
    "(Release ID: 12345)\n",
    "\"\"\"\n",
    "cleaned_test = clean_text(test_text)\n",
    "print(f\"Original:\\n{test_text}\")\n",
    "print(f\"\\nCleaned:\\n{cleaned_test}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Load the raw data into a DataFrame called 'raw_df'\n",
    "raw_df = load_data(INPUT_JSONL)\n",
    "\n",
    "# <<< CRUCIAL CHECK >>>\n",
    "if 'raw_df' not in locals() or raw_df.empty:\n",
    "    print(\"************************************************************\")\n",
    "    print(\"ERROR: Block 2 failed to load data into `raw_df`.\")\n",
    "    print(\"Cannot proceed to Block 3. Please check the 'Data Loading' output above.\")\n",
    "    print(f\"Verify the INPUT_JSONL path ('{INPUT_JSONL}') and the file content.\")\n",
    "    print(\"************************************************************\")\n",
    "    # Optional: Stop execution here in a notebook context if desired\n",
    "    # raise RuntimeError(\"Failed to load raw data. Stopping execution.\")\n",
    "else:\n",
    "    print(\"Block 2 completed. `raw_df` created successfully.\")\n",
    "    logging.info(\"Block 2 completed. `raw_df` created successfully.\")\n",
    "\n",
    "# A variable 'processed_df' will be created in the next block IF raw_df is valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75ebefb2-1ee9-445f-8ffa-0fa5df8e57aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 18:30:34,489 - INFO - Preprocessing started. Cache path: model_attention_files/processed_dataframe.parquet\n",
      "2025-04-24 18:30:34,490 - INFO - Loading processed data from cache: model_attention_files/processed_dataframe.parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceeding with preprocessing using `raw_df`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 18:30:35,017 - INFO - Successfully loaded 37310 records from cache.\n",
      "2025-04-24 18:30:35,020 - INFO - Block 3 completed. `processed_df` created or loaded successfully.\n",
      "2025-04-24 18:30:35,205 - INFO - Cleaned up raw_df from memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preprocessing Pipeline (Block 3) ---\n",
      "Loaded 37310 records from cache: model_attention_files/processed_dataframe.parquet\n",
      "Columns: ['cleaned_text', 'target_summary']\n",
      "Sample processed data (from cache):\n",
      "                                         cleaned_text  \\\n",
      "0  states can flexibly use of central assistance ...   \n",
      "1  consultative committee of the  dr mahesh sharm...   \n",
      "2  prime minister's office pm pays tributes to sw...   \n",
      "3  prime minister's office pm appalled at the new...   \n",
      "4  shri bandaru dattatreya chairs tripartite meet...   \n",
      "\n",
      "                                      target_summary  \n",
      "0  <start> a meeting of the parliamentary consult...  \n",
      "1  <start> a meeting of the parliamentary consult...  \n",
      "2  <start> prime minister narendra modi paid trib...  \n",
      "3  <start> prime minister narendra modi expressed...  \n",
      "4  <start> this press release summarizes a tripar...  \n",
      "------------------------------\n",
      "Block 3 completed. `processed_df` created or loaded successfully.\n",
      "Cleaning up raw_df from memory...\n"
     ]
    }
   ],
   "source": [
    "# Block 3: Preprocessing Pipeline + Execution\n",
    "# (This block also remains unchanged from the original working code)\n",
    "\n",
    "# --- Function Definition ---\n",
    "def preprocess_data(df, text_col='extracted_text', summary_col='gemini_summary', cache_path=OUTPUT_PARQUET):\n",
    "    \"\"\"Applies the full preprocessing pipeline to the dataframe.\"\"\"\n",
    "    logging.info(f\"Preprocessing started. Cache path: {cache_path}\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # --- Cache Check ---\n",
    "    if os.path.exists(cache_path):\n",
    "        try:\n",
    "            logging.info(f\"Loading processed data from cache: {cache_path}\")\n",
    "            processed_df_from_cache = pd.read_parquet(cache_path)\n",
    "            # Basic validation of cached data\n",
    "            if not isinstance(processed_df_from_cache, pd.DataFrame) or processed_df_from_cache.empty:\n",
    "                 raise ValueError(\"Cached file did not contain a valid DataFrame.\")\n",
    "            if 'cleaned_text' not in processed_df_from_cache.columns or 'target_summary' not in processed_df_from_cache.columns:\n",
    "                 raise ValueError(\"Cached DataFrame missing required columns ('cleaned_text', 'target_summary').\")\n",
    "\n",
    "            logging.info(f\"Successfully loaded {len(processed_df_from_cache)} records from cache.\")\n",
    "            # --- Debugging Info ---\n",
    "            print(\"\\n--- Preprocessing Pipeline (Block 3) ---\")\n",
    "            print(f\"Loaded {len(processed_df_from_cache)} records from cache: {cache_path}\")\n",
    "            print(\"Columns:\", processed_df_from_cache.columns.tolist())\n",
    "            print(\"Sample processed data (from cache):\\n\", processed_df_from_cache.head())\n",
    "            print(\"-\" * 30)\n",
    "            return processed_df_from_cache # Return cached data\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to load or parse cache file {cache_path}: {e}\", exc_info=True)\n",
    "            print(f\"\\n--- Preprocessing Cache Error ---\")\n",
    "            print(f\"Error reading cache file {cache_path}: {e}. Proceeding with reprocessing.\")\n",
    "            # If cache fails, delete it to force reprocessing next time\n",
    "            try:\n",
    "                os.remove(cache_path)\n",
    "                logging.info(f\"Removed corrupted cache file: {cache_path}\")\n",
    "            except OSError:\n",
    "                 pass # Ignore error if file couldn't be removed\n",
    "\n",
    "\n",
    "    # --- Preprocessing Steps (if cache doesn't exist or failed) ---\n",
    "    print(\"\\n--- Preprocessing Pipeline (Block 3) ---\")\n",
    "    print(f\"Cache not used or failed. Starting processing of {len(df)} raw records...\")\n",
    "    logging.info(f\"Cache not used or failed. Starting processing of {len(df)} raw records...\")\n",
    "\n",
    "\n",
    "    # Ensure required columns exist in the input df\n",
    "    if text_col not in df.columns or summary_col not in df.columns:\n",
    "        logging.error(f\"Input DataFrame missing required columns: '{text_col}' or '{summary_col}'. Available: {df.columns.tolist()}\")\n",
    "        print(\"\\n--- Preprocessing Pipeline Error ---\")\n",
    "        print(f\"Input DataFrame missing required columns: '{text_col}' or '{summary_col}'. Available: {df.columns.tolist()}\")\n",
    "        print(\"Cannot preprocess.\")\n",
    "        print(\"-\" * 30)\n",
    "        return pd.DataFrame() # Return empty dataframe\n",
    "\n",
    "    # Make a copy to avoid SettingWithCopyWarning on the original raw_df\n",
    "    df_processed = df[[text_col, summary_col]].copy() # Only copy necessary columns\n",
    "\n",
    "    logging.info(\"Applying text cleaning...\")\n",
    "    tqdm.pandas(desc=\"Cleaning Text\")\n",
    "    df_processed['cleaned_text'] = df_processed[text_col].progress_apply(clean_text)\n",
    "    tqdm.pandas(desc=\"Cleaning Summary\")\n",
    "    df_processed['cleaned_summary'] = df_processed[summary_col].progress_apply(clean_text)\n",
    "\n",
    "    initial_count = len(df_processed)\n",
    "    logging.info(f\"Initial record count for processing: {initial_count}\")\n",
    "    print(f\"Initial record count for processing: {initial_count}\")\n",
    "    print(\"Sample after basic cleaning:\")\n",
    "    cols_to_display = ['cleaned_text', 'cleaned_summary']\n",
    "    if not df_processed.empty:\n",
    "        print(df_processed[cols_to_display].head())\n",
    "\n",
    "\n",
    "    # Language Filtering\n",
    "    if _langdetect_installed: # Only run if library is available\n",
    "        logging.info(\"Applying language filtering...\")\n",
    "        valid_indices = []\n",
    "        skipped_lang = 0\n",
    "        for index, text in tqdm(df_processed['cleaned_text'].items(), total=len(df_processed), desc=\"Language Filtering\"):\n",
    "            try:\n",
    "                # Skip if text is too short or missing (less prone to langdetect errors)\n",
    "                if not text or len(text.split()) < 5: # Use word count as proxy for meaningful text\n",
    "                    skipped_lang += 1\n",
    "                    continue\n",
    "                # Detect on first 500 chars for efficiency\n",
    "                lang = detect(text[:500])\n",
    "                if lang == 'en': # Simple check for English\n",
    "                    valid_indices.append(index)\n",
    "                else:\n",
    "                    skipped_lang += 1\n",
    "            except LangDetectException:\n",
    "                # This happens on very short/ambiguous text, treat as non-English\n",
    "                skipped_lang += 1\n",
    "            except Exception as e:\n",
    "                # Catch any other unexpected errors during detection\n",
    "                logging.warning(f\"Language detection error on index {index}: {e}\")\n",
    "                skipped_lang += 1\n",
    "\n",
    "        df_processed = df_processed.loc[valid_indices].copy() # Use .loc and .copy()\n",
    "        lang_filtered_count = len(df_processed)\n",
    "        logging.info(f\"Language filtering: Kept {lang_filtered_count}, Removed {initial_count - lang_filtered_count} non-English/error/short records.\")\n",
    "        print(f\"Count after language filtering: {lang_filtered_count} ({initial_count - lang_filtered_count} removed)\")\n",
    "    else:\n",
    "        print(\"Skipping language filtering as langdetect is not available.\")\n",
    "        logging.warning(\"Skipping language filtering as langdetect is not available.\")\n",
    "        lang_filtered_count = len(df_processed) # Count remains the same\n",
    "\n",
    "\n",
    "    # Length Filtering (only if records remain)\n",
    "    original_count_before_len_filter = len(df_processed)\n",
    "    if not df_processed.empty:\n",
    "        logging.info(\"Applying length filtering...\")\n",
    "        # Avoid SettingWithCopyWarning using .loc for assignments\n",
    "        df_processed.loc[:, 'text_word_count'] = df_processed['cleaned_text'].apply(lambda x: len(x.split()))\n",
    "        df_processed.loc[:, 'summary_word_count'] = df_processed['cleaned_summary'].apply(lambda x: len(x.split()))\n",
    "\n",
    "        df_processed = df_processed[\n",
    "            (df_processed['text_word_count'] >= MIN_INPUT_WORDS) &\n",
    "            (df_processed['summary_word_count'] >= MIN_SUMMARY_WORDS)\n",
    "        ].copy() # Use boolean indexing and copy\n",
    "        len_filtered_count = len(df_processed)\n",
    "        logging.info(f\"Length filtering: Kept {len_filtered_count}, Removed {original_count_before_len_filter - len_filtered_count} short records.\")\n",
    "        print(f\"Count after length filtering: {len_filtered_count} ({original_count_before_len_filter - len_filtered_count} removed)\")\n",
    "    else:\n",
    "        print(\"Skipping length filtering as DataFrame is empty after previous steps.\")\n",
    "        logging.warning(\"Skipping length filtering due to empty DataFrame.\")\n",
    "        len_filtered_count = 0\n",
    "\n",
    "    # Add Start/End Tokens (only if records remain)\n",
    "    final_processed_df = pd.DataFrame() # Initialize\n",
    "    if not df_processed.empty:\n",
    "        logging.info(\"Adding <start> and <end> tokens to summaries...\")\n",
    "        # Use .loc for assignment to avoid warnings\n",
    "        df_processed.loc[:, 'cleaned_summary_tagged'] = df_processed['cleaned_summary'].apply(lambda x: f\"<start> {x} <end>\")\n",
    "        # Final Selection and Renaming\n",
    "        final_processed_df = df_processed[['cleaned_text', 'cleaned_summary_tagged']].rename(columns={'cleaned_summary_tagged': 'target_summary'})\n",
    "    else:\n",
    "        print(\"Skipping token tagging as DataFrame is empty.\")\n",
    "        logging.warning(\"Skipping token tagging due to empty DataFrame.\")\n",
    "\n",
    "\n",
    "    # --- Final Output and Caching ---\n",
    "    final_count = len(final_processed_df)\n",
    "    print(f\"Final processed record count: {final_count}\")\n",
    "    logging.info(f\"Final processed record count: {final_count}\")\n",
    "    if not final_processed_df.empty:\n",
    "        print(\"Sample processed data (final):\\n\", final_processed_df.head())\n",
    "        # Cache the result\n",
    "        try:\n",
    "            logging.info(f\"Caching processed data to: {cache_path}\")\n",
    "            final_processed_df.to_parquet(cache_path, index=False)\n",
    "            logging.info(\"Caching successful.\")\n",
    "            print(f\"Processed data cached successfully to {cache_path}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to cache processed data to {cache_path}: {e}\", exc_info=True)\n",
    "            print(f\"\\n--- Caching Error ---\")\n",
    "            print(f\"Failed to cache data to {cache_path}: {e}\")\n",
    "    else:\n",
    "        logging.warning(\"Processed DataFrame is empty. No data will be cached.\")\n",
    "        print(\"Warning: Processed DataFrame is empty. Nothing to cache.\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    logging.info(f\"Preprocessing finished in {end_time - start_time:.2f} seconds.\")\n",
    "    print(f\"Preprocessing finished in {end_time - start_time:.2f} seconds.\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    return final_processed_df\n",
    "\n",
    "# --- Execution for Block 3 ---\n",
    "processed_df = pd.DataFrame() # Initialize as empty DataFrame\n",
    "\n",
    "# Only proceed if raw_df from Block 2 exists and is not empty\n",
    "if 'raw_df' in locals() and isinstance(raw_df, pd.DataFrame) and not raw_df.empty:\n",
    "    print(\"Proceeding with preprocessing using `raw_df`...\")\n",
    "    processed_df = preprocess_data(raw_df, cache_path=OUTPUT_PARQUET)\n",
    "else:\n",
    "    print(\"Skipping Block 3 execution because `raw_df` is not available or is empty.\")\n",
    "    print(\"Check the output of Block 2 for errors.\")\n",
    "    logging.error(\"Skipping Block 3 execution because `raw_df` is not available or is empty.\")\n",
    "\n",
    "# <<< CRUCIAL CHECK >>>\n",
    "if 'processed_df' not in locals() or processed_df.empty:\n",
    "    print(\"************************************************************\")\n",
    "    print(\"WARNING: Block 3 resulted in an empty `processed_df`.\")\n",
    "    print(\"This could be due to loading errors, aggressive filtering, or issues during processing.\")\n",
    "    print(\"Subsequent blocks (Tokenizer, Model Training) will likely fail or be skipped.\")\n",
    "    print(\"Review the 'Preprocessing Pipeline' output above.\")\n",
    "    print(\"************************************************************\")\n",
    "    logging.warning(\"Block 3 resulted in an empty `processed_df`.\")\n",
    "else:\n",
    "    print(\"Block 3 completed. `processed_df` created or loaded successfully.\")\n",
    "    logging.info(\"Block 3 completed. `processed_df` created or loaded successfully.\")\n",
    "\n",
    "# Clean up raw_df if memory is a concern and processing was successful\n",
    "if 'processed_df' in locals() and not processed_df.empty and 'raw_df' in locals():\n",
    "   print(\"Cleaning up raw_df from memory...\")\n",
    "   del raw_df\n",
    "   gc.collect()\n",
    "   logging.info(\"Cleaned up raw_df from memory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0259a400-a7bb-456a-8f0a-b26faa8ce5a6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Block 4: Tokenizer Training and Usage + Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "698e85b2-1b47-4b62-a79d-efc621fd6a47",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (19.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c364143-ed3d-4575-9480-3f28c186de69",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 18:30:37,075 - INFO - Block 4: `processed_df` is valid. Proceeding with tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block 4: `processed_df` is valid. Proceeding with tokenizer.\n",
      "Preparing data for tokenizer training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 18:30:37,114 - INFO - Found existing tokenizer model: model_attention_files/pib_summarizer_spm_50k.model. Skipping training.\n",
      "2025-04-24 18:30:37,115 - INFO - Loading SentencePiece tokenizer from: model_attention_files/pib_summarizer_spm_50k.model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full corpus size for tokenizer: 74620\n",
      "Tokenizer model model_attention_files/pib_summarizer_spm_50k.model already exists. Skipping training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 18:30:37,163 - INFO - Successfully loaded tokenizer: model_attention_files/pib_summarizer_spm_50k.model\n",
      "2025-04-24 18:30:37,164 - INFO - Tokenizing 37310 texts with max_len=1024...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Tokenizer Loading ---\n",
      "Successfully loaded tokenizer: model_attention_files/pib_summarizer_spm_50k.model\n",
      "Vocabulary Size: 30000\n",
      "PAD ID (<pad>): 0 (Config: 0)\n",
      "UNK ID (<unk>): 1 (Config: 1)\n",
      "BOS/Start ID (<start>): 2 (Config: 2)\n",
      "EOS/End ID (<end>): 3 (Config: 3)\n",
      "------------------------------\n",
      "Tokenizing cleaned text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 18:30:40,863 - INFO - Tokenization successful for 37310 texts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Tokenization ---\n",
      "Tokenized 37310 texts.\n",
      "Shape of padded sequences: (37310, 1024)\n",
      "Original Text (sample 0): states can flexibly use of central assistance of inr4,000 per toilet, says shri venkaiah naidu parli...\n",
      "Tokenized IDs (sample 0): [   84   183 18762   759   296   271     7    86   291     7    69  7086\n",
      "   105  3498     5  2017    31  1648   824   713]...\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 18:30:41,129 - INFO - Tokenizing 37310 texts with max_len=150...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing target summaries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 18:30:42,161 - INFO - Tokenization successful for 37310 texts.\n",
      "2025-04-24 18:30:42,225 - INFO - Tokenization successful. Shapes: Encoder=(37310, 1024), DecoderIn=(37310, 149), DecoderOut=(37310, 149)\n",
      "2025-04-24 18:30:42,226 - INFO - Block 4 completed successfully. Tokenized data created.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Tokenization ---\n",
      "Tokenized 37310 texts.\n",
      "Shape of padded sequences: (37310, 150)\n",
      "Original Text (sample 0): <start> a meeting of the parliamentary consultative committee discussed shortcomings in the jawaharl...\n",
      "Tokenized IDs (sample 0): [   12     1  8371     1    14    88     7     4  1381    12  3207   195\n",
      "   598 10431    11     4  3281  3181  2737   308]...\n",
      "------------------------------\n",
      "Creating decoder input/target sequences...\n",
      "\n",
      "--- Data Shapes After Tokenization & Shifting ---\n",
      "Encoder Input Shape: (37310, 1024)\n",
      "Decoder Input Shape: (37310, 149)\n",
      "Decoder Target Shape: (37310, 149)\n",
      "------------------------------\n",
      "Block 4 completed successfully. Tokenized data created.\n"
     ]
    }
   ],
   "source": [
    "# Block 4: Tokenizer Training and Usage + Execution\n",
    "# (This block remains unchanged as tokenizer logic is independent of model architecture details like attention)\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tqdm.notebook import tqdm\n",
    "# Ensure sentencepiece is imported, handle if missing from Block 1\n",
    "try:\n",
    "    import sentencepiece as spm\n",
    "except ImportError:\n",
    "    spm = None # Already handled in Block 1, but good practice to check\n",
    "\n",
    "# --- Function Definitions ---\n",
    "\n",
    "def train_sentencepiece(data_series, model_prefix, vocab_size, special_tokens_map):\n",
    "    \"\"\"Trains a SentencePiece Unigram model.\"\"\"\n",
    "    if spm is None:\n",
    "        logging.error(\"SentencePiece library not available. Cannot train tokenizer.\")\n",
    "        print(\"\\n--- SentencePiece Training Error ---\")\n",
    "        print(\"SentencePiece library not available. Install it first.\")\n",
    "        print(\"-\" * 30)\n",
    "        return False\n",
    "\n",
    "    logging.info(f\"Starting SentencePiece training. Output prefix: {model_prefix}\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Create a temporary file to store the text data for training\n",
    "    temp_dir = os.path.dirname(model_prefix)\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "    temp_text_file = f\"{model_prefix}_training_data.txt\" # Use prefix for temp file name\n",
    "\n",
    "    try:\n",
    "        if data_series.empty:\n",
    "            logging.error(\"Cannot train SentencePiece on empty data series.\")\n",
    "            print(\"\\n--- SentencePiece Training Error ---\")\n",
    "            print(\"Input data series is empty. Cannot train tokenizer.\")\n",
    "            print(\"-\" * 30)\n",
    "            return False # Indicate failure\n",
    "\n",
    "        # Write data ensuring strings and handling potential NaN\n",
    "        with open(temp_text_file, 'w', encoding='utf-8') as f:\n",
    "            for text in tqdm(data_series, desc=\"Writing Training Data\"):\n",
    "                 if pd.notna(text): # Check for NaN or None\n",
    "                    f.write(str(text) + '\\n') # Ensure text is string\n",
    "        logging.info(f\"Training data written to {temp_text_file}\")\n",
    "\n",
    "        # Build command string using the map for special tokens\n",
    "        spm_command = (\n",
    "            f'--input={temp_text_file} --model_prefix={model_prefix} '\n",
    "            f'--vocab_size={vocab_size} --model_type=unigram '\n",
    "            f'--pad_id={special_tokens_map[\"pad_id\"]} --unk_id={special_tokens_map[\"unk_id\"]} '\n",
    "            f'--bos_id={special_tokens_map[\"bos_id\"]} --eos_id={special_tokens_map[\"eos_id\"]} '\n",
    "            f'--unk_piece={special_tokens_map[\"unk_piece\"]} --bos_piece={special_tokens_map[\"bos_piece\"]} '\n",
    "            f'--eos_piece={special_tokens_map[\"eos_piece\"]} --pad_piece={special_tokens_map[\"pad_piece\"]} '\n",
    "            f'--hard_vocab_limit=false '\n",
    "            f'--character_coverage=1.0 ' # Recommended default\n",
    "            f'--shuffle_input_sentence=true --input_sentence_size=10000000 ' # Process up to 10M lines\n",
    "            f'--seed_sentencepiece_size=1000000 ' # Use 1M sentences for seeding\n",
    "            f'--shrinking_factor=0.75 '\n",
    "            f'--num_threads=16 ' # Use multiple threads if available\n",
    "            f'--num_sub_iterations=2 '\n",
    "            f'--max_sentence_length=4192 ' # Default max length\n",
    "            f'--model_type=unigram ' # Use Unigram model\n",
    "        )\n",
    "\n",
    "        print(\"\\n--- SentencePiece Training ---\")\n",
    "        logging.info(f\"Running SentencePiece with command args...\")\n",
    "        print(f\"Running SentencePiece training command...\") # Don't print full command with paths\n",
    "\n",
    "        spm.SentencePieceTrainer.train(spm_command)\n",
    "\n",
    "        training_duration = time.time() - start_time\n",
    "        logging.info(f\"SentencePiece training completed in {training_duration:.2f} seconds.\")\n",
    "        print(f\"SentencePiece model files created: {model_prefix}.model, {model_prefix}.vocab\")\n",
    "        print(f\"Training duration: {training_duration:.2f} seconds.\")\n",
    "\n",
    "        os.remove(temp_text_file)\n",
    "        logging.info(f\"Removed temporary training file: {temp_text_file}\")\n",
    "        print(\"-\" * 30)\n",
    "        return True # Indicate success\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"SentencePiece training failed: {e}\", exc_info=True)\n",
    "        print(f\"\\n--- SentencePiece Training Error ---\")\n",
    "        print(f\"SentencePiece training failed: {e}\")\n",
    "        # Clean up potentially created files on failure\n",
    "        if os.path.exists(f\"{model_prefix}.model\"): os.remove(f\"{model_prefix}.model\")\n",
    "        if os.path.exists(f\"{model_prefix}.vocab\"): os.remove(f\"{model_prefix}.vocab\")\n",
    "        if os.path.exists(temp_text_file): os.remove(temp_text_file)\n",
    "        print(\"-\" * 30)\n",
    "        return False # Indicate failure\n",
    "\n",
    "\n",
    "def load_tokenizer(model_path):\n",
    "    \"\"\"Loads a trained SentencePiece model.\"\"\"\n",
    "    if spm is None:\n",
    "        logging.error(\"SentencePiece library not available. Cannot load tokenizer.\")\n",
    "        print(\"\\n--- Tokenizer Loading Error ---\")\n",
    "        print(\"SentencePiece library not available.\")\n",
    "        print(\"-\" * 30)\n",
    "        return None\n",
    "\n",
    "    logging.info(f\"Loading SentencePiece tokenizer from: {model_path}\")\n",
    "    if not os.path.exists(model_path):\n",
    "        logging.error(f\"Tokenizer model file not found at {model_path}\")\n",
    "        print(f\"\\n--- Tokenizer Loading Error ---\")\n",
    "        print(f\"Error: Tokenizer model file not found at {model_path}\")\n",
    "        print(\"-\" * 30)\n",
    "        return None\n",
    "    try:\n",
    "        tokenizer = spm.SentencePieceProcessor()\n",
    "        tokenizer.load(model_path)\n",
    "        print(\"\\n--- Tokenizer Loading ---\")\n",
    "        logging.info(f\"Successfully loaded tokenizer: {model_path}\")\n",
    "        print(f\"Successfully loaded tokenizer: {model_path}\")\n",
    "\n",
    "        # Verify special token IDs match configuration (using global constants from Block 1)\n",
    "        pad_id_val = tokenizer.pad_id()\n",
    "        unk_id_val = tokenizer.unk_id()\n",
    "        bos_id_val = tokenizer.bos_id()\n",
    "        eos_id_val = tokenizer.eos_id()\n",
    "\n",
    "        pad_piece_str = tokenizer.id_to_piece(pad_id_val) if pad_id_val != -1 else 'N/A'\n",
    "        unk_piece_str = tokenizer.id_to_piece(unk_id_val) if unk_id_val != -1 else 'N/A'\n",
    "        bos_piece_str = tokenizer.id_to_piece(bos_id_val) if bos_id_val != -1 else 'N/A'\n",
    "        eos_piece_str = tokenizer.id_to_piece(eos_id_val) if eos_id_val != -1 else 'N/A'\n",
    "\n",
    "        print(f\"Vocabulary Size: {tokenizer.vocab_size()}\")\n",
    "        print(f\"PAD ID ({pad_piece_str}): {pad_id_val} (Config: {PAD_ID})\")\n",
    "        print(f\"UNK ID ({unk_piece_str}): {unk_id_val} (Config: {UNK_ID})\")\n",
    "        print(f\"BOS/Start ID ({bos_piece_str}): {bos_id_val} (Config: {START_ID})\")\n",
    "        print(f\"EOS/End ID ({eos_piece_str}): {eos_id_val} (Config: {END_ID})\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "        # Sanity check special token IDs\n",
    "        id_mismatch = False\n",
    "        if pad_id_val != PAD_ID: logging.warning(\"PAD ID mismatch!\"); id_mismatch=True\n",
    "        if unk_id_val != UNK_ID: logging.warning(\"UNK ID mismatch!\"); id_mismatch=True\n",
    "        if bos_id_val != START_ID: logging.warning(\"BOS ID mismatch!\"); id_mismatch=True\n",
    "        if eos_id_val != END_ID: logging.warning(\"EOS ID mismatch!\"); id_mismatch=True\n",
    "        if id_mismatch:\n",
    "             print(\"WARNING: Loaded tokenizer special token IDs DO NOT match configured IDs!\")\n",
    "             # Optional: Treat as error? For now, just warn.\n",
    "             # return None # Uncomment to treat ID mismatch as a fatal error\n",
    "\n",
    "        return tokenizer\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to load or process tokenizer model from {model_path}: {e}\", exc_info=True)\n",
    "        print(f\"\\n--- Tokenizer Loading Error ---\")\n",
    "        print(f\"Failed to load or process tokenizer model from {model_path}: {e}\")\n",
    "        print(\"-\" * 30)\n",
    "        return None\n",
    "\n",
    "\n",
    "def tokenize_texts(texts, tokenizer, max_len):\n",
    "    \"\"\"Tokenizes a list/series of texts and pads/truncates them.\"\"\"\n",
    "    if tokenizer is None:\n",
    "        logging.error(\"Cannot tokenize texts: Tokenizer is None.\")\n",
    "        return np.array([])\n",
    "    if texts is None or texts.empty:\n",
    "        logging.warning(\"Attempted to tokenize an empty list/series of texts.\")\n",
    "        print(\"Warning: Input texts for tokenization is empty.\")\n",
    "        return np.array([]) # Return empty numpy array\n",
    "\n",
    "    logging.info(f\"Tokenizing {len(texts)} texts with max_len={max_len}...\")\n",
    "    try:\n",
    "        # Ensure all items are strings, replace None/NaN with empty string\n",
    "        texts_list = [str(text) if pd.notna(text) else '' for text in texts.tolist()]\n",
    "        # Use encode_as_ids for efficiency\n",
    "        tokenized_sequences = tokenizer.encode_as_ids(texts_list)\n",
    "\n",
    "        padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "            tokenized_sequences,\n",
    "            maxlen=max_len,\n",
    "            padding='post',      # Pad at the end\n",
    "            truncating='post',   # Truncate at the end\n",
    "            value=PAD_ID         # Use the defined PAD_ID\n",
    "        )\n",
    "        print(\"\\n--- Tokenization ---\")\n",
    "        logging.info(f\"Tokenization successful for {len(texts_list)} texts.\")\n",
    "        print(f\"Tokenized {len(texts_list)} texts.\")\n",
    "        print(f\"Shape of padded sequences: {padded_sequences.shape}\")\n",
    "        if len(texts_list) > 0 and len(padded_sequences) > 0:\n",
    "            print(f\"Original Text (sample 0): {texts_list[0][:100]}...\")\n",
    "            print(f\"Tokenized IDs (sample 0): {padded_sequences[0][:20]}...\")\n",
    "        print(\"-\" * 30)\n",
    "        # Ensure output is int32 for TensorFlow compatibility\n",
    "        return padded_sequences.astype(np.int32)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during text tokenization: {e}\", exc_info=True)\n",
    "        print(f\"\\n--- Tokenization Error ---\")\n",
    "        print(f\"An error occurred during tokenization: {e}\")\n",
    "        print(\"-\" * 30)\n",
    "        return np.array([])\n",
    "\n",
    "\n",
    "def detokenize_sequences(sequences, tokenizer):\n",
    "    \"\"\"Converts sequences of token IDs back to text. Handles single sequence or batch.\"\"\"\n",
    "    if tokenizer is None:\n",
    "        logging.error(\"Detokenization failed: Tokenizer is None.\")\n",
    "        return \"[Detokenization Error: No Tokenizer]\"\n",
    "    if sequences is None: return [] if isinstance(sequences, list) else \"\"\n",
    "\n",
    "    try:\n",
    "        # Handle different input types (Tensor, ndarray, list)\n",
    "        if isinstance(sequences, tf.Tensor): sequences = sequences.numpy()\n",
    "        if isinstance(sequences, np.ndarray): sequences = sequences.tolist()\n",
    "\n",
    "        # Check if it's a batch or a single sequence\n",
    "        is_batch = isinstance(sequences, list) and (len(sequences) == 0 or isinstance(sequences[0], list) or isinstance(sequences[0], np.ndarray))\n",
    "        if not is_batch: sequences = [sequences] # Wrap single list/ndarray for uniform processing\n",
    "\n",
    "        texts = []\n",
    "        for seq in sequences:\n",
    "            # Filter out PAD, START, END tokens before decoding\n",
    "            actual_tokens = [int(token_id) for token_id in seq\n",
    "                             if int(token_id) not in [PAD_ID, START_ID, END_ID]]\n",
    "            # Use decode_ids for efficiency\n",
    "            texts.append(tokenizer.decode_ids(actual_tokens))\n",
    "\n",
    "        return texts if is_batch else texts[0]\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during detokenization: {e}\", exc_info=True)\n",
    "        # Try to decode pieces individually in case of error\n",
    "        try:\n",
    "            problem_seq_str = [tokenizer.id_to_piece(int(t)) for t in seq]\n",
    "            logging.error(f\"Problematic sequence pieces: {problem_seq_str}\")\n",
    "        except: pass # Ignore errors during error reporting\n",
    "        return \"[Detokenization Error]\"\n",
    "\n",
    "# --- Execution for Block 4 ---\n",
    "tokenizer = None\n",
    "encoder_input_data = np.array([])\n",
    "decoder_input_data = np.array([])\n",
    "decoder_target_data = np.array([])\n",
    "tokenization_failed = False # Flag to track status\n",
    "\n",
    "# Only proceed if processed_df from Block 3 exists and is not empty\n",
    "if 'processed_df' in locals() and isinstance(processed_df, pd.DataFrame) and not processed_df.empty:\n",
    "    print(\"Block 4: `processed_df` is valid. Proceeding with tokenizer.\")\n",
    "    logging.info(\"Block 4: `processed_df` is valid. Proceeding with tokenizer.\")\n",
    "\n",
    "    # Combine text and summary for tokenizer training data\n",
    "    if 'cleaned_text' in processed_df.columns and 'target_summary' in processed_df.columns:\n",
    "        print(\"Preparing data for tokenizer training...\")\n",
    "        # Ensure consistency by dropping NaN before concatenation\n",
    "        text_data = processed_df['cleaned_text'].dropna()\n",
    "        summary_data = processed_df['target_summary'].dropna()\n",
    "        full_corpus = pd.concat([text_data, summary_data], ignore_index=True)\n",
    "\n",
    "        print(f\"Full corpus size for tokenizer: {len(full_corpus)}\")\n",
    "\n",
    "        # Define special tokens map using constants from Block 1\n",
    "        special_tokens_map = {\n",
    "            \"pad_id\": PAD_ID, \"unk_id\": UNK_ID, \"bos_id\": START_ID, \"eos_id\": END_ID,\n",
    "            \"pad_piece\": \"<pad>\", \"unk_piece\": \"<unk>\", \"bos_piece\": \"<start>\", \"eos_piece\": \"<end>\"\n",
    "        }\n",
    "\n",
    "        # Train only if model file doesn't exist\n",
    "        if not os.path.exists(TOKENIZER_MODEL_FILE):\n",
    "            print(f\"Tokenizer model {TOKENIZER_MODEL_FILE} not found. Starting training...\")\n",
    "            training_successful = train_sentencepiece(full_corpus, TOKENIZER_MODEL_PREFIX, VOCAB_SIZE, special_tokens_map)\n",
    "            if not training_successful:\n",
    "                 print(\"Tokenizer training failed. Cannot proceed with tokenization.\")\n",
    "                 logging.error(\"Tokenizer training failed.\")\n",
    "                 tokenization_failed = True # Set failure flag\n",
    "            else:\n",
    "                 print(\"Tokenizer training successful.\")\n",
    "                 logging.info(\"Tokenizer training successful.\")\n",
    "        else:\n",
    "            print(f\"Tokenizer model {TOKENIZER_MODEL_FILE} already exists. Skipping training.\")\n",
    "            logging.info(f\"Found existing tokenizer model: {TOKENIZER_MODEL_FILE}. Skipping training.\")\n",
    "\n",
    "        # Load the tokenizer (only if training didn't fail)\n",
    "        if not tokenization_failed:\n",
    "            tokenizer = load_tokenizer(TOKENIZER_MODEL_FILE)\n",
    "\n",
    "            if tokenizer:\n",
    "                # --- Tokenization ---\n",
    "                print(\"Tokenizing cleaned text...\")\n",
    "                encoder_input_data = tokenize_texts(processed_df['cleaned_text'], tokenizer, MAX_LEN_INPUT)\n",
    "\n",
    "                print(\"Tokenizing target summaries...\")\n",
    "                # Tokenize summaries (which include <start>/<end>) up to MAX_LEN_SUMMARY\n",
    "                decoder_full_data = tokenize_texts(processed_df['target_summary'], tokenizer, MAX_LEN_SUMMARY)\n",
    "\n",
    "                # --- Create Decoder Input/Target ---\n",
    "                # Check if tokenization produced valid results before slicing\n",
    "                if encoder_input_data.size > 0 and decoder_full_data.size > 0:\n",
    "                    if encoder_input_data.shape[0] != decoder_full_data.shape[0]:\n",
    "                        logging.error(f\"Mismatch in tokenized samples: Encoder {encoder_input_data.shape[0]}, Decoder {decoder_full_data.shape[0]}\")\n",
    "                        print(\"ERROR: Mismatch in number of tokenized encoder/decoder samples. Check preprocessing/tokenization.\")\n",
    "                        tokenization_failed = True\n",
    "                    else:\n",
    "                        print(\"Creating decoder input/target sequences...\")\n",
    "                        # Decoder input: <start> token ... second-to-last token\n",
    "                        # Note: We slice decoder_full_data, max length becomes MAX_LEN_SUMMARY-1\n",
    "                        decoder_input_data = decoder_full_data[:, :-1]\n",
    "\n",
    "                        # Decoder target: first token ... <end> token\n",
    "                        # Note: We slice decoder_full_data, max length becomes MAX_LEN_SUMMARY-1\n",
    "                        decoder_target_data = decoder_full_data[:, 1:]\n",
    "\n",
    "                        print(\"\\n--- Data Shapes After Tokenization & Shifting ---\")\n",
    "                        print(\"Encoder Input Shape:\", encoder_input_data.shape)\n",
    "                        print(\"Decoder Input Shape:\", decoder_input_data.shape)\n",
    "                        print(\"Decoder Target Shape:\", decoder_target_data.shape)\n",
    "                        print(\"-\" * 30)\n",
    "                        logging.info(f\"Tokenization successful. Shapes: Encoder={encoder_input_data.shape}, DecoderIn={decoder_input_data.shape}, DecoderOut={decoder_target_data.shape}\")\n",
    "                else:\n",
    "                    print(\"Tokenization resulted in empty arrays. Cannot proceed.\")\n",
    "                    logging.error(\"Tokenization resulted in empty arrays.\")\n",
    "                    tokenization_failed = True # Set failure flag\n",
    "                    # Ensure arrays are empty\n",
    "                    encoder_input_data = np.array([])\n",
    "                    decoder_input_data = np.array([])\n",
    "                    decoder_target_data = np.array([])\n",
    "            else:\n",
    "                print(\"Failed to load tokenizer. Cannot proceed.\")\n",
    "                logging.error(\"Failed to load tokenizer.\")\n",
    "                tokenization_failed = True # Set failure flag\n",
    "    else:\n",
    "         print(\"Skipping tokenizer step: required columns ('cleaned_text', 'target_summary') missing in processed_df.\")\n",
    "         logging.error(\"Skipping tokenizer step: required columns missing.\")\n",
    "         tokenization_failed = True # Set failure flag\n",
    "else:\n",
    "     # This case should have been caught earlier if Blocks 2/3 failed\n",
    "     print(\"Skipping Block 4 execution because `processed_df` is not available or is empty.\")\n",
    "     print(\"Check the output of Block 3.\")\n",
    "     logging.error(\"Skipping Block 4 execution because `processed_df` is not available or is empty.\")\n",
    "     tokenization_failed = True # Set failure flag\n",
    "\n",
    "\n",
    "# Final status check for Block 4\n",
    "if tokenization_failed:\n",
    "    print(\"************************************************************\")\n",
    "    print(\"ERROR: Block 4 failed or was skipped due to issues in previous blocks or during tokenization.\")\n",
    "    print(\"Cannot proceed to Block 5 (Dataset Creation).\")\n",
    "    print(\"************************************************************\")\n",
    "else:\n",
    "    print(\"Block 4 completed successfully. Tokenized data created.\")\n",
    "    logging.info(\"Block 4 completed successfully. Tokenized data created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cb9237-ac1f-4add-beb6-cf585a8e7af3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Block 5: Data Preparation for TensorFlow (tf.data.Dataset) + Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ada3b48a-8b9f-4852-ab4a-291104a115aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 18:30:42,246 - INFO - Block 5: Tokenized data is valid. Proceeding with dataset creation.\n",
      "2025-04-24 18:30:42,247 - INFO - Splitting data: 33579 train, 3731 validation.\n",
      "2025-04-24 18:30:42,327 - INFO - Creating tf.data.Dataset. Shuffle=True, Batch Size=32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block 5: Tokenized data is valid. Proceeding with dataset creation.\n",
      "Splitting data: 33579 train, 3731 validation.\n",
      "Shuffled data indices before splitting.\n",
      "\n",
      "Creating training dataset...\n",
      "\n",
      "--- tf.data.Dataset Creation ---\n",
      "Input shapes: Encoder=(33579, 1024), DecoderIn=(33579, 149), DecoderOut=(33579, 149)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 18:30:42.407806: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-24 18:30:42.543715: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20750 MB memory:  -> device: 0, name: NVIDIA L4, pci bus id: 0000:00:03.0, compute capability: 8.9\n",
      "2025-04-24 18:30:42,866 - INFO - Shuffling dataset with buffer size 33579\n",
      "2025-04-24 18:30:42,873 - INFO - tf.data.Dataset created successfully.\n",
      "2025-04-24 18:30:42,873 - INFO - Creating tf.data.Dataset. Shuffle=False, Batch Size=32\n",
      "2025-04-24 18:30:42,887 - INFO - tf.data.Dataset created successfully.\n",
      "2025-04-24 18:30:42,890 - INFO - Block 5 completed successfully. Train/Validation datasets created.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.data.Dataset created (shuffle=True).\n",
      "Element Spec (structure of one batch):\n",
      "({'encoder_inputs': TensorSpec(shape=(32, 1024), dtype=tf.int32, name=None), 'decoder_inputs': TensorSpec(shape=(32, 149), dtype=tf.int32, name=None)}, {'output_layer': TensorSpec(shape=(32, 149), dtype=tf.int32, name=None)})\n",
      "------------------------------\n",
      "\n",
      "Creating validation dataset...\n",
      "\n",
      "--- tf.data.Dataset Creation ---\n",
      "Input shapes: Encoder=(3731, 1024), DecoderIn=(3731, 149), DecoderOut=(3731, 149)\n",
      "tf.data.Dataset created (shuffle=False).\n",
      "Element Spec (structure of one batch):\n",
      "({'encoder_inputs': TensorSpec(shape=(32, 1024), dtype=tf.int32, name=None), 'decoder_inputs': TensorSpec(shape=(32, 149), dtype=tf.int32, name=None)}, {'output_layer': TensorSpec(shape=(32, 149), dtype=tf.int32, name=None)})\n",
      "------------------------------\n",
      "\n",
      "--- Dataset Splitting and Creation Summary ---\n",
      "Total samples tokenized: 37310\n",
      "Training samples: 33579, Validation samples: 3731\n",
      "Train dataset created: Yes (Steps per epoch: 1049)\n",
      "Validation dataset created: Yes (Steps per epoch: 116)\n",
      "------------------------------\n",
      "Block 5 completed successfully. Train/Validation datasets created.\n"
     ]
    }
   ],
   "source": [
    "# Block 5: Data Preparation for TensorFlow (tf.data.Dataset) + Execution\n",
    "# (This block also remains unchanged as it prepares data for consumption, compatible with both model.fit and custom loops)\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "# --- Function Definition ---\n",
    "def create_tf_dataset(encoder_inputs, decoder_inputs, decoder_targets, batch_size, shuffle=True):\n",
    "    \"\"\"Creates a tf.data.Dataset for training or validation.\"\"\"\n",
    "    if not isinstance(encoder_inputs, np.ndarray) or \\\n",
    "       not isinstance(decoder_inputs, np.ndarray) or \\\n",
    "       not isinstance(decoder_targets, np.ndarray):\n",
    "        logging.error(\"Inputs to create_tf_dataset must be numpy arrays.\")\n",
    "        print(\"Error: Inputs for dataset creation are not numpy arrays.\")\n",
    "        return None\n",
    "\n",
    "    if encoder_inputs.size == 0 or decoder_inputs.size == 0 or decoder_targets.size == 0:\n",
    "        logging.error(\"Cannot create dataset from empty numpy arrays.\")\n",
    "        print(\"Error: Input arrays for dataset creation are empty.\")\n",
    "        return None\n",
    "    if not (encoder_inputs.shape[0] == decoder_inputs.shape[0] == decoder_targets.shape[0]):\n",
    "        logging.error(f\"Mismatch in number of samples: Enc={encoder_inputs.shape[0]}, DecIn={decoder_inputs.shape[0]}, DecOut={decoder_targets.shape[0]}\")\n",
    "        print(\"Error: Mismatch in number of samples between input/output arrays.\")\n",
    "        return None\n",
    "\n",
    "    logging.info(f\"Creating tf.data.Dataset. Shuffle={shuffle}, Batch Size={batch_size}\")\n",
    "    print(\"\\n--- tf.data.Dataset Creation ---\")\n",
    "    print(f\"Input shapes: Encoder={encoder_inputs.shape}, DecoderIn={decoder_inputs.shape}, DecoderOut={decoder_targets.shape}\")\n",
    "\n",
    "    try:\n",
    "        # Create slices for inputs (dictionary) and targets (dictionary)\n",
    "        # Ensure keys match the input/output names expected by the model later\n",
    "        # Keras Functional API uses layer names by default.\n",
    "        # If using subclassing, define input/output names in the call method.\n",
    "        # For custom loop, we'll unpack this dictionary later.\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(\n",
    "            (\n",
    "                {\"encoder_inputs\": encoder_inputs.astype(np.int32), # Ensure int32\n",
    "                 \"decoder_inputs\": decoder_inputs.astype(np.int32)}, # Ensure int32\n",
    "                {\"output_layer\": decoder_targets.astype(np.int32)}  # Target key matches final Dense layer name\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if shuffle:\n",
    "            # Use a buffer size approx the size of the dataset for good shuffling\n",
    "            buffer_size = len(encoder_inputs)\n",
    "            dataset = dataset.shuffle(buffer_size=buffer_size, reshuffle_each_iteration=True)\n",
    "            logging.info(f\"Shuffling dataset with buffer size {buffer_size}\")\n",
    "\n",
    "        # Batch the dataset. drop_remainder=True is often good for stateful operations or consistent processing.\n",
    "        # For custom loops, you might handle the last partial batch manually if needed.\n",
    "        dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "        # Prefetch for performance\n",
    "        dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "        logging.info(\"tf.data.Dataset created successfully.\")\n",
    "        print(f\"tf.data.Dataset created (shuffle={shuffle}).\")\n",
    "        print(\"Element Spec (structure of one batch):\")\n",
    "        print(dataset.element_spec)\n",
    "        print(\"-\" * 30)\n",
    "        return dataset\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to create tf.data.Dataset: {e}\", exc_info=True)\n",
    "        print(f\"\\n--- Dataset Creation Error ---\")\n",
    "        print(f\"Failed to create tf.data.Dataset: {e}\")\n",
    "        print(\"-\" * 30)\n",
    "        return None\n",
    "\n",
    "# --- Execution for Block 5 ---\n",
    "train_dataset = None\n",
    "val_dataset = None\n",
    "num_train_samples = 0\n",
    "num_val_samples = 0\n",
    "dataset_creation_failed = False # Flag\n",
    "\n",
    "# Check if Block 4 succeeded and data exists\n",
    "if ('tokenization_failed' in locals() and not tokenization_failed and\n",
    "   'encoder_input_data' in locals() and encoder_input_data.size > 0 and\n",
    "   'decoder_input_data' in locals() and decoder_input_data.size > 0 and\n",
    "   'decoder_target_data' in locals() and decoder_target_data.size > 0):\n",
    "\n",
    "    print(\"Block 5: Tokenized data is valid. Proceeding with dataset creation.\")\n",
    "    logging.info(\"Block 5: Tokenized data is valid. Proceeding with dataset creation.\")\n",
    "\n",
    "    num_samples = encoder_input_data.shape[0]\n",
    "    if num_samples > 0:\n",
    "        # Simple 90/10 split, ensure validation set isn't empty if possible\n",
    "        num_val_samples = max(1, int(0.1 * num_samples)) if num_samples > 1 else 0\n",
    "        num_train_samples = num_samples - num_val_samples\n",
    "\n",
    "        if num_train_samples <= 0:\n",
    "             print(f\"Error: Not enough samples ({num_samples}) for a train/validation split (Val samples = {num_val_samples}).\")\n",
    "             logging.error(f\"Not enough samples ({num_samples}) for train/val split.\")\n",
    "             dataset_creation_failed = True\n",
    "        else:\n",
    "            print(f\"Splitting data: {num_train_samples} train, {num_val_samples} validation.\")\n",
    "            logging.info(f\"Splitting data: {num_train_samples} train, {num_val_samples} validation.\")\n",
    "\n",
    "            # Shuffle indices *before* splitting\n",
    "            indices = np.arange(num_samples)\n",
    "            np.random.seed(42) # for reproducibility\n",
    "            np.random.shuffle(indices)\n",
    "            encoder_input_data = encoder_input_data[indices]\n",
    "            decoder_input_data = decoder_input_data[indices]\n",
    "            decoder_target_data = decoder_target_data[indices]\n",
    "            print(\"Shuffled data indices before splitting.\")\n",
    "\n",
    "            # Perform the split\n",
    "            encoder_input_train = encoder_input_data[:num_train_samples]\n",
    "            decoder_input_train = decoder_input_data[:num_train_samples]\n",
    "            decoder_target_train = decoder_target_data[:num_train_samples]\n",
    "\n",
    "            encoder_input_val = encoder_input_data[num_train_samples:]\n",
    "            decoder_input_val = decoder_input_data[num_train_samples:]\n",
    "            decoder_target_val = decoder_target_data[num_train_samples:]\n",
    "\n",
    "            # Create datasets\n",
    "            print(\"\\nCreating training dataset...\")\n",
    "            train_dataset = create_tf_dataset(\n",
    "                encoder_input_train, decoder_input_train, decoder_target_train, BATCH_SIZE, shuffle=True\n",
    "            )\n",
    "            print(\"\\nCreating validation dataset...\")\n",
    "            val_dataset = create_tf_dataset(\n",
    "                encoder_input_val, decoder_input_val, decoder_target_val, BATCH_SIZE, shuffle=False # No need to shuffle validation\n",
    "            )\n",
    "\n",
    "            if train_dataset is None or val_dataset is None:\n",
    "                 print(\"Error: Failed to create train or validation dataset.\")\n",
    "                 logging.error(\"Failed to create train or validation dataset.\")\n",
    "                 dataset_creation_failed = True\n",
    "            else:\n",
    "                # Calculate steps per epoch (useful for custom loop)\n",
    "                train_steps_per_epoch = len(train_dataset)\n",
    "                val_steps_per_epoch = len(val_dataset)\n",
    "                print(\"\\n--- Dataset Splitting and Creation Summary ---\")\n",
    "                print(f\"Total samples tokenized: {num_samples}\")\n",
    "                print(f\"Training samples: {num_train_samples}, Validation samples: {num_val_samples}\")\n",
    "                print(f\"Train dataset created: Yes (Steps per epoch: {train_steps_per_epoch})\")\n",
    "                print(f\"Validation dataset created: Yes (Steps per epoch: {val_steps_per_epoch})\")\n",
    "                print(\"-\" * 30)\n",
    "    else:\n",
    "        print(\"No samples found in tokenized data. Cannot create datasets.\")\n",
    "        logging.error(\"No samples found in tokenized data. Cannot create datasets.\")\n",
    "        dataset_creation_failed = True\n",
    "else:\n",
    "    print(\"Skipping Block 5 execution: Tokenization failed or resulted in empty data.\")\n",
    "    logging.error(\"Skipping Block 5 execution: Tokenization failed or resulted in empty data.\")\n",
    "    dataset_creation_failed = True\n",
    "\n",
    "\n",
    "# Final status check for Block 5\n",
    "if dataset_creation_failed:\n",
    "    print(\"************************************************************\")\n",
    "    print(\"ERROR: Block 5 failed or was skipped.\")\n",
    "    print(\"Cannot proceed to Block 6 (Model Building).\")\n",
    "    print(\"Review the output from Block 4 and 5.\")\n",
    "    print(\"************************************************************\")\n",
    "else:\n",
    "    print(\"Block 5 completed successfully. Train/Validation datasets created.\")\n",
    "    logging.info(\"Block 5 completed successfully. Train/Validation datasets created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9caadc8a-b215-4c62-9bcd-30a33dbc0da4",
   "metadata": {},
   "source": [
    "## Block 6: Model Architecture (Seq2Seq with Attention) + Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a75a248-1bdc-4a3e-9013-14f64ba54e52",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 18:30:42,910 - INFO - Building Seq2Seq Model with Attention...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceeding to build the Attention model...\n",
      "\n",
      "--- Seq2Seq Model Build (with Attention) ---\n",
      "Building Encoder with 1 BiLSTM layers...\n",
      "Building Decoder with 1 LSTM layers and Attention...\n",
      "\n",
      "--- Combined Training Model (with Attention) Summary ---\n",
      "Model: \"model\"\n",
      "________________________________________________________________________________________________________________________\n",
      " Layer (type)                          Output Shape               Param #       Connected to                            \n",
      "========================================================================================================================\n",
      " encoder_inputs (InputLayer)           [(None, 1024)]             0             []                                      \n",
      "                                                                                                                        \n",
      " encoder_embedding (Embedding)         (None, 1024, 100)          3000000       ['encoder_inputs[0][0]']                \n",
      "                                                                                                                        \n",
      " decoder_inputs (InputLayer)           [(None, 149)]              0             []                                      \n",
      "                                                                                                                        \n",
      " dropout (Dropout)                     (None, 1024, 100)          0             ['encoder_embedding[0][0]']             \n",
      "                                                                                                                        \n",
      " decoder_embedding (Embedding)         (None, 149, 100)           3000000       ['decoder_inputs[0][0]']                \n",
      "                                                                                                                        \n",
      " bidirectional (Bidirectional)         [(None, 1024, 256),        234496        ['dropout[0][0]']                       \n",
      "                                        (None, 128),                                                                    \n",
      "                                        (None, 128),                                                                    \n",
      "                                        (None, 128),                                                                    \n",
      "                                        (None, 128)]                                                                    \n",
      "                                                                                                                        \n",
      " dropout_1 (Dropout)                   (None, 149, 100)           0             ['decoder_embedding[0][0]']             \n",
      "                                                                                                                        \n",
      " encoder_final_h (Concatenate)         (None, 256)                0             ['bidirectional[0][1]',                 \n",
      "                                                                                 'bidirectional[0][3]']                 \n",
      "                                                                                                                        \n",
      " encoder_final_c (Concatenate)         (None, 256)                0             ['bidirectional[0][2]',                 \n",
      "                                                                                 'bidirectional[0][4]']                 \n",
      "                                                                                                                        \n",
      " decoder_lstm_1 (LSTM)                 [(None, 149, 256),         365568        ['dropout_1[0][0]',                     \n",
      "                                        (None, 256),                             'encoder_final_h[0][0]',               \n",
      "                                        (None, 256)]                             'encoder_final_c[0][0]']               \n",
      "                                                                                                                        \n",
      " attention_layer (AdditiveAttention)   ((None, 149, 256),         256           ['decoder_lstm_1[0][0]',                \n",
      "                                        (None, 149, 1024))                       'bidirectional[0][0]']                 \n",
      "                                                                                                                        \n",
      " decoder_attention_concat (Concatenate  (None, 149, 512)          0             ['decoder_lstm_1[0][0]',                \n",
      " )                                                                               'attention_layer[0][0]']               \n",
      "                                                                                                                        \n",
      " dropout_2 (Dropout)                   (None, 149, 512)           0             ['decoder_attention_concat[0][0]']      \n",
      "                                                                                                                        \n",
      " output_layer (Dense)                  (None, 149, 30000)         15390000      ['dropout_2[0][0]']                     \n",
      "                                                                                                                        \n",
      "========================================================================================================================\n",
      "Total params: 21,990,320\n",
      "Trainable params: 21,990,320\n",
      "Non-trainable params: 0\n",
      "________________________________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 18:30:45,534 - INFO - Model with Attention built successfully.\n",
      "2025-04-24 18:30:45,537 - INFO - Block 6 completed successfully. Model with Attention created.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Inputs: ['encoder_inputs: (None, 1024)', 'decoder_inputs: (None, 149)']\n",
      "Model Outputs: ['output_layer/Softmax:0: (None, 149, 30000)']\n",
      "Output layer name: output_layer (should match dataset target key: 'output_layer')\n",
      "------------------------------\n",
      "Block 6 completed successfully. Model with Attention created.\n"
     ]
    }
   ],
   "source": [
    "# Block 6: Model Architecture (Seq2Seq with Attention) + Execution\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (Input, Embedding, LSTM, Bidirectional, Dense,\n",
    "                                     Concatenate, AdditiveAttention, Dropout)\n",
    "from tensorflow.keras.models import Model\n",
    "import logging\n",
    "\n",
    "# --- Function Definition ---\n",
    "\n",
    "def build_seq2seq_attention_model(vocab_size, embedding_dim, lstm_units, decoder_lstm_units,\n",
    "                                  attention_units, num_encoder_layers, num_decoder_layers,\n",
    "                                  dropout_rate, max_len_input, max_len_summary_dec_input):\n",
    "    \"\"\"Builds the Encoder-Decoder model with Additive (Bahdanau) Attention.\"\"\"\n",
    "    logging.info(\"Building Seq2Seq Model with Attention...\")\n",
    "    print(\"\\n--- Seq2Seq Model Build (with Attention) ---\")\n",
    "\n",
    "    # --- Encoder ---\n",
    "    encoder_input_layer = Input(shape=(max_len_input,), dtype='int32', name=\"encoder_inputs\")\n",
    "\n",
    "    # Embedding layer with masking\n",
    "    encoder_embedding_layer = Embedding(vocab_size, embedding_dim, mask_zero=True, name=\"encoder_embedding\")\n",
    "    encoder_embeddings = encoder_embedding_layer(encoder_input_layer)\n",
    "    encoder_embeddings = Dropout(dropout_rate)(encoder_embeddings) # Dropout after embedding\n",
    "\n",
    "    # Stacked Bidirectional LSTMs\n",
    "    encoder_outputs = encoder_embeddings\n",
    "    encoder_states_list = [] # To store final states [h, c] from each layer\n",
    "\n",
    "    print(f\"Building Encoder with {num_encoder_layers} BiLSTM layers...\")\n",
    "    for i in range(num_encoder_layers):\n",
    "        # return_sequences=True is crucial for all layers to feed to the next, and for attention from the last layer.\n",
    "        # return_state=True is needed to get the final states for decoder initialization.\n",
    "        bilstm_layer = Bidirectional(\n",
    "            LSTM(lstm_units, return_sequences=True, return_state=True, dropout=dropout_rate, name=f\"encoder_bilstm_{i+1}\")\n",
    "        )\n",
    "        encoder_outputs, forward_h, forward_c, backward_h, backward_c = bilstm_layer(encoder_outputs)\n",
    "\n",
    "        # Store the states from the *last* layer for decoder initialization\n",
    "        if i == num_encoder_layers - 1:\n",
    "            state_h = Concatenate(name=\"encoder_final_h\")([forward_h, backward_h])\n",
    "            state_c = Concatenate(name=\"encoder_final_c\")([forward_c, backward_c])\n",
    "            # Final encoder states should match the decoder LSTM units if used directly\n",
    "            # Ensure DECODER_LSTM_UNITS = LSTM_UNITS * 2 if using this state init scheme\n",
    "            encoder_states_list = [state_h, state_c]\n",
    "\n",
    "    # encoder_outputs shape: (batch_size, max_len_input, 2 * lstm_units) - This is the 'value' for attention\n",
    "    # encoder_states_list: [ (batch_size, 2 * lstm_units), (batch_size, 2 * lstm_units) ] - Final h and c\n",
    "\n",
    "    # --- Decoder ---\n",
    "    decoder_input_layer = Input(shape=(max_len_summary_dec_input,), dtype='int32', name=\"decoder_inputs\") # max_len_summary - 1\n",
    "\n",
    "    # Decoder Embedding layer (separate instance recommended)\n",
    "    decoder_embedding_layer = Embedding(vocab_size, embedding_dim, mask_zero=True, name=\"decoder_embedding\")\n",
    "    decoder_embeddings = decoder_embedding_layer(decoder_input_layer)\n",
    "    decoder_embeddings = Dropout(dropout_rate)(decoder_embeddings) # Dropout after embedding\n",
    "\n",
    "    # Attention Layer (Additive/Bahdanau)\n",
    "    # The query will be the decoder hidden state, value will be the encoder outputs\n",
    "    attention_layer = AdditiveAttention(name=\"attention_layer\")\n",
    "\n",
    "    # Stacked Decoder LSTMs\n",
    "    decoder_lstm_outputs = decoder_embeddings\n",
    "    # Initial state for the first decoder LSTM comes from the final encoder state\n",
    "    current_decoder_states = encoder_states_list\n",
    "\n",
    "    print(f\"Building Decoder with {num_decoder_layers} LSTM layers and Attention...\")\n",
    "    for i in range(num_decoder_layers):\n",
    "        decoder_lstm_layer = LSTM(decoder_lstm_units, return_sequences=True, return_state=True, dropout=dropout_rate, name=f\"decoder_lstm_{i+1}\")\n",
    "\n",
    "        # Pass initial state only to the first layer\n",
    "        if i == 0:\n",
    "             # We need the full sequence and the state from the LSTM\n",
    "            decoder_lstm_outputs, state_h, state_c = decoder_lstm_layer(decoder_lstm_outputs, initial_state=current_decoder_states)\n",
    "        else:\n",
    "             # Subsequent layers receive the output sequence from the previous layer\n",
    "             # Keras handles state propagation internally *if initial_state isn't provided*\n",
    "             # However, for attention, we often need the state from the *last* decoder layer.\n",
    "             # Let's explicitly manage states for clarity, though might be redundant for training model only.\n",
    "             # Use the states from the previous decoder layer as initial_state for the current one.\n",
    "             decoder_lstm_outputs, state_h, state_c = decoder_lstm_layer(decoder_lstm_outputs, initial_state=current_decoder_states)\n",
    "\n",
    "        # Update the current states for the next iteration or for attention query\n",
    "        current_decoder_states = [state_h, state_c]\n",
    "\n",
    "    # Now, calculate attention using the final decoder LSTM output sequence and encoder outputs\n",
    "    # The query for attention is typically the decoder output sequence (or just the state)\n",
    "    # The value (and key) is the full encoder output sequence\n",
    "    # Attention query shape: (batch_size, max_len_summary_dec_input, decoder_lstm_units)\n",
    "    # Attention value shape: (batch_size, max_len_input, 2 * lstm_units)\n",
    "    context_vector, attention_weights = attention_layer(\n",
    "        [decoder_lstm_outputs, encoder_outputs], # Pass inputs as a list\n",
    "        return_attention_scores=True # Keyword args for other options are fine\n",
    "    )\n",
    "    # context_vector shape: (batch_size, max_len_summary_dec_input, 2 * lstm_units)\n",
    "\n",
    "    # Concatenate the context vector and the decoder LSTM output\n",
    "    # This combined information is fed to the final prediction layer\n",
    "    decoder_combined_context = Concatenate(axis=-1, name=\"decoder_attention_concat\")(\n",
    "        [decoder_lstm_outputs, context_vector]\n",
    "    )\n",
    "    decoder_combined_context = Dropout(dropout_rate)(decoder_combined_context) # Dropout before final Dense\n",
    "\n",
    "    # --- Final Output Layer ---\n",
    "    # Predicts the next token probability distribution\n",
    "    # Name matches the key in the target part of the tf.data.Dataset\n",
    "    output_dense_layer = Dense(vocab_size, activation='softmax', name=\"output_layer\")\n",
    "    decoder_pred_outputs = output_dense_layer(decoder_combined_context)\n",
    "\n",
    "    # Define the full model for training\n",
    "    model = Model(inputs=[encoder_input_layer, decoder_input_layer], outputs=decoder_pred_outputs)\n",
    "\n",
    "    print(\"\\n--- Combined Training Model (with Attention) Summary ---\")\n",
    "    model.summary(line_length=120)\n",
    "    print(f\"Model Inputs: {[inp.name + ': ' + str(inp.shape) for inp in model.inputs]}\")\n",
    "    print(f\"Model Outputs: {[out.name + ': ' + str(out.shape) for out in model.outputs]}\")\n",
    "    print(f\"Output layer name: {model.layers[-1].name} (should match dataset target key: 'output_layer')\")\n",
    "    print(\"-\" * 30)\n",
    "    logging.info(\"Model with Attention built successfully.\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# --- Execution for Block 6 ---\n",
    "model = None # Initialize model variable\n",
    "\n",
    "# Check if datasets are ready from Block 5\n",
    "if ('dataset_creation_failed' not in locals() or not dataset_creation_failed):\n",
    "    print(\"Proceeding to build the Attention model...\")\n",
    "    # Build the model using constants defined in Block 1\n",
    "    # Note: Decoder input length is MAX_LEN_SUMMARY - 1 because of the shifting\n",
    "    model = build_seq2seq_attention_model(\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        embedding_dim=EMBEDDING_DIM,\n",
    "        lstm_units=LSTM_UNITS, # Encoder units per direction\n",
    "        decoder_lstm_units=DECODER_LSTM_UNITS, # Decoder units\n",
    "        attention_units=ATTENTION_UNITS, # Attention layer units\n",
    "        num_encoder_layers=NUM_ENCODER_LAYERS,\n",
    "        num_decoder_layers=NUM_DECODER_LAYERS,\n",
    "        dropout_rate=DROPOUT_RATE,\n",
    "        max_len_input=MAX_LEN_INPUT,\n",
    "        max_len_summary_dec_input=MAX_LEN_SUMMARY - 1 # Decoder input length\n",
    "    )\n",
    "\n",
    "    if model is None:\n",
    "        print(\"************************************************************\")\n",
    "        print(\"ERROR: Model building failed. Check the logs and model definition.\")\n",
    "        print(\"Cannot proceed to Block 7 (Optimizer/Loss Setup).\")\n",
    "        print(\"************************************************************\")\n",
    "        logging.error(\"Model building failed.\")\n",
    "    else:\n",
    "        print(\"Block 6 completed successfully. Model with Attention created.\")\n",
    "        logging.info(\"Block 6 completed successfully. Model with Attention created.\")\n",
    "else:\n",
    "     print(\"Skipping Block 6 execution because dataset creation failed in Block 5.\")\n",
    "     logging.error(\"Skipping Block 6 execution due to dataset creation failure.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00eb7dd7-2b6b-45dc-a4c9-f9147aa9ca10",
   "metadata": {},
   "source": [
    "## Block 7: Optimizer and Loss Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d136b81d-451f-459e-ac26-3705360a1ecb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 18:30:45,551 - INFO - Setting up optimizer and loss function...\n",
      "2025-04-24 18:30:45,556 - INFO - Optimizer: Adam (LR=0.001)\n",
      "2025-04-24 18:30:45,557 - INFO - Loss Function: sparse_categorical_crossentropy (reduction=none)\n",
      "2025-04-24 18:30:45,558 - INFO - Block 7 completed successfully. Optimizer and loss object created.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model found. Setting up optimizer and loss...\n",
      "\n",
      "--- Optimizer and Loss Setup ---\n",
      "Optimizer: Adam (LR=0.001)\n",
      "Loss Function: sparse_categorical_crossentropy\n",
      "Metrics will be defined and updated within the custom training loop.\n",
      "------------------------------\n",
      "Block 7 completed successfully. Optimizer and loss object created.\n"
     ]
    }
   ],
   "source": [
    "# Block 7: Optimizer and Loss Setup + Execution\n",
    "# (Modified for Custom Training Loop)\n",
    "\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "\n",
    "# --- Function Definitions (Simplified for Custom Loop) ---\n",
    "\n",
    "def setup_optimizer_and_loss(learning_rate):\n",
    "    \"\"\"Sets up the optimizer and loss function for the custom training loop.\"\"\"\n",
    "    logging.info(f\"Setting up optimizer and loss function...\")\n",
    "    print(\"\\n--- Optimizer and Loss Setup ---\")\n",
    "\n",
    "    # Optimizer - Adam is a good default\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, name='Adam')\n",
    "    print(f\"Optimizer: {optimizer.name} (LR={learning_rate})\")\n",
    "    logging.info(f\"Optimizer: {optimizer.name} (LR={learning_rate})\")\n",
    "\n",
    "    # Loss Function - SparseCategoricalCrossentropy because target tokens are integers\n",
    "    # reduction=NONE allows calculating loss per-token, then masking, then averaging.\n",
    "    # Using from_logits=False because the model's last layer has a softmax activation.\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=False, reduction='none'\n",
    "    )\n",
    "    print(f\"Loss Function: {loss_object.name}\")\n",
    "    logging.info(f\"Loss Function: {loss_object.name} (reduction=none)\")\n",
    "\n",
    "    # Metrics - We'll track these manually in the training loop\n",
    "    # Example: train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "    # Example: train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "    print(\"Metrics will be defined and updated within the custom training loop.\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    return optimizer, loss_object\n",
    "\n",
    "# --- Execution for Block 7 ---\n",
    "optimizer = None\n",
    "loss_object = None\n",
    "\n",
    "# Check if model exists from Block 6\n",
    "if 'model' in locals() and model is not None:\n",
    "    print(\"Model found. Setting up optimizer and loss...\")\n",
    "    optimizer, loss_object = setup_optimizer_and_loss(LEARNING_RATE)\n",
    "\n",
    "    if optimizer is None or loss_object is None:\n",
    "         print(\"************************************************************\")\n",
    "         print(\"ERROR: Failed to set up optimizer or loss object.\")\n",
    "         print(\"Cannot proceed to Block 8 (Custom Training Loop).\")\n",
    "         print(\"************************************************************\")\n",
    "         logging.error(\"Optimizer or loss object setup failed.\")\n",
    "    else:\n",
    "         print(\"Block 7 completed successfully. Optimizer and loss object created.\")\n",
    "         logging.info(\"Block 7 completed successfully. Optimizer and loss object created.\")\n",
    "\n",
    "else:\n",
    "     print(\"Skipping Block 7 execution because the model was not built successfully in Block 6.\")\n",
    "     logging.error(\"Skipping Block 7 execution due to model building failure.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a59b75a-1534-4a95-94cf-c449684b6d03",
   "metadata": {},
   "source": [
    "## Block 8: Custom Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94181dac-7b82-4cd5-aeac-a4e77264702c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 18:30:45,612 - INFO - Starting custom model training loop (Teacher Forcing)...\n",
      "2025-04-24 18:30:45,621 - INFO - Epoch 1 Starting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint directory: model_attention_files/training_checkpoints\n",
      "Best model save path: model_attention_files/pib_summarizer_attention_best.keras\n",
      "TensorBoard Log Directory (for custom loop): model_attention_files/logs/custom_train/20250424-183033\n",
      "\n",
      "--- Custom Model Training (Teacher Forcing) ---\n",
      "NOTE: Scheduled Sampling implementation deferred due to complexity with Functional API.\n",
      "Train steps per epoch: 1049\n",
      "Validation steps per epoch: 116\n",
      "\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 18:30:52.394065: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"NVIDIA L4\" frequency: 2040 num_cores: 58 environment { key: \"architecture\" value: \"8.9\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 50331648 shared_memory_size_per_multiprocessor: 102400 memory_size: 21758083072 bandwidth: 300048000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "2025-04-24 18:30:53.062693: W tensorflow/core/common_runtime/type_inference.cc:339] Type inference failed. This indicates an invalid graph that escaped type checking. Error message: INVALID_ARGUMENT: expected compatible input types, but input 1:\n",
      "type_id: TFT_OPTIONAL\n",
      "args {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_TENSOR\n",
      "    args {\n",
      "      type_id: TFT_INT32\n",
      "    }\n",
      "  }\n",
      "}\n",
      " is neither a subtype nor a supertype of the combined inputs preceding it:\n",
      "type_id: TFT_OPTIONAL\n",
      "args {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_TENSOR\n",
      "    args {\n",
      "      type_id: TFT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\twhile inferring type of node 'cond_20/output/_23'\n",
      "2025-04-24 18:30:53.270463: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8900\n",
      "2025-04-24 18:30:53.749804: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2025-04-24 18:30:53.812892: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x7f12c80e7670 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-04-24 18:30:53.812920: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA L4, Compute Capability 8.9\n",
      "2025-04-24 18:30:53.818477: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-04-24 18:30:53.956138: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1049/1049 [==============================] - 658s 619ms/step - loss: 5.9816 - accuracy: 0.1815\n",
      "\n",
      "Validation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 18:41:45.635978: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"NVIDIA L4\" frequency: 2040 num_cores: 58 environment { key: \"architecture\" value: \"8.9\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 50331648 shared_memory_size_per_multiprocessor: 102400 memory_size: 21758083072 bandwidth: 300048000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116/116 [==============================] - 27s 209ms/step - val_loss: 5.1210 - val_accuracy: 0.2581\n",
      "\n",
      "Validation loss improved from inf to 5.1210. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 18:42:10,832 - INFO - Saving checkpoint for epoch 1 at model_attention_files/training_checkpoints/ckpt-1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 18:42:10,833 - WARNING - Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "2025-04-24 18:42:10,974 - INFO - Best model saved to model_attention_files/pib_summarizer_attention_best.keras\n",
      "2025-04-24 18:42:10,979 - INFO - Epoch 2 Starting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full model saved to model_attention_files/pib_summarizer_attention_best.keras\n",
      "Time taken for epoch 1: 685.36 sec\n",
      "\n",
      "Epoch 2/30\n",
      "1049/1049 [==============================] - 580s 552ms/step - loss: 4.8011 - accuracy: 0.2769\n",
      "\n",
      "Validation:\n",
      "116/116 [==============================] - 24s 208ms/step - val_loss: 4.4453 - val_accuracy: 0.3067\n",
      "\n",
      "Validation loss improved from 5.1210 to 4.4453. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 18:52:15,096 - INFO - Saving checkpoint for epoch 2 at model_attention_files/training_checkpoints/ckpt-2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 18:52:15,097 - WARNING - Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "2025-04-24 18:52:15,486 - INFO - Best model saved to model_attention_files/pib_summarizer_attention_best.keras\n",
      "2025-04-24 18:52:15,491 - INFO - Epoch 3 Starting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full model saved to model_attention_files/pib_summarizer_attention_best.keras\n",
      "Time taken for epoch 2: 604.51 sec\n",
      "\n",
      "Epoch 3/30\n",
      "1049/1049 [==============================] - 561s 535ms/step - loss: 4.3021 - accuracy: 0.3104\n",
      "\n",
      "Validation:\n",
      "116/116 [==============================] - 24s 208ms/step - val_loss: 4.0901 - val_accuracy: 0.3342\n",
      "\n",
      "Validation loss improved from 4.4453 to 4.0901. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 19:02:01,521 - INFO - Saving checkpoint for epoch 3 at model_attention_files/training_checkpoints/ckpt-3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 19:02:01,522 - WARNING - Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "2025-04-24 19:02:01,902 - INFO - Best model saved to model_attention_files/pib_summarizer_attention_best.keras\n",
      "2025-04-24 19:02:01,906 - INFO - Epoch 4 Starting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full model saved to model_attention_files/pib_summarizer_attention_best.keras\n",
      "Time taken for epoch 3: 586.42 sec\n",
      "\n",
      "Epoch 4/30\n",
      "1049/1049 [==============================] - 554s 528ms/step - loss: 3.9971 - accuracy: 0.3318\n",
      "\n",
      "Validation:\n",
      "116/116 [==============================] - 24s 208ms/step - val_loss: 3.8742 - val_accuracy: 0.3535\n",
      "\n",
      "Validation loss improved from 4.0901 to 3.8742. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 19:11:40,322 - INFO - Saving checkpoint for epoch 4 at model_attention_files/training_checkpoints/ckpt-4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 19:11:40,322 - WARNING - Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "2025-04-24 19:11:40,706 - INFO - Best model saved to model_attention_files/pib_summarizer_attention_best.keras\n",
      "2025-04-24 19:11:40,710 - INFO - Epoch 5 Starting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full model saved to model_attention_files/pib_summarizer_attention_best.keras\n",
      "Time taken for epoch 4: 578.80 sec\n",
      "\n",
      "Epoch 5/30\n",
      "1049/1049 [==============================] - 551s 525ms/step - loss: 3.7875 - accuracy: 0.3469\n",
      "\n",
      "Validation:\n",
      "116/116 [==============================] - 24s 208ms/step - val_loss: 3.7333 - val_accuracy: 0.3656\n",
      "\n",
      "Validation loss improved from 3.8742 to 3.7333. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 19:21:16,607 - INFO - Saving checkpoint for epoch 5 at model_attention_files/training_checkpoints/ckpt-5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 19:21:16,608 - WARNING - Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "2025-04-24 19:21:16,990 - INFO - Best model saved to model_attention_files/pib_summarizer_attention_best.keras\n",
      "2025-04-24 19:21:16,995 - INFO - Epoch 6 Starting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full model saved to model_attention_files/pib_summarizer_attention_best.keras\n",
      "Time taken for epoch 5: 576.28 sec\n",
      "\n",
      "Epoch 6/30\n",
      "1049/1049 [==============================] - 546s 520ms/step - loss: 3.6307 - accuracy: 0.3591\n",
      "\n",
      "Validation:\n",
      "116/116 [==============================] - 24s 208ms/step - val_loss: 3.6355 - val_accuracy: 0.3750\n",
      "\n",
      "Validation loss improved from 3.7333 to 3.6355. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 19:30:47,148 - INFO - Saving checkpoint for epoch 6 at model_attention_files/training_checkpoints/ckpt-6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 19:30:47,149 - WARNING - Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "2025-04-24 19:30:47,526 - INFO - Best model saved to model_attention_files/pib_summarizer_attention_best.keras\n",
      "2025-04-24 19:30:47,531 - INFO - Epoch 7 Starting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full model saved to model_attention_files/pib_summarizer_attention_best.keras\n",
      "Time taken for epoch 6: 570.54 sec\n",
      "\n",
      "Epoch 7/30\n",
      "1049/1049 [==============================] - 547s 521ms/step - loss: 3.5060 - accuracy: 0.3689\n",
      "\n",
      "Validation:\n",
      "116/116 [==============================] - 24s 208ms/step - val_loss: 3.5583 - val_accuracy: 0.3826\n",
      "\n",
      "Validation loss improved from 3.6355 to 3.5583. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 19:40:19,013 - INFO - Saving checkpoint for epoch 7 at model_attention_files/training_checkpoints/ckpt-7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 19:40:19,014 - WARNING - Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "2025-04-24 19:40:19,394 - INFO - Best model saved to model_attention_files/pib_summarizer_attention_best.keras\n",
      "2025-04-24 19:40:19,398 - INFO - Epoch 8 Starting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full model saved to model_attention_files/pib_summarizer_attention_best.keras\n",
      "Time taken for epoch 7: 571.87 sec\n",
      "\n",
      "Epoch 8/30\n",
      "1049/1049 [==============================] - 547s 521ms/step - loss: 3.4050 - accuracy: 0.3772\n",
      "\n",
      "Validation:\n",
      "116/116 [==============================] - 24s 208ms/step - val_loss: 3.5038 - val_accuracy: 0.3891\n",
      "\n",
      "Validation loss improved from 3.5583 to 3.5038. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 19:49:50,527 - INFO - Saving checkpoint for epoch 8 at model_attention_files/training_checkpoints/ckpt-8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 19:49:50,528 - WARNING - Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "2025-04-24 19:49:50,982 - INFO - Best model saved to model_attention_files/pib_summarizer_attention_best.keras\n",
      "2025-04-24 19:49:50,986 - INFO - Epoch 9 Starting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full model saved to model_attention_files/pib_summarizer_attention_best.keras\n",
      "Time taken for epoch 8: 571.59 sec\n",
      "\n",
      "Epoch 9/30\n",
      "1049/1049 [==============================] - 544s 519ms/step - loss: 3.3194 - accuracy: 0.3847\n",
      "\n",
      "Validation:\n",
      "116/116 [==============================] - 24s 208ms/step - val_loss: 3.4577 - val_accuracy: 0.3938\n",
      "\n",
      "Validation loss improved from 3.5038 to 3.4577. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 19:59:19,695 - INFO - Saving checkpoint for epoch 9 at model_attention_files/training_checkpoints/ckpt-9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 19:59:19,696 - WARNING - Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "2025-04-24 19:59:20,074 - INFO - Best model saved to model_attention_files/pib_summarizer_attention_best.keras\n",
      "2025-04-24 19:59:20,078 - INFO - Epoch 10 Starting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full model saved to model_attention_files/pib_summarizer_attention_best.keras\n",
      "Time taken for epoch 9: 569.09 sec\n",
      "\n",
      "Epoch 10/30\n",
      "1049/1049 [==============================] - 543s 517ms/step - loss: 3.2475 - accuracy: 0.3913\n",
      "\n",
      "Validation:\n",
      "116/116 [==============================] - 24s 208ms/step - val_loss: 3.4221 - val_accuracy: 0.3981\n",
      "\n",
      "Validation loss improved from 3.4577 to 3.4221. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 20:08:47,349 - INFO - Saving checkpoint for epoch 10 at model_attention_files/training_checkpoints/ckpt-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 20:08:47,350 - WARNING - Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "2025-04-24 20:08:47,726 - INFO - Best model saved to model_attention_files/pib_summarizer_attention_best.keras\n",
      "2025-04-24 20:08:47,730 - INFO - Epoch 11 Starting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full model saved to model_attention_files/pib_summarizer_attention_best.keras\n",
      "Time taken for epoch 10: 567.65 sec\n",
      "\n",
      "Epoch 11/30\n",
      "1049/1049 [==============================] - 542s 516ms/step - loss: 3.0781 - accuracy: 0.4075\n",
      "\n",
      "Validation:\n",
      "116/116 [==============================] - 24s 208ms/step - val_loss: 3.3503 - val_accuracy: 0.4067\n",
      "\n",
      "Validation loss improved from 3.3699 to 3.3503. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 20:37:07,014 - INFO - Saving checkpoint for epoch 13 at model_attention_files/training_checkpoints/ckpt-13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 20:37:07,016 - WARNING - Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "2025-04-24 20:37:07,390 - INFO - Best model saved to model_attention_files/pib_summarizer_attention_best.keras\n",
      "2025-04-24 20:37:07,394 - INFO - Epoch 14 Starting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full model saved to model_attention_files/pib_summarizer_attention_best.keras\n",
      "Time taken for epoch 13: 566.80 sec\n",
      "\n",
      "Epoch 14/30\n",
      "1049/1049 [==============================] - 542s 517ms/step - loss: 3.0342 - accuracy: 0.4117\n",
      "\n",
      "Validation:\n",
      "116/116 [==============================] - 24s 208ms/step - val_loss: 3.3363 - val_accuracy: 0.4089\n",
      "\n",
      "Validation loss improved from 3.3503 to 3.3363. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 20:46:34,326 - INFO - Saving checkpoint for epoch 14 at model_attention_files/training_checkpoints/ckpt-14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 20:46:34,328 - WARNING - Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "2025-04-24 20:46:34,710 - INFO - Best model saved to model_attention_files/pib_summarizer_attention_best.keras\n",
      "2025-04-24 20:46:34,714 - INFO - Epoch 15 Starting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full model saved to model_attention_files/pib_summarizer_attention_best.keras\n",
      "Time taken for epoch 14: 567.32 sec\n",
      "\n",
      "Epoch 15/30\n",
      "1049/1049 [==============================] - 542s 516ms/step - loss: 2.9944 - accuracy: 0.4157\n",
      "\n",
      "Validation:\n",
      "116/116 [==============================] - 24s 208ms/step - val_loss: 3.3220 - val_accuracy: 0.4114\n",
      "\n",
      "Validation loss improved from 3.3363 to 3.3220. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 20:56:00,800 - INFO - Saving checkpoint for epoch 15 at model_attention_files/training_checkpoints/ckpt-15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 20:56:00,800 - WARNING - Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "2025-04-24 20:56:01,178 - INFO - Best model saved to model_attention_files/pib_summarizer_attention_best.keras\n",
      "2025-04-24 20:56:01,182 - INFO - Epoch 16 Starting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full model saved to model_attention_files/pib_summarizer_attention_best.keras\n",
      "Time taken for epoch 15: 566.47 sec\n",
      "\n",
      "Epoch 16/30\n",
      "1049/1049 [==============================] - 541s 516ms/step - loss: 2.9591 - accuracy: 0.4194\n",
      "\n",
      "Validation:\n",
      "116/116 [==============================] - 24s 208ms/step - val_loss: 3.3100 - val_accuracy: 0.4129\n",
      "\n",
      "Validation loss improved from 3.3220 to 3.3100. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 21:05:26,836 - INFO - Saving checkpoint for epoch 16 at model_attention_files/training_checkpoints/ckpt-16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 21:05:26,837 - WARNING - Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "2025-04-24 21:05:27,214 - INFO - Best model saved to model_attention_files/pib_summarizer_attention_best.keras\n",
      "2025-04-24 21:05:27,218 - INFO - Epoch 17 Starting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full model saved to model_attention_files/pib_summarizer_attention_best.keras\n",
      "Time taken for epoch 16: 566.04 sec\n",
      "\n",
      "Epoch 17/30\n",
      "1049/1049 [==============================] - 541s 516ms/step - loss: 2.9261 - accuracy: 0.4228\n",
      "\n",
      "Validation:\n",
      "116/116 [==============================] - 24s 208ms/step - val_loss: 3.2983 - val_accuracy: 0.4140\n",
      "\n",
      "Validation loss improved from 3.3100 to 3.2983. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 21:14:52,921 - INFO - Saving checkpoint for epoch 17 at model_attention_files/training_checkpoints/ckpt-17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 21:14:52,922 - WARNING - Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "2025-04-24 21:14:53,298 - INFO - Best model saved to model_attention_files/pib_summarizer_attention_best.keras\n",
      "2025-04-24 21:14:53,302 - INFO - Epoch 18 Starting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full model saved to model_attention_files/pib_summarizer_attention_best.keras\n",
      "Time taken for epoch 17: 566.08 sec\n",
      "\n",
      "Epoch 18/30\n",
      "1049/1049 [==============================] - 541s 516ms/step - loss: 2.8968 - accuracy: 0.4259\n",
      "\n",
      "Validation:\n",
      "116/116 [==============================] - 24s 208ms/step - val_loss: 3.2891 - val_accuracy: 0.4160\n",
      "\n",
      "Validation loss improved from 3.2983 to 3.2891. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 21:24:18,948 - INFO - Saving checkpoint for epoch 18 at model_attention_files/training_checkpoints/ckpt-18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 21:24:18,949 - WARNING - Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "2025-04-24 21:24:19,326 - INFO - Best model saved to model_attention_files/pib_summarizer_attention_best.keras\n",
      "2025-04-24 21:24:19,330 - INFO - Epoch 19 Starting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full model saved to model_attention_files/pib_summarizer_attention_best.keras\n",
      "Time taken for epoch 18: 566.03 sec\n",
      "\n",
      "Epoch 19/30\n",
      "1049/1049 [==============================] - 541s 516ms/step - loss: 2.8694 - accuracy: 0.4286\n",
      "\n",
      "Validation:\n",
      "116/116 [==============================] - 24s 208ms/step - val_loss: 3.2802 - val_accuracy: 0.4174\n",
      "\n",
      "Validation loss improved from 3.2891 to 3.2802. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 21:33:44,719 - INFO - Saving checkpoint for epoch 19 at model_attention_files/training_checkpoints/ckpt-19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 21:33:44,720 - WARNING - Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "2025-04-24 21:33:45,098 - INFO - Best model saved to model_attention_files/pib_summarizer_attention_best.keras\n",
      "2025-04-24 21:33:45,102 - INFO - Epoch 20 Starting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full model saved to model_attention_files/pib_summarizer_attention_best.keras\n",
      "Time taken for epoch 19: 565.77 sec\n",
      "\n",
      "Epoch 20/30\n",
      "1049/1049 [==============================] - 541s 515ms/step - loss: 2.8441 - accuracy: 0.4314\n",
      "\n",
      "Validation:\n",
      "116/116 [==============================] - 24s 208ms/step - val_loss: 3.2742 - val_accuracy: 0.4185\n",
      "\n",
      "Validation loss improved from 3.2802 to 3.2742. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 21:43:10,153 - INFO - Saving checkpoint for epoch 20 at model_attention_files/training_checkpoints/ckpt-20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 21:43:10,154 - WARNING - Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "2025-04-24 21:43:10,534 - INFO - Best model saved to model_attention_files/pib_summarizer_attention_best.keras\n",
      "2025-04-24 21:43:10,538 - INFO - Epoch 21 Starting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full model saved to model_attention_files/pib_summarizer_attention_best.keras\n",
      "Time taken for epoch 20: 565.44 sec\n",
      "\n",
      "Epoch 21/30\n",
      "1049/1049 [==============================] - 540s 515ms/step - loss: 2.8213 - accuracy: 0.4338\n",
      "\n",
      "Validation:\n",
      "116/116 [==============================] - 24s 208ms/step - val_loss: 3.2683 - val_accuracy: 0.4193\n",
      "\n",
      "Validation loss improved from 3.2742 to 3.2683. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 21:52:35,072 - INFO - Saving checkpoint for epoch 21 at model_attention_files/training_checkpoints/ckpt-21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 21:52:35,073 - WARNING - Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "2025-04-24 21:52:35,450 - INFO - Best model saved to model_attention_files/pib_summarizer_attention_best.keras\n",
      "2025-04-24 21:52:35,454 - INFO - Epoch 22 Starting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full model saved to model_attention_files/pib_summarizer_attention_best.keras\n",
      "Time taken for epoch 21: 564.92 sec\n",
      "\n",
      "Epoch 22/30\n",
      "1049/1049 [==============================] - 540s 515ms/step - loss: 2.7981 - accuracy: 0.4364\n",
      "\n",
      "Validation:\n",
      "116/116 [==============================] - 24s 207ms/step - val_loss: 3.2685 - val_accuracy: 0.4208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 22:01:59,741 - INFO - Epoch 23 Starting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation loss did not improve from 3.2683.\n",
      "Time taken for epoch 22: 564.29 sec\n",
      "\n",
      "Epoch 23/30\n",
      "1049/1049 [==============================] - 541s 516ms/step - loss: 2.7782 - accuracy: 0.4385\n",
      "\n",
      "Validation:\n",
      "116/116 [==============================] - 24s 208ms/step - val_loss: 3.2591 - val_accuracy: 0.4216\n",
      "\n",
      "Validation loss improved from 3.2683 to 3.2591. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 22:11:25,340 - INFO - Saving checkpoint for epoch 23 at model_attention_files/training_checkpoints/ckpt-22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 22:11:25,341 - WARNING - Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "2025-04-24 22:11:25,718 - INFO - Best model saved to model_attention_files/pib_summarizer_attention_best.keras\n",
      "2025-04-24 22:11:25,723 - INFO - Epoch 24 Starting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full model saved to model_attention_files/pib_summarizer_attention_best.keras\n",
      "Time taken for epoch 23: 565.98 sec\n",
      "\n",
      "Epoch 24/30\n",
      "1049/1049 [==============================] - 540s 515ms/step - loss: 2.7593 - accuracy: 0.4406\n",
      "\n",
      "Validation:\n",
      "116/116 [==============================] - 24s 208ms/step - val_loss: 3.2521 - val_accuracy: 0.4227\n",
      "\n",
      "Validation loss improved from 3.2591 to 3.2521. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 22:20:50,551 - INFO - Saving checkpoint for epoch 24 at model_attention_files/training_checkpoints/ckpt-23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 22:20:50,552 - WARNING - Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "2025-04-24 22:20:50,930 - INFO - Best model saved to model_attention_files/pib_summarizer_attention_best.keras\n",
      "2025-04-24 22:20:50,935 - INFO - Epoch 25 Starting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full model saved to model_attention_files/pib_summarizer_attention_best.keras\n",
      "Time taken for epoch 24: 565.21 sec\n",
      "\n",
      "Epoch 25/30\n",
      "1049/1049 [==============================] - 540s 515ms/step - loss: 2.7410 - accuracy: 0.4424\n",
      "\n",
      "Validation:\n",
      "116/116 [==============================] - 24s 208ms/step - val_loss: 3.2508 - val_accuracy: 0.4236\n",
      "\n",
      "Validation loss improved from 3.2521 to 3.2508. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 22:30:15,836 - INFO - Saving checkpoint for epoch 25 at model_attention_files/training_checkpoints/ckpt-24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 22:30:15,837 - WARNING - Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "2025-04-24 22:30:16,214 - INFO - Best model saved to model_attention_files/pib_summarizer_attention_best.keras\n",
      "2025-04-24 22:30:16,218 - INFO - Epoch 26 Starting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full model saved to model_attention_files/pib_summarizer_attention_best.keras\n",
      "Time taken for epoch 25: 565.28 sec\n",
      "\n",
      "Epoch 26/30\n",
      "1049/1049 [==============================] - 540s 515ms/step - loss: 2.7254 - accuracy: 0.4444\n",
      "\n",
      "Validation:\n",
      "116/116 [==============================] - 24s 208ms/step - val_loss: 3.2493 - val_accuracy: 0.4249\n",
      "\n",
      "Validation loss improved from 3.2508 to 3.2493. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 22:39:40,528 - INFO - Saving checkpoint for epoch 26 at model_attention_files/training_checkpoints/ckpt-25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 22:39:40,529 - WARNING - Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "2025-04-24 22:39:40,906 - INFO - Best model saved to model_attention_files/pib_summarizer_attention_best.keras\n",
      "2025-04-24 22:39:40,910 - INFO - Epoch 27 Starting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full model saved to model_attention_files/pib_summarizer_attention_best.keras\n",
      "Time taken for epoch 26: 564.69 sec\n",
      "\n",
      "Epoch 27/30\n",
      "1049/1049 [==============================] - 541s 516ms/step - loss: 2.7089 - accuracy: 0.4462\n",
      "\n",
      "Validation:\n",
      "116/116 [==============================] - 24s 208ms/step - val_loss: 3.2459 - val_accuracy: 0.4249\n",
      "\n",
      "Validation loss improved from 3.2493 to 3.2459. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 22:49:06,442 - INFO - Saving checkpoint for epoch 27 at model_attention_files/training_checkpoints/ckpt-26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 22:49:06,443 - WARNING - Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "2025-04-24 22:49:06,822 - INFO - Best model saved to model_attention_files/pib_summarizer_attention_best.keras\n",
      "2025-04-24 22:49:06,827 - INFO - Epoch 28 Starting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full model saved to model_attention_files/pib_summarizer_attention_best.keras\n",
      "Time taken for epoch 27: 565.92 sec\n",
      "\n",
      "Epoch 28/30\n",
      "1049/1049 [==============================] - 540s 514ms/step - loss: 2.6938 - accuracy: 0.4479\n",
      "\n",
      "Validation:\n",
      "116/116 [==============================] - 24s 208ms/step - val_loss: 3.2397 - val_accuracy: 0.4255\n",
      "\n",
      "Validation loss improved from 3.2459 to 3.2397. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 22:58:31,033 - INFO - Saving checkpoint for epoch 28 at model_attention_files/training_checkpoints/ckpt-27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 22:58:31,034 - WARNING - Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "2025-04-24 22:58:31,414 - INFO - Best model saved to model_attention_files/pib_summarizer_attention_best.keras\n",
      "2025-04-24 22:58:31,418 - INFO - Epoch 29 Starting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full model saved to model_attention_files/pib_summarizer_attention_best.keras\n",
      "Time taken for epoch 28: 564.59 sec\n",
      "\n",
      "Epoch 29/30\n",
      "1049/1049 [==============================] - 541s 515ms/step - loss: 2.6794 - accuracy: 0.4494\n",
      "\n",
      "Validation:\n",
      "116/116 [==============================] - 24s 207ms/step - val_loss: 3.2382 - val_accuracy: 0.4263\n",
      "\n",
      "Validation loss improved from 3.2397 to 3.2382. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 23:07:56,458 - INFO - Saving checkpoint for epoch 29 at model_attention_files/training_checkpoints/ckpt-28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 23:07:56,458 - WARNING - Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "2025-04-24 23:07:56,838 - INFO - Best model saved to model_attention_files/pib_summarizer_attention_best.keras\n",
      "2025-04-24 23:07:56,842 - INFO - Epoch 30 Starting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full model saved to model_attention_files/pib_summarizer_attention_best.keras\n",
      "Time taken for epoch 29: 565.42 sec\n",
      "\n",
      "Epoch 30/30\n",
      "1049/1049 [==============================] - 541s 515ms/step - loss: 2.6661 - accuracy: 0.4509\n",
      "\n",
      "Validation:\n",
      "116/116 [==============================] - 24s 208ms/step - val_loss: 3.2377 - val_accuracy: 0.4264\n",
      "\n",
      "Validation loss improved from 3.2382 to 3.2377. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 23:17:21,910 - INFO - Saving checkpoint for epoch 30 at model_attention_files/training_checkpoints/ckpt-29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 23:17:21,911 - WARNING - Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "2025-04-24 23:17:22,290 - INFO - Best model saved to model_attention_files/pib_summarizer_attention_best.keras\n",
      "2025-04-24 23:17:22,291 - INFO - Training finished. Best validation loss: 3.2377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full model saved to model_attention_files/pib_summarizer_attention_best.keras\n",
      "Time taken for epoch 30: 565.45 sec\n",
      "\n",
      "--- Training Finished ---\n",
      "Best validation loss achieved: 3.2377\n",
      "Loading best model weights from model_attention_files/pib_summarizer_attention_best.keras...\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 23:17:24,682 - WARNING - No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "2025-04-24 23:17:24,683 - INFO - Best model loaded successfully.\n",
      "2025-04-24 23:17:24,685 - INFO - Block 8 completed. Model trained with custom loop (Teacher Forcing).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model loaded successfully.\n",
      "Block 8 completed. Model trained with custom loop (Teacher Forcing).\n"
     ]
    }
   ],
   "source": [
    "# Block 8: Custom Training Loop (Scheduled Sampling) + Execution (Corrected train/val steps)\n",
    "\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# --- Metrics Definition --- (Unchanged)\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "val_loss = tf.keras.metrics.Mean(name='val_loss')\n",
    "val_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='val_accuracy')\n",
    "\n",
    "# --- Masked Loss Function --- (Unchanged)\n",
    "def masked_loss(real, pred):\n",
    "    loss = loss_object(real, pred)\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, PAD_ID))\n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "    total_loss = tf.reduce_sum(loss)\n",
    "    num_active_elements = tf.reduce_sum(mask)\n",
    "    return tf.math.divide_no_nan(total_loss, num_active_elements)\n",
    "\n",
    "# --- Masked Accuracy Function (Removed - Handled directly by metric) ---\n",
    "# We update metrics directly in train/val steps\n",
    "\n",
    "# --- Train Step Function (Simplified Forward Pass) ---\n",
    "@tf.function\n",
    "def train_step(inputs):\n",
    "    \"\"\"Performs one training step with Gradient Tape.\"\"\"\n",
    "    # Unpack the dictionary format from tf.data.Dataset\n",
    "    encoder_input_seq = inputs[0]['encoder_inputs']\n",
    "    decoder_input_seq = inputs[0]['decoder_inputs'] # Ground truth for teacher forcing\n",
    "    decoder_target_seq = inputs[1]['output_layer'] # Ground truth targets\n",
    "\n",
    "    # We are using Teacher Forcing during the training forward pass defined by the Functional Model\n",
    "    # The Model architecture handles the sequence processing internally.\n",
    "    # Scheduled Sampling logic needs to be implemented differently, typically\n",
    "    # by creating a separate model or modifying the layer's call method,\n",
    "    # which is much more complex than originally anticipated with the Functional API.\n",
    "    # For now, we will proceed WITHOUT scheduled sampling, using standard Teacher Forcing\n",
    "    # as defined by the Keras model structure.\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Perform the forward pass using the entire model\n",
    "        # Input is a dictionary matching the keys used in tf.data.Dataset\n",
    "        predictions = model({'encoder_inputs': encoder_input_seq, 'decoder_inputs': decoder_input_seq}, training=True)\n",
    "        # predictions shape: (batch_size, max_len_summary_dec_input, vocab_size)\n",
    "\n",
    "        # Calculate loss for the entire sequence prediction\n",
    "        # The target sequence `decoder_target_seq` already corresponds to the `predictions`\n",
    "        # (shifted by one, handled during data prep)\n",
    "        batch_loss = masked_loss(decoder_target_seq, predictions)\n",
    "\n",
    "    # Calculate Gradients\n",
    "    variables = model.trainable_variables\n",
    "    gradients = tape.gradient(batch_loss, variables)\n",
    "\n",
    "    # Apply Gradients\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    # Update Metrics (use the actual targets and predictions)\n",
    "    train_loss.update_state(batch_loss)\n",
    "    train_accuracy.update_state(decoder_target_seq, predictions,\n",
    "                                sample_weight=tf.cast(tf.math.logical_not(tf.math.equal(decoder_target_seq, PAD_ID)), dtype=tf.float32))\n",
    "\n",
    "\n",
    "    return batch_loss\n",
    "\n",
    "# --- Validation Step Function (Simplified Forward Pass) ---\n",
    "@tf.function\n",
    "def val_step(inputs):\n",
    "    \"\"\"Performs one validation step (using Teacher Forcing).\"\"\"\n",
    "    encoder_input_seq = inputs[0]['encoder_inputs']\n",
    "    decoder_input_seq = inputs[0]['decoder_inputs'] # Ground truth for teacher forcing\n",
    "    decoder_target_seq = inputs[1]['output_layer']\n",
    "\n",
    "    # Perform forward pass in inference mode (training=False)\n",
    "    predictions = model({'encoder_inputs': encoder_input_seq, 'decoder_inputs': decoder_input_seq}, training=False)\n",
    "\n",
    "    # Calculate loss\n",
    "    batch_loss = masked_loss(decoder_target_seq, predictions)\n",
    "\n",
    "    # Update Validation Metrics\n",
    "    val_loss.update_state(batch_loss)\n",
    "    val_accuracy.update_state(decoder_target_seq, predictions,\n",
    "                              sample_weight=tf.cast(tf.math.logical_not(tf.math.equal(decoder_target_seq, PAD_ID)), dtype=tf.float32))\n",
    "\n",
    "    return batch_loss\n",
    "\n",
    "# --- Custom Training Function (Simplified - No Scheduled Sampling Implemented Yet) ---\n",
    "def train_model_custom(model_to_train, train_ds, val_ds, epochs, optimizer_obj,\n",
    "                       # Remove sampling parameters for now\n",
    "                       checkpoint_dir, best_model_path, tensorboard_log_dir,\n",
    "                       early_stopping_patience, reduce_lr_patience, reduce_lr_factor):\n",
    "    \"\"\"Runs the custom training loop (currently using Teacher Forcing).\"\"\"\n",
    "\n",
    "    logging.info(\"Starting custom model training loop (Teacher Forcing)...\")\n",
    "    print(\"\\n--- Custom Model Training (Teacher Forcing) ---\")\n",
    "    print(\"NOTE: Scheduled Sampling implementation deferred due to complexity with Functional API.\")\n",
    "\n",
    "    # --- Setup Checkpointing --- (Unchanged)\n",
    "    checkpoint = tf.train.Checkpoint(optimizer=optimizer_obj, model=model_to_train)\n",
    "    ckpt_manager = tf.train.CheckpointManager(checkpoint, checkpoint_dir, max_to_keep=1) # Keep only the best\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter_early_stop = 0\n",
    "    patience_counter_lr_reduce = 0\n",
    "\n",
    "    # --- Setup TensorBoard --- (Unchanged)\n",
    "    summary_writer = tf.summary.create_file_writer(tensorboard_log_dir)\n",
    "\n",
    "    # --- Calculate steps per epoch --- (Unchanged)\n",
    "    try:\n",
    "        steps_per_epoch_train = len(train_ds)\n",
    "        steps_per_epoch_val = len(val_ds)\n",
    "        print(f\"Train steps per epoch: {steps_per_epoch_train}\")\n",
    "        print(f\"Validation steps per epoch: {steps_per_epoch_val}\")\n",
    "    except TypeError:\n",
    "        print(\"Could not determine dataset lengths. Ensure drop_remainder=True.\")\n",
    "        logging.error(\"Could not determine dataset lengths.\")\n",
    "        return None # Exit if dataset length unknown\n",
    "\n",
    "    # --- Epoch Loop ---\n",
    "    for epoch in range(epochs):\n",
    "        start_time_epoch = time.time()\n",
    "\n",
    "        # Reset metrics at the start of each epoch (Unchanged)\n",
    "        train_loss.reset_states()\n",
    "        train_accuracy.reset_states()\n",
    "        val_loss.reset_states()\n",
    "        val_accuracy.reset_states()\n",
    "\n",
    "        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "        logging.info(f\"Epoch {epoch + 1} Starting\")\n",
    "\n",
    "        # --- Training Phase ---\n",
    "        pb_train = tf.keras.utils.Progbar(steps_per_epoch_train, stateful_metrics=['loss', 'accuracy'])\n",
    "        for i, batch_data in enumerate(train_ds):\n",
    "            batch_train_loss = train_step(batch_data) # Call simplified train_step\n",
    "            metrics_values = [('loss', train_loss.result()), ('accuracy', train_accuracy.result())]\n",
    "            pb_train.update(i + 1, values=metrics_values)\n",
    "\n",
    "        # --- Validation Phase ---\n",
    "        print(\"\\nValidation:\")\n",
    "        pb_val = tf.keras.utils.Progbar(steps_per_epoch_val, stateful_metrics=['val_loss', 'val_accuracy'])\n",
    "        for i, batch_data in enumerate(val_ds):\n",
    "            val_step(batch_data) # Call simplified val_step\n",
    "            metrics_values = [('val_loss', val_loss.result()), ('val_accuracy', val_accuracy.result())]\n",
    "            pb_val.update(i + 1, values=metrics_values)\n",
    "\n",
    "        epoch_val_loss = val_loss.result()\n",
    "        epoch_train_loss = train_loss.result()\n",
    "        epoch_val_acc = val_accuracy.result()\n",
    "        epoch_train_acc = train_accuracy.result()\n",
    "\n",
    "        # --- Log Metrics to TensorBoard --- (Unchanged, but removed sampling prob)\n",
    "        with summary_writer.as_default():\n",
    "            tf.summary.scalar('train_loss', epoch_train_loss, step=epoch)\n",
    "            tf.summary.scalar('train_accuracy', epoch_train_acc, step=epoch)\n",
    "            tf.summary.scalar('val_loss', epoch_val_loss, step=epoch)\n",
    "            tf.summary.scalar('val_accuracy', epoch_val_acc, step=epoch)\n",
    "            tf.summary.scalar('learning_rate', optimizer_obj.learning_rate, step=epoch)\n",
    "            # tf.summary.scalar('sampling_probability', current_sampling_prob, step=epoch) # Removed\n",
    "\n",
    "        # --- Checkpointing (Save Best Model based on val_loss) --- (Unchanged)\n",
    "        if epoch_val_loss < best_val_loss:\n",
    "            print(f\"\\nValidation loss improved from {best_val_loss:.4f} to {epoch_val_loss:.4f}. Saving model...\")\n",
    "            best_val_loss = epoch_val_loss\n",
    "            ckpt_save_path = ckpt_manager.save()\n",
    "            logging.info(f'Saving checkpoint for epoch {epoch+1} at {ckpt_save_path}')\n",
    "            try:\n",
    "                 model_to_train.save(best_model_path, save_format='keras')\n",
    "                 print(f\"Full model saved to {best_model_path}\")\n",
    "                 logging.info(f\"Best model saved to {best_model_path}\")\n",
    "            except Exception as e:\n",
    "                 print(f\"Error saving full model: {e}\")\n",
    "                 logging.error(f\"Error saving full model: {e}\", exc_info=True)\n",
    "            patience_counter_early_stop = 0\n",
    "            patience_counter_lr_reduce = 0\n",
    "        else:\n",
    "            print(f\"\\nValidation loss did not improve from {best_val_loss:.4f}.\")\n",
    "            patience_counter_early_stop += 1\n",
    "            patience_counter_lr_reduce += 1\n",
    "\n",
    "        # --- Reduce Learning Rate on Plateau --- (Unchanged)\n",
    "        if patience_counter_lr_reduce >= reduce_lr_patience:\n",
    "            new_lr = optimizer_obj.learning_rate * reduce_lr_factor\n",
    "            if new_lr >= 1e-6:\n",
    "                optimizer_obj.learning_rate.assign(new_lr)\n",
    "                print(f\"Reducing learning rate to {optimizer_obj.learning_rate.numpy():.6f}.\")\n",
    "                logging.info(f\"Reducing learning rate to {optimizer_obj.learning_rate.numpy():.6f}.\")\n",
    "                patience_counter_lr_reduce = 0\n",
    "            else:\n",
    "                print(f\"Learning rate reduction skipped, already at minimum.\")\n",
    "                logging.warning(f\"Learning rate reduction skipped, already at or below minimum.\")\n",
    "\n",
    "        # --- Early Stopping --- (Unchanged)\n",
    "        if patience_counter_early_stop >= early_stopping_patience:\n",
    "            print(f\"\\nEarly stopping triggered after {early_stopping_patience} epochs without improvement.\")\n",
    "            logging.info(f\"Early stopping triggered at epoch {epoch+1}.\")\n",
    "            break\n",
    "\n",
    "        # --- NO Epsilon Decay Update Needed Now ---\n",
    "\n",
    "        end_time_epoch = time.time()\n",
    "        print(f\"Time taken for epoch {epoch + 1}: {end_time_epoch - start_time_epoch:.2f} sec\")\n",
    "\n",
    "    print(\"\\n--- Training Finished ---\")\n",
    "    print(f\"Best validation loss achieved: {best_val_loss:.4f}\")\n",
    "    logging.info(f\"Training finished. Best validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "    # Optional: Load the best weights back into the model (Unchanged)\n",
    "    print(f\"Loading best model weights from {best_model_path}...\")\n",
    "    try:\n",
    "        model_to_train = tf.keras.models.load_model(best_model_path)\n",
    "        print(\"Best model loaded successfully.\")\n",
    "        logging.info(\"Best model loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading best model: {e}\")\n",
    "        logging.error(f\"Error loading best model: {e}\")\n",
    "\n",
    "    return model_to_train\n",
    "\n",
    "# --- Execution for Block 8 --- (Simplified Call)\n",
    "\n",
    "# Check if all required components are available (Unchanged)\n",
    "if ('model' in locals() and model and\n",
    "    'optimizer' in locals() and optimizer and\n",
    "    'loss_object' in locals() and loss_object and\n",
    "    'train_dataset' in locals() and train_dataset and\n",
    "    'val_dataset' in locals() and val_dataset):\n",
    "\n",
    "    # Define paths (Unchanged)\n",
    "    CHECKPOINT_DIR = os.path.join(OUTPUT_DIR, 'training_checkpoints')\n",
    "    BEST_MODEL_SAVE_PATH = os.path.join(OUTPUT_DIR, 'pib_summarizer_attention_best.keras')\n",
    "    TENSORBOARD_LOG_DIR = LOG_DIR\n",
    "\n",
    "    print(f\"Checkpoint directory: {CHECKPOINT_DIR}\")\n",
    "    print(f\"Best model save path: {BEST_MODEL_SAVE_PATH}\")\n",
    "    print(f\"TensorBoard Log Directory (for custom loop): {TENSORBOARD_LOG_DIR}\")\n",
    "    os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(TENSORBOARD_LOG_DIR), exist_ok=True)\n",
    "\n",
    "\n",
    "    # Start training (call simplified function)\n",
    "    trained_model = train_model_custom(\n",
    "        model_to_train=model,\n",
    "        train_ds=train_dataset,\n",
    "        val_ds=val_dataset,\n",
    "        epochs=EPOCHS,\n",
    "        optimizer_obj=optimizer,\n",
    "        # Remove sampling parameters from call\n",
    "        checkpoint_dir=CHECKPOINT_DIR,\n",
    "        best_model_path=BEST_MODEL_SAVE_PATH,\n",
    "        tensorboard_log_dir=TENSORBOARD_LOG_DIR,\n",
    "        early_stopping_patience=EARLY_STOPPING_PATIENCE_MANUAL,\n",
    "        reduce_lr_patience=REDUCE_LR_PATIENCE_MANUAL,\n",
    "        reduce_lr_factor=REDUCE_LR_FACTOR_MANUAL\n",
    "    )\n",
    "\n",
    "    # Update the 'model' variable (Unchanged)\n",
    "    if trained_model:\n",
    "        model = trained_model\n",
    "        print(\"Block 8 completed. Model trained with custom loop (Teacher Forcing).\")\n",
    "        logging.info(\"Block 8 completed. Model trained with custom loop (Teacher Forcing).\")\n",
    "    else:\n",
    "        print(\"Block 8: Custom training loop did not return a valid model.\")\n",
    "        logging.error(\"Block 8: Custom training loop did not return a valid model.\")\n",
    "\n",
    "else:\n",
    "     # Error message (Unchanged)\n",
    "     print(\"Skipping custom training (Block 8) due to missing model, optimizer, loss object, or datasets.\")\n",
    "     logging.error(\"Skipping Block 8 execution due to missing components.\")\n",
    "     print(f\"  Model exists: {'model' in locals() and model is not None}\")\n",
    "     print(f\"  Optimizer exists: {'optimizer' in locals() and optimizer is not None}\")\n",
    "     print(f\"  Loss object exists: {'loss_object' in locals() and loss_object is not None}\")\n",
    "     print(f\"  Train dataset exists: {'train_dataset' in locals() and train_dataset is not None}\")\n",
    "     print(f\"  Validation dataset exists: {'val_dataset' in locals() and val_dataset is not None}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "036543d1-cf7b-42ca-b1dc-efbbf578e836",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-25 03:28:05,490 - INFO - Setting up inference models with attention from: model_attention_files/pib_summarizer_attention_best.keras\n",
      "2025-04-25 03:28:05,491 - INFO - Loading the full trained model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load best model from: model_attention_files/pib_summarizer_attention_best.keras\n",
      "\n",
      "--- Inference Setup (with Attention) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-25 03:28:07,858 - INFO - Layers found in loaded model: ['encoder_inputs', 'encoder_embedding', 'decoder_inputs', 'dropout', 'decoder_embedding', 'bidirectional', 'dropout_1', 'encoder_final_h', 'encoder_final_c', 'decoder_lstm_1', 'attention_layer', 'decoder_attention_concat', 'dropout_2', 'output_layer']\n",
      "2025-04-25 03:28:07,859 - INFO - Creating inference encoder model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded trained model from model_attention_files/pib_summarizer_attention_best.keras\n",
      "Note: 'No training configuration' warning during load is expected and benign.\n",
      "Layers found in loaded model: ['encoder_inputs', 'encoder_embedding', 'decoder_inputs', 'dropout', 'decoder_embedding', 'bidirectional', 'dropout_1', 'encoder_final_h', 'encoder_final_c', 'decoder_lstm_1', 'attention_layer', 'decoder_attention_concat', 'dropout_2', 'output_layer']\n",
      "Inference Encoder created.\n",
      "Model: \"inference_encoder\"\n",
      "____________________________________________________________________________________________________\n",
      " Layer (type)                    Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      " encoder_inputs (InputLayer)     [(None, 1024)]        0           []                               \n",
      "                                                                                                    \n",
      " encoder_embedding (Embedding)   (None, 1024, 100)     3000000     ['encoder_inputs[0][0]']         \n",
      "                                                                                                    \n",
      " dropout (Dropout)               (None, 1024, 100)     0           ['encoder_embedding[0][0]']      \n",
      "                                                                                                    \n",
      " bidirectional (Bidirectional)   [(None, 1024, 256),   234496      ['dropout[0][0]']                \n",
      "                                  (None, 128),                                                      \n",
      "                                  (None, 128),                                                      \n",
      "                                  (None, 128),                                                      \n",
      "                                  (None, 128)]                                                      \n",
      "                                                                                                    \n",
      " encoder_final_h (Concatenate)   (None, 256)           0           ['bidirectional[0][1]',          \n",
      "                                                                    'bidirectional[0][3]']          \n",
      "                                                                                                    \n",
      " encoder_final_c (Concatenate)   (None, 256)           0           ['bidirectional[0][2]',          \n",
      "                                                                    'bidirectional[0][4]']          \n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Total params: 3,234,496\n",
      "Trainable params: 3,234,496\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-25 03:28:07,874 - INFO - Creating inference decoder model...\n",
      "2025-04-25 03:28:07,880 - INFO - Using 1 decoder LSTM layer(s).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1 decoder LSTM layer(s) based on loaded model.\n",
      "Inference Decoder created.\n",
      "Model: \"inference_decoder\"\n",
      "________________________________________________________________________________________________________________________\n",
      " Layer (type)                          Output Shape               Param #       Connected to                            \n",
      "========================================================================================================================\n",
      " inf_decoder_input_token (InputLayer)  [(None, 1)]                0             []                                      \n",
      "                                                                                                                        \n",
      " inf_decoder_embedding (Embedding)     (None, 1, 100)             3000000       ['inf_decoder_input_token[0][0]']       \n",
      "                                                                                                                        \n",
      " inf_dropout_1 (Dropout)               (None, 1, 100)             0             ['inf_decoder_embedding[0][0]']         \n",
      "                                                                                                                        \n",
      " inf_decoder_input_h_0 (InputLayer)    [(None, 256)]              0             []                                      \n",
      "                                                                                                                        \n",
      " inf_decoder_input_c_0 (InputLayer)    [(None, 256)]              0             []                                      \n",
      "                                                                                                                        \n",
      " inf_decoder_lstm_1 (LSTM)             [(None, 1, 256),           365568        ['inf_dropout_1[0][0]',                 \n",
      "                                        (None, 256),                             'inf_decoder_input_h_0[0][0]',         \n",
      "                                        (None, 256)]                             'inf_decoder_input_c_0[0][0]']         \n",
      "                                                                                                                        \n",
      " inf_encoder_output_seq (InputLayer)   [(None, 1024, 256)]        0             []                                      \n",
      "                                                                                                                        \n",
      " inf_attention_layer (AdditiveAttentio  ((None, 1, 256),          256           ['inf_decoder_lstm_1[0][0]',            \n",
      " n)                                     (None, 1, 1024))                         'inf_encoder_output_seq[0][0]']        \n",
      "                                                                                                                        \n",
      " inf_decoder_attention_concat (Concate  (None, 1, 512)            0             ['inf_decoder_lstm_1[0][0]',            \n",
      " nate)                                                                           'inf_attention_layer[0][0]']           \n",
      "                                                                                                                        \n",
      " inf_dropout_2 (Dropout)               (None, 1, 512)             0             ['inf_decoder_attention_concat[0][0]']  \n",
      "                                                                                                                        \n",
      " inf_output_layer (Dense)              (None, 1, 30000)           15390000      ['inf_dropout_2[0][0]']                 \n",
      "                                                                                                                        \n",
      "========================================================================================================================\n",
      "Total params: 18,755,824\n",
      "Trainable params: 18,755,824\n",
      "Non-trainable params: 0\n",
      "________________________________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-25 03:28:08,229 - INFO - Block 9 completed successfully. Inference encoder and decoder created.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Block 9 completed successfully. Inference encoder and decoder created.\n"
     ]
    }
   ],
   "source": [
    "# Block 9: Inference Model Setup (with Attention) + Execution (Corrected for 1 Enc/1 Dec Layer)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Bidirectional, Dense, Concatenate, AdditiveAttention, Dropout\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "# --- Function Definition (Corrected for 1 Enc / 1 Dec Layer & Layer Names) ---\n",
    "def setup_inference_models_attention_alt(trained_model_path,\n",
    "                                         num_decoder_layers_config, # <<< ADDED ARGUMENT HERE\n",
    "                                         decoder_lstm_units,\n",
    "                                         max_len_input, lstm_units): # lstm_units = encoder units per dir # lstm_units = encoder units per dir\n",
    "    \"\"\"Loads the trained model and creates separate encoder/decoder models for inference.\"\"\"\n",
    "    logging.info(f\"Setting up inference models with attention from: {trained_model_path}\")\n",
    "    print(\"\\n--- Inference Setup (with Attention) ---\")\n",
    "\n",
    "    # --- 1. Load the trained model ---\n",
    "    if not os.path.exists(trained_model_path):\n",
    "        print(f\"Error: Trained model file not found at {trained_model_path}. Cannot setup inference.\")\n",
    "        logging.error(f\"Trained model not found: {trained_model_path}\")\n",
    "        print(\"-\" * 30)\n",
    "        return None, None\n",
    "    try:\n",
    "        logging.info(\"Loading the full trained model...\")\n",
    "        original_verbosity = tf.get_logger().level\n",
    "        tf.get_logger().setLevel(logging.ERROR)\n",
    "        trained_model = load_model(trained_model_path, compile=False)\n",
    "        tf.get_logger().setLevel(original_verbosity)\n",
    "        print(f\"Successfully loaded trained model from {trained_model_path}\")\n",
    "        print(\"Note: 'No training configuration' warning during load is expected and benign.\")\n",
    "        loaded_layer_names = [layer.name for layer in trained_model.layers]\n",
    "        print(f\"Layers found in loaded model: {loaded_layer_names}\")\n",
    "        logging.info(f\"Layers found in loaded model: {loaded_layer_names}\")\n",
    "    except Exception as e:\n",
    "        tf.get_logger().setLevel(original_verbosity)\n",
    "        logging.error(f\"Failed to load trained model from {trained_model_path}: {e}\", exc_info=True)\n",
    "        print(f\"Error: Failed to load trained model from {trained_model_path}: {e}\")\n",
    "        print(\"-\" * 30)\n",
    "        return None, None\n",
    "\n",
    "    # --- 2. Create Inference Encoder Model ---\n",
    "    logging.info(\"Creating inference encoder model...\")\n",
    "    inf_encoder = None\n",
    "    try:\n",
    "        encoder_input_layer = trained_model.get_layer(\"encoder_inputs\").input\n",
    "        # Use the correct layer name 'bidirectional' (assuming 1 encoder layer)\n",
    "        last_encoder_bilstm_layer = trained_model.get_layer(\"bidirectional\")\n",
    "        encoder_full_seq_output = last_encoder_bilstm_layer.output[0]\n",
    "        # Get the final concatenated states\n",
    "        encoder_final_state_h = trained_model.get_layer(\"encoder_final_h\").output\n",
    "        encoder_final_state_c = trained_model.get_layer(\"encoder_final_c\").output\n",
    "\n",
    "        inf_encoder = Model(inputs=encoder_input_layer,\n",
    "                            outputs=[encoder_full_seq_output, encoder_final_state_h, encoder_final_state_c],\n",
    "                            name=\"inference_encoder\")\n",
    "        print(\"Inference Encoder created.\")\n",
    "        inf_encoder.summary(line_length=100) # Optional summary\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to create inference encoder: {e}\", exc_info=True)\n",
    "        print(f\"Error: Failed to create inference encoder: {e}\")\n",
    "        return None, None # Return None for both if encoder fails\n",
    "\n",
    "    # --- 3. Create Inference Decoder Model ---\n",
    "    logging.info(\"Creating inference decoder model...\")\n",
    "    inf_decoder = None\n",
    "    try:\n",
    "        # Define Inputs\n",
    "        decoder_input_single_token = Input(shape=(1,), dtype='int32', name=\"inf_decoder_input_token\")\n",
    "        # Encoder output sequence depth = 2 * encoder lstm units per direction\n",
    "        inf_encoder_output_seq = Input(shape=(max_len_input, lstm_units * 2), name=\"inf_encoder_output_seq\")\n",
    "\n",
    "        # --- Define All Decoder Layer Instances FIRST ---\n",
    "        # Determine actual number of decoder layers (Should be 1 based on loaded layers)\n",
    "        actual_num_decoder_layers = 0\n",
    "        if \"decoder_lstm_1\" in loaded_layer_names: actual_num_decoder_layers = 1\n",
    "        # if \"decoder_lstm_2\" in loaded_layer_names: actual_num_decoder_layers = 2 # Add checks if needed\n",
    "        print(f\"Using {actual_num_decoder_layers} decoder LSTM layer(s) based on loaded model.\")\n",
    "        logging.info(f\"Using {actual_num_decoder_layers} decoder LSTM layer(s).\")\n",
    "        if actual_num_decoder_layers == 0:\n",
    "             raise ValueError(\"No decoder LSTM layers found in the loaded model.\")\n",
    "\n",
    "        # State inputs only needed for the layers present (h0, c0)\n",
    "        decoder_state_inputs = []\n",
    "        state_h_input = Input(shape=(decoder_lstm_units,), name=f'inf_decoder_input_h_0')\n",
    "        state_c_input = Input(shape=(decoder_lstm_units,), name=f'inf_decoder_input_c_0')\n",
    "        decoder_state_inputs.extend([state_h_input, state_c_input])\n",
    "\n",
    "        # Embedding Layer Instance\n",
    "        trained_decoder_embedding = trained_model.get_layer(\"decoder_embedding\")\n",
    "        inf_decoder_embedding = Embedding(trained_decoder_embedding.input_dim,\n",
    "                                           trained_decoder_embedding.output_dim,\n",
    "                                           mask_zero=False, name=\"inf_decoder_embedding\")\n",
    "        inf_decoder_embedding.build(input_shape=(None, 1))\n",
    "        inf_decoder_embedding.set_weights(trained_decoder_embedding.get_weights())\n",
    "        # Dropout after embedding (Use name 'dropout_1' from loaded list)\n",
    "        trained_dec_emb_dropout_layer = trained_model.get_layer(\"dropout_1\") # CHECKED from list\n",
    "        inf_dec_emb_dropout = Dropout(trained_dec_emb_dropout_layer.rate, name=\"inf_dropout_1\")\n",
    "\n",
    "        # LSTM Layer Instance (Only 1)\n",
    "        layer_name = \"decoder_lstm_1\"\n",
    "        trained_lstm_layer = trained_model.get_layer(layer_name)\n",
    "        inf_decoder_lstm = LSTM(decoder_lstm_units, return_sequences=True, return_state=True,\n",
    "                                dropout=trained_lstm_layer.dropout,\n",
    "                                recurrent_dropout=trained_lstm_layer.recurrent_dropout,\n",
    "                                name=f\"inf_{layer_name}\")\n",
    "        # Build LSTM\n",
    "        lstm_input_shape = (1, 1, trained_decoder_embedding.output_dim) # Takes embedding output\n",
    "        lstm_state_shape = (1, decoder_lstm_units)\n",
    "        inf_decoder_lstm.build(input_shape=[lstm_input_shape, lstm_state_shape, lstm_state_shape])\n",
    "        inf_decoder_lstm.set_weights(trained_lstm_layer.get_weights())\n",
    "\n",
    "        # Attention Layer Instance\n",
    "        trained_attention_layer = trained_model.get_layer(\"attention_layer\")\n",
    "        inf_attention_layer = AdditiveAttention(name=\"inf_attention_layer\")\n",
    "        # Build explicitly\n",
    "        build_query_shape = (1, 1, decoder_lstm_units)\n",
    "        build_value_shape = (1, max_len_input, lstm_units * 2)\n",
    "        inf_attention_layer.build(input_shape=[build_query_shape, build_value_shape])\n",
    "        if trained_attention_layer.get_weights():\n",
    "             inf_attention_layer.set_weights(trained_attention_layer.get_weights())\n",
    "\n",
    "        # Concatenate Layer Instance\n",
    "        inf_concat_layer = Concatenate(axis=-1, name=\"inf_decoder_attention_concat\")\n",
    "\n",
    "        # Post-Attention Dropout Instance (Use name 'dropout_2' from loaded list)\n",
    "        trained_post_attn_dropout_layer = trained_model.get_layer(\"dropout_2\") # CHECKED from list\n",
    "        inf_post_attn_dropout = Dropout(trained_post_attn_dropout_layer.rate, name=\"inf_dropout_2\")\n",
    "\n",
    "        # Dense Layer Instance\n",
    "        trained_dense_layer = trained_model.get_layer(\"output_layer\")\n",
    "        inf_dense_layer = Dense(trained_dense_layer.units,\n",
    "                                activation=trained_dense_layer.activation, name=\"inf_output_layer\")\n",
    "        # Build explicitly\n",
    "        concat_output_dim = decoder_lstm_units + (lstm_units * 2)\n",
    "        inf_dense_layer.build(input_shape=(None, 1, concat_output_dim))\n",
    "        inf_dense_layer.set_weights(trained_dense_layer.get_weights())\n",
    "\n",
    "        # --- Connect Layers using Symbolic Tensors ---\n",
    "        decoder_embeddings_tensor = inf_decoder_embedding(decoder_input_single_token)\n",
    "        decoder_output_step = inf_dec_emb_dropout(decoder_embeddings_tensor) # Apply dropout\n",
    "\n",
    "        # Call the single LSTM layer\n",
    "        decoder_output_step, state_h_out, state_c_out = inf_decoder_lstm(\n",
    "            decoder_output_step, initial_state=decoder_state_inputs # Pass [h0_in, c0_in]\n",
    "        )\n",
    "        # Define the state outputs\n",
    "        decoder_state_outputs = [state_h_out, state_c_out]\n",
    "\n",
    "        # Attention Call (Using positional list as keywords failed before)\n",
    "        # <<< Trying positional list again, maybe build fixed it >>>\n",
    "        context_vector, attention_weights = inf_attention_layer(\n",
    "             [decoder_output_step, inf_encoder_output_seq],\n",
    "             return_attention_scores=True # Get weights if needed later\n",
    "        )\n",
    "\n",
    "        # Concatenate\n",
    "        decoder_combined_context = inf_concat_layer([decoder_output_step, context_vector])\n",
    "        decoder_combined_context = inf_post_attn_dropout(decoder_combined_context) # Apply dropout\n",
    "\n",
    "        # Dense prediction\n",
    "        decoder_pred_outputs = inf_dense_layer(decoder_combined_context)\n",
    "\n",
    "        # Define the inference decoder model\n",
    "        inf_decoder = Model(\n",
    "             inputs=[decoder_input_single_token, inf_encoder_output_seq] + decoder_state_inputs,\n",
    "             outputs=[decoder_pred_outputs] + decoder_state_outputs, # logits + [h0_out, c0_out]\n",
    "             name=\"inference_decoder\"\n",
    "        )\n",
    "\n",
    "        print(\"Inference Decoder created.\")\n",
    "        inf_decoder.summary(line_length=120)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to create inference decoder: {e}\", exc_info=True)\n",
    "        print(f\"Error: Failed to create inference decoder: {e}\")\n",
    "        inf_decoder = None\n",
    "\n",
    "    finally:\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "    return inf_encoder, inf_decoder\n",
    "\n",
    "\n",
    "# --- Execution for Block 9 ---\n",
    "inference_encoder = None\n",
    "inference_decoder = None\n",
    "\n",
    "if os.path.exists(BEST_MODEL_SAVE_PATH):\n",
    "    print(f\"Attempting to load best model from: {BEST_MODEL_SAVE_PATH}\")\n",
    "    # Call the alternative setup function\n",
    "    # Pass NUM_DECODER_LAYERS from config (currently 1), function verifies against loaded model\n",
    "    inference_encoder, inference_decoder = setup_inference_models_attention_alt(\n",
    "        BEST_MODEL_SAVE_PATH,\n",
    "        num_decoder_layers_config=NUM_DECODER_LAYERS, # Pass config value\n",
    "        decoder_lstm_units=DECODER_LSTM_UNITS,\n",
    "        max_len_input=MAX_LEN_INPUT,\n",
    "        lstm_units=LSTM_UNITS\n",
    "    )\n",
    "\n",
    "    if inference_encoder is None or inference_decoder is None:\n",
    "        print(\"************************************************************\")\n",
    "        print(\"ERROR: Inference model setup failed. Cannot proceed to generation.\")\n",
    "        print(\"Check logs above for specific errors.\")\n",
    "        print(\"************************************************************\")\n",
    "        logging.error(\"Inference model setup failed.\")\n",
    "    else:\n",
    "        print(\"Block 9 completed successfully. Inference encoder and decoder created.\")\n",
    "        logging.info(\"Block 9 completed successfully. Inference encoder and decoder created.\")\n",
    "else:\n",
    "    print(f\"Skipping inference setup: Trained model not found at {BEST_MODEL_SAVE_PATH}\")\n",
    "    print(\"Ensure that training (Block 8) ran successfully and saved the model.\")\n",
    "    logging.error(f\"Trained model not found at {BEST_MODEL_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b70c88e-7ae4-468e-8b3c-6b44de5b74db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-25 03:37:34,226 - INFO - Block 10 completed. Generation functions defined.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Block 10: Generation Functions Defined ---\n",
      "Functions `generate_summary_greedy` and `generate_summary_beam_search` are now available.\n",
      "They will be used in Block 11 for evaluation.\n",
      "\n",
      "--- Sample Generation Test ---\n",
      "Input Text (sample):\n",
      "states can flexibly use of central assistance of inr4,000 per toilet, says shri venkaiah naidu parliament members discuss new urban schemes suggest assured flow of gst revenue to urban local bodies shortcomings of jnnurm reviewed shri naidu outlines new design improvements members of the parliamenta...\n",
      "\n",
      "Generating (Greedy):\n",
      "DEBUG: Greedy tokens before decode: [16924, 1, 4, 22, 7, 38, 13, 284, 211, 5, 31, 2156, 1776, 5, 375, 4, 203, 7, 4, 915, 179, 144, 5, 14, 1059, 149, 10, 205, 4, 233, 7, 362, 8, 592, 6, 4, 88, 5, 422, 18, 4, 22, 7, 38, 13, 284, 211, 5, 31, 1352, 768, 5, 150, 4, 177, 7, 4, 81, 1619, 11, 19, 108, 149, 6, 4, 22, 150, 4, 177, 7, 4, 81, 1619, 11, 19, 108, 149, 5, 2363, 32, 4, 36, 23, 15, 170, 10, 205, 4, 895, 7, 4, 1171, 5, 61, 4, 271, 7, 87, 95, 4, 81, 1619, 11, 19, 108, 149, 6, 4, 22, 34, 878, 4, 36, 23, 15, 226, 10, 1137, 4, 297, 23, 15, 41, 8, 4, 177, 7, 4, 81, 1619, 11, 19, 108, 149, 6, 4, 128, 34, 258, 14, 3751, 26627, 7, 4, 171, 1511, 5, 8, 4, 228]\n",
      "Time: 9.43s\n",
      "Output:\n",
      "▁dismantling<unk>▁the▁minister▁of▁state▁for▁home▁affairs,▁shri▁kiren▁rijiju,▁announced▁the▁progress▁of▁the▁swachh▁bharat▁mission,▁a▁nationwide▁initiative▁to▁improve▁the▁quality▁of▁life▁and▁efficiency.▁the▁meeting,▁attended▁by▁the▁minister▁of▁state▁for▁home▁affairs,▁shri▁piyush▁goyal,▁highlighted▁the▁importance▁of▁the▁\"make▁in▁india\"▁initiative.▁the▁minister▁highlighted▁the▁importance▁of▁the▁\"make▁in▁india\"▁initiative,▁noting▁that▁the▁government's▁efforts▁to▁improve▁the▁lives▁of▁the▁poor,▁including▁the▁use▁of▁technology▁like▁the▁\"make▁in▁india\"▁initiative.▁the▁minister▁also▁mentioned▁the▁government's▁commitment▁to▁supporting▁the▁nation's▁development▁and▁the▁importance▁of▁the▁\"make▁in▁india\"▁initiative.▁the▁event▁also▁included▁a▁brief▁biography▁of▁the▁chief▁guest,▁and▁the▁release\n",
      "\n",
      "Generating (Beam Search):\n",
      "DEBUG: Beam tokens before decode: [22980, 1, 8371, 1, 22, 7, 38, 13, 284, 211, 5, 31, 3665, 4624, 2622, 5, 150, 4, 803, 184, 7, 19, 23, 15, 488, 16, 4688, 1010, 6, 40, 202, 4, 177, 7, 3439, 19, 23, 15, 203, 11, 426, 4, 895, 7, 8087, 425, 5, 456, 30, 4, 803, 184, 7, 249, 11, 842, 4, 297, 23, 15, 771, 7, 2414, 14, 385, 297, 18, 3774, 172, 3284, 4, 2177, 7, 14, 2000, 1702, 24, 4, 268, 7, 608, 11, 4, 219, 402, 100, 5, 4, 36, 23, 15, 226, 10, 426, 4, 1065, 7, 1308, 325, 35, 34, 878, 6, 4, 22, 34, 878, 4, 36, 23, 15, 226, 10, 426, 4, 1065, 7, 1308, 325, 5, 4823, 4, 36, 23, 15, 226, 10, 426, 4, 1065, 7, 1308, 325, 6, 12, 1, 3394, 1, 3394, 1, 150, 4, 803, 184, 7, 19, 23, 15, 292, 11]\n",
      "Time: 40.12s\n",
      "Output:\n",
      "▁385<unk>start<unk>▁minister▁of▁state▁for▁home▁affairs,▁shri▁shripad▁yesso▁naik,▁highlighted▁the▁crucial▁role▁of▁india's▁self-help▁groups.▁he▁emphasized▁the▁importance▁of▁preserving▁india's▁progress▁in▁improving▁the▁lives▁of▁ordinary▁citizens,▁emphasizing▁the▁crucial▁role▁of▁youth▁in▁achieving▁the▁nation's▁goal▁of▁becoming▁a▁developed▁nation▁by▁2047.▁while▁acknowledging▁the▁lack▁of▁a▁shift▁away▁from▁the▁number▁of▁funds▁in▁the▁last▁five▁years,▁the▁government's▁commitment▁to▁improving▁the▁ease▁of▁doing▁business▁was▁also▁mentioned.▁the▁minister▁also▁mentioned▁the▁government's▁commitment▁to▁improving▁the▁ease▁of▁doing▁business,▁referencing▁the▁government's▁commitment▁to▁improving▁the▁ease▁of▁doing▁business.▁<unk>end<unk>end<unk>▁highlighted▁the▁crucial▁role▁of▁india's▁participation▁in\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Block 10: Generation Functions (Greedy and Beam Search) + Execution (with Detokenization Workaround)\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import logging\n",
    "import time\n",
    "from tqdm import tqdm # Use standard tqdm for non-notebook loops\n",
    "\n",
    "# --- Greedy Search Generation Function (with Detokenization Workaround) ---\n",
    "def generate_summary_greedy(input_text, tokenizer_obj, inf_encoder_model, inf_decoder_model,\n",
    "                             max_len_input, max_len_summary, start_token_id, end_token_id,\n",
    "                             pad_token_id):\n",
    "    \"\"\"Generates a summary using greedy decoding.\"\"\"\n",
    "    if not all([tokenizer_obj, inf_encoder_model, inf_decoder_model]):\n",
    "        logging.error(\"Greedy generation failed: Missing tokenizer or inference models.\")\n",
    "        return \"[Error: Missing components]\"\n",
    "\n",
    "    # 1. Tokenize and pad input text\n",
    "    try:\n",
    "        input_seq = tokenizer_obj.encode_as_ids(str(input_text))\n",
    "        encoder_input_data = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "            [input_seq], maxlen=max_len_input, padding='post', truncating='post', value=pad_token_id\n",
    "        ).astype(np.int32)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error tokenizing input for greedy generation: {e}\")\n",
    "        return \"[Error: Tokenization failed]\"\n",
    "\n",
    "    # 2. Encode the input sequence\n",
    "    try:\n",
    "        encoder_outputs = inf_encoder_model.predict(encoder_input_data, verbose=0)\n",
    "        encoder_output_seq = encoder_outputs[0]\n",
    "        decoder_states_value = [encoder_outputs[1], encoder_outputs[2]] # [h, c] for 1 decoder layer\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error running inference encoder: {e}\", exc_info=True)\n",
    "        return \"[Error: Encoder failure]\"\n",
    "\n",
    "    # 3. Initialize Decoder Input\n",
    "    decoder_input_token = np.array([[start_token_id]], dtype=np.int32)\n",
    "\n",
    "    # 4. Greedy Decoding Loop\n",
    "    decoded_tokens = []\n",
    "    for _ in range(max_len_summary):\n",
    "        try:\n",
    "            decoder_inputs = [decoder_input_token, encoder_output_seq] + decoder_states_value\n",
    "            decoder_outputs = inf_decoder_model.predict(decoder_inputs, verbose=0)\n",
    "            output_tokens_logits = decoder_outputs[0]\n",
    "            new_states = decoder_outputs[1:]\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during greedy decoder prediction step: {e}\", exc_info=True)\n",
    "            if not decoded_tokens or decoded_tokens[-1] != end_token_id:\n",
    "                 decoded_tokens.append(end_token_id)\n",
    "            break\n",
    "\n",
    "        sampled_token_id = int(np.argmax(output_tokens_logits[0, -1, :]))\n",
    "\n",
    "        if sampled_token_id == end_token_id:\n",
    "            break\n",
    "\n",
    "        if sampled_token_id != pad_token_id and sampled_token_id != start_token_id:\n",
    "             if isinstance(sampled_token_id, int):\n",
    "                decoded_tokens.append(sampled_token_id)\n",
    "             else:\n",
    "                logging.error(f\"Non-integer sampled_token_id encountered in greedy: {sampled_token_id} (type: {type(sampled_token_id)})\")\n",
    "                break\n",
    "\n",
    "        if len(decoded_tokens) >= max_len_summary:\n",
    "            break\n",
    "\n",
    "        decoder_input_token = np.array([[sampled_token_id]], dtype=np.int32)\n",
    "        if len(new_states) == len(decoder_states_value):\n",
    "           decoder_states_value = new_states\n",
    "        else:\n",
    "             logging.error(f\"State length mismatch during greedy decoding.\")\n",
    "             break\n",
    "\n",
    "    # 5. Detokenize the result (Piece by Piece Workaround)\n",
    "    print(f\"DEBUG: Greedy tokens before decode: {decoded_tokens}\") # Keep debug print\n",
    "    logging.debug(f\"Greedy tokens before decode: {decoded_tokens}\")\n",
    "    summary = \"[Error: Detokenization failed]\" # Default error message\n",
    "    try:\n",
    "        if not decoded_tokens:\n",
    "            summary = \"\"\n",
    "            logging.warning(\"Token list is empty for detokenization (Greedy).\")\n",
    "        else:\n",
    "            # Decode piece by piece\n",
    "            pieces = [tokenizer_obj.id_to_piece(token_id) for token_id in decoded_tokens]\n",
    "            # Join pieces - Replace SentencePiece space ' ' (U+2581) with normal space\n",
    "            summary = \"\".join(pieces).replace(' ', ' ').strip()\n",
    "            # Optional: More sophisticated joining\n",
    "            # summary = tokenizer_obj.DecodePieces(pieces)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error detokenizing greedy result (piece by piece): {e}\")\n",
    "        summary = f\"[Error: Detokenization failed ({e})]\"\n",
    "\n",
    "    return summary\n",
    "\n",
    "# --- Beam Search Generation Function (with Detokenization Workaround) ---\n",
    "def generate_summary_beam_search(input_text, tokenizer_obj, inf_encoder_model, inf_decoder_model,\n",
    "                                 max_len_input, max_len_summary, start_token_id, end_token_id,\n",
    "                                 pad_token_id, beam_width):\n",
    "    \"\"\"Generates a summary using beam search decoding.\"\"\"\n",
    "    if not all([tokenizer_obj, inf_encoder_model, inf_decoder_model]):\n",
    "        logging.error(\"Beam search generation failed: Missing tokenizer or inference models.\")\n",
    "        return \"[Error: Missing components]\"\n",
    "\n",
    "     # 1. Tokenize and pad input text\n",
    "    try:\n",
    "        input_seq = tokenizer_obj.encode_as_ids(str(input_text))\n",
    "        encoder_input_data = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "            [input_seq], maxlen=max_len_input, padding='post', truncating='post', value=pad_token_id\n",
    "        ).astype(np.int32)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error tokenizing input for beam search: {e}\")\n",
    "        return \"[Error: Tokenization failed]\"\n",
    "\n",
    "    # 2. Encode the input sequence\n",
    "    try:\n",
    "        encoder_outputs = inf_encoder_model.predict(encoder_input_data, verbose=0)\n",
    "        encoder_output_seq = encoder_outputs[0]\n",
    "        decoder_initial_states = [encoder_outputs[1], encoder_outputs[2]]\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error running inference encoder for beam search: {e}\", exc_info=True)\n",
    "        return \"[Error: Encoder failure]\"\n",
    "\n",
    "    # 3. Initialize Beam Search\n",
    "    initial_beam = (0.0, [start_token_id], decoder_initial_states)\n",
    "    beams = [initial_beam]\n",
    "    completed_beams = []\n",
    "\n",
    "    # 4. Beam Search Decoding Loop\n",
    "    for _ in range(max_len_summary):\n",
    "        new_beams = []\n",
    "        all_candidates = []\n",
    "        processed_beams_count = 0\n",
    "\n",
    "        for log_prob, seq, current_states in beams:\n",
    "            processed_beams_count += 1\n",
    "            if seq[-1] == end_token_id:\n",
    "                completed_beams.append((log_prob, seq, current_states))\n",
    "                continue\n",
    "\n",
    "            decoder_input_token = np.array([[seq[-1]]], dtype=np.int32)\n",
    "            try:\n",
    "                decoder_inputs = [decoder_input_token, encoder_output_seq] + current_states\n",
    "                decoder_outputs = inf_decoder_model.predict(decoder_inputs, verbose=0)\n",
    "                output_tokens_logits = decoder_outputs[0][0, -1, :]\n",
    "                new_states = decoder_outputs[1:]\n",
    "            except Exception as e:\n",
    "                 logging.warning(f\"Decoder prediction failed for beam step {processed_beams_count}: {e}\")\n",
    "                 completed_beams.append((log_prob - 100.0, seq + [end_token_id], current_states))\n",
    "                 continue\n",
    "\n",
    "            log_probs = tf.nn.log_softmax(output_tokens_logits).numpy()\n",
    "            vocab_size = len(log_probs)\n",
    "            effective_beam_width = min(beam_width, vocab_size)\n",
    "            top_k_indices = np.argsort(log_probs)[-effective_beam_width:]\n",
    "            top_k_log_probs = log_probs[top_k_indices]\n",
    "\n",
    "            for i in range(len(top_k_indices)):\n",
    "                token_id = int(top_k_indices[i]) # Ensure integer\n",
    "                if token_id == pad_token_id or token_id == start_token_id:\n",
    "                    continue\n",
    "                token_log_prob = top_k_log_probs[i]\n",
    "                new_seq = seq + [token_id]\n",
    "                new_log_prob = log_prob + token_log_prob\n",
    "                if all(isinstance(t, int) for t in new_seq):\n",
    "                    all_candidates.append((new_log_prob, new_seq, new_states))\n",
    "                else:\n",
    "                    logging.error(f\"Non-integer found in beam candidate sequence: {new_seq}\")\n",
    "\n",
    "\n",
    "        if processed_beams_count == len(completed_beams) and processed_beams_count > 0:\n",
    "            break\n",
    "        if not all_candidates:\n",
    "            break\n",
    "\n",
    "        ordered_candidates = sorted(all_candidates, key=lambda x: x[0], reverse=True)\n",
    "        beams = ordered_candidates[:beam_width]\n",
    "\n",
    "        if all(b[1][-1] == end_token_id for b in beams):\n",
    "            completed_beams.extend(beams)\n",
    "            break\n",
    "\n",
    "    # 5. Final Selection\n",
    "    completed_beams.extend(beams)\n",
    "    if not completed_beams:\n",
    "         logging.warning(\"Beam search finished with no completed beams.\")\n",
    "         return \"[Error: No sequences generated]\"\n",
    "\n",
    "    best_beam = sorted(completed_beams, key=lambda x: x[0], reverse=True)[0]\n",
    "    best_seq = best_beam[1]\n",
    "\n",
    "    # 6. Detokenize (Piece by Piece Workaround)\n",
    "    final_tokens = [token for token in best_seq if token not in [start_token_id, end_token_id, pad_token_id]]\n",
    "    print(f\"DEBUG: Beam tokens before decode: {final_tokens}\") # Keep debug print\n",
    "    logging.debug(f\"Beam tokens before decode: {final_tokens}\")\n",
    "    summary = \"[Error: Detokenization failed]\" # Default error message\n",
    "    try:\n",
    "        if not final_tokens:\n",
    "            summary = \"\"\n",
    "            logging.warning(\"Token list is empty for detokenization (Beam).\")\n",
    "        else:\n",
    "             # Decode piece by piece\n",
    "            pieces = [tokenizer_obj.id_to_piece(token_id) for token_id in final_tokens]\n",
    "            # Join pieces\n",
    "            summary = \"\".join(pieces).replace(' ', ' ').strip()\n",
    "            # Optional: Try DecodePieces\n",
    "            # summary = tokenizer_obj.DecodePieces(pieces)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error detokenizing beam search result (piece by piece): {e}\")\n",
    "        summary = f\"[Error: Detokenization failed ({e})]\"\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "# --- Execution for Block 10 ---\n",
    "print(\"\\n--- Block 10: Generation Functions Defined ---\")\n",
    "print(\"Functions `generate_summary_greedy` and `generate_summary_beam_search` are now available.\")\n",
    "print(\"They will be used in Block 11 for evaluation.\")\n",
    "logging.info(\"Block 10 completed. Generation functions defined.\")\n",
    "\n",
    "# --- Optional: Test one generation ---\n",
    "# Check if all necessary components exist before testing\n",
    "if ('inference_encoder' in locals() and inference_encoder and\n",
    "    'inference_decoder' in locals() and inference_decoder and\n",
    "    'tokenizer' in locals() and tokenizer and\n",
    "    'processed_df' in locals() and not processed_df.empty):\n",
    "\n",
    "    # Select a sample ensuring it's not empty\n",
    "    sample_text = None\n",
    "    for text in processed_df['cleaned_text']:\n",
    "        if text and isinstance(text, str) and len(text.split()) > 5:\n",
    "             sample_text = text\n",
    "             break\n",
    "\n",
    "    if sample_text:\n",
    "        print(\"\\n--- Sample Generation Test ---\")\n",
    "        print(f\"Input Text (sample):\\n{sample_text[:300]}...\")\n",
    "\n",
    "        print(\"\\nGenerating (Greedy):\")\n",
    "        start_g = time.time()\n",
    "        greedy_summary = generate_summary_greedy(\n",
    "            sample_text, tokenizer, inference_encoder, inference_decoder,\n",
    "            MAX_LEN_INPUT, MAX_LEN_SUMMARY, START_ID, END_ID, PAD_ID\n",
    "        )\n",
    "        print(f\"Time: {time.time() - start_g:.2f}s\")\n",
    "        print(f\"Output:\\n{greedy_summary}\") # Check this output carefully\n",
    "\n",
    "        print(\"\\nGenerating (Beam Search):\")\n",
    "        start_b = time.time()\n",
    "        beam_summary = generate_summary_beam_search(\n",
    "            sample_text, tokenizer, inference_encoder, inference_decoder,\n",
    "            MAX_LEN_INPUT, MAX_LEN_SUMMARY, START_ID, END_ID, PAD_ID, BEAM_WIDTH\n",
    "        )\n",
    "        print(f\"Time: {time.time() - start_b:.2f}s\")\n",
    "        print(f\"Output:\\n{beam_summary}\") # Check this output carefully\n",
    "        print(\"-\" * 30)\n",
    "    else:\n",
    "        print(\"\\nCould not find a valid non-empty sample text in processed_df for testing.\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nSkipping sample generation test: Inference models, tokenizer, or processed_df not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4504ca5f-df67-41a0-a3c6-0ceda778b080",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-25 03:54:01,468 - INFO - Opened PDF: Test_Pdfs_folder/PIB_270235_2025_04_15.pdf, Pages: 1\n",
      "2025-04-25 03:54:01,488 - INFO - Successfully extracted text from Test_Pdfs_folder/PIB_270235_2025_04_15.pdf.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Summarizing PDF: Test_Pdfs_folder/PIB_270235_2025_04_15.pdf ---\n",
      "\n",
      "Step 1: Extracting text from PDF...\n",
      "Extraction time: 0.04s\n",
      "Successfully extracted 2446 characters.\n",
      "\n",
      "Step 2: Cleaning extracted text...\n",
      "Cleaning time: 0.00s\n",
      "Text cleaned successfully.\n",
      "\n",
      "Cleaned Text (First 500 chars):\n",
      " 15-april-2025 1054 ist ' haj pilgrimage the accords high priority for indian muslims to undertake the annual haj pilgrimage. as a result of its efforts, the country allocation for india which was 136,020 in 2014 has gradually increased to 175,025 in 2025. these quotas are finalized by the saudi authorities closer to the time of the pilgrimage. the moma through the haj committee of india manages arrangements for the bulk of the quota allotted to india, which is 122,518 in the current year. all th ...\n",
      "\n",
      "Step 3: Generating summary using Beam Search...\n",
      "DEBUG: Beam tokens before decode: [29854, 1, 8371, 1, 25, 315, 228, 24, 4, 384, 15, 170, 10, 205, 19, 23, 15, 299, 8, 121, 6, 40, 150, 4, 36, 23, 15, 226, 10, 426, 4, 1065, 7, 1308, 325, 5, 479, 4, 177, 7, 3439, 19, 23, 15, 356, 630, 6, 172, 3284, 4, 353, 1899, 18, 19, 23, 15, 1445, 3575, 5, 12, 1561, 2703, 103, 26, 4, 1966, 7, 4, 81, 1619, 11, 19, 108, 149, 5, 166, 10, 205, 4, 895, 7, 8087, 425, 6, 12, 1, 3394, 1, 3394, 1, 150, 4, 177, 7, 3439, 19, 23, 15, 528, 8, 57, 226, 10, 426, 4, 1065, 7, 1308, 325, 6, 12, 1, 3394, 1, 3394, 1, 150, 4, 803, 184, 7, 19, 23, 15, 488, 16, 4688, 1010, 6, 12, 1, 3394, 1, 3394, 1, 202, 4, 177, 7, 3439, 19, 23, 15, 356, 630, 6, 12, 1, 3394, 1]\n",
      "Generation time: 38.64s\n",
      "\n",
      "--- Generated Summary ---\n",
      "14.10<unk>start<unk>▁this▁press▁release▁from▁the▁'s▁efforts▁to▁improve▁india's▁safety▁and▁security.▁he▁highlighted▁the▁government's▁commitment▁to▁improving▁the▁ease▁of▁doing▁business,▁highlighting▁the▁importance▁of▁preserving▁india's▁cultural▁heritage.▁while▁acknowledging▁the▁challenges▁faced▁by▁india's▁freedom▁struggle,▁citing▁examples▁such▁as▁the▁example▁of▁the▁\"make▁in▁india\"▁initiative,▁aims▁to▁improve▁the▁lives▁of▁ordinary▁citizens.▁<unk>end<unk>end<unk>▁highlighted▁the▁importance▁of▁preserving▁india's▁economy▁and▁its▁commitment▁to▁improving▁the▁ease▁of▁doing▁business.▁<unk>end<unk>end<unk>▁highlighted▁the▁crucial▁role▁of▁india's▁self-help▁groups.▁<unk>end<unk>end<unk>▁emphasized▁the▁importance▁of▁preserving▁india's▁cultural▁heritage.▁<unk>end<unk>\n",
      "-------------------------\n",
      "\n",
      "PDF Summarization Test Complete.\n"
     ]
    }
   ],
   "source": [
    "# Block 12: Summarize a Single PDF File\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "# Attempt to import the PDF library\n",
    "try:\n",
    "    import fitz  # PyMuPDF\n",
    "    _pymupdf_installed = True\n",
    "except ImportError:\n",
    "    print(\"PyMuPDF not found. Please install it to process PDFs:\")\n",
    "    print(\"!pip install PyMuPDF\")\n",
    "    print(\"Then RESTART the kernel and re-run this block.\")\n",
    "    _pymupdf_installed = False\n",
    "\n",
    "# --- PDF Text Extraction Function ---\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extracts text from all pages of a PDF file using PyMuPDF.\"\"\"\n",
    "    if not _pymupdf_installed:\n",
    "        logging.error(\"PyMuPDF library not available. Cannot extract PDF text.\")\n",
    "        return None, \"PyMuPDF library not installed.\"\n",
    "\n",
    "    if not os.path.exists(pdf_path):\n",
    "        logging.error(f\"PDF file not found at: {pdf_path}\")\n",
    "        return None, f\"PDF file not found at: {pdf_path}\"\n",
    "\n",
    "    extracted_text = \"\"\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        logging.info(f\"Opened PDF: {pdf_path}, Pages: {doc.page_count}\")\n",
    "        for page_num in range(doc.page_count):\n",
    "            page = doc.load_page(page_num)\n",
    "            extracted_text += page.get_text(\"text\") # Extract plain text\n",
    "            extracted_text += \"\\n\" # Add newline between pages\n",
    "        doc.close()\n",
    "        logging.info(f\"Successfully extracted text from {pdf_path}.\")\n",
    "        return extracted_text, None # Return text and no error\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to extract text from PDF {pdf_path}: {e}\", exc_info=True)\n",
    "        return None, f\"Error extracting text from PDF: {e}\"\n",
    "\n",
    "# --- Summarization Execution ---\n",
    "\n",
    "# <<< IMPORTANT: SET THE PATH TO YOUR PDF FILE BELOW >>>\n",
    "pdf_path_to_summarize = \"Test_Pdfs_folder/PIB_270235_2025_04_15.pdf\" # REPLACE WITH ACTUAL PATH\n",
    "\n",
    "print(f\"\\n--- Summarizing PDF: {pdf_path_to_summarize} ---\")\n",
    "\n",
    "# Check if all necessary components are ready\n",
    "summarization_possible = True\n",
    "if not _pymupdf_installed:\n",
    "    print(\"Cannot proceed: PyMuPDF is not installed.\")\n",
    "    summarization_possible = False\n",
    "elif 'tokenizer' not in locals() or tokenizer is None:\n",
    "    print(\"Cannot proceed: Tokenizer not found (Run Block 4?).\")\n",
    "    summarization_possible = False\n",
    "elif 'inference_encoder' not in locals() or inference_encoder is None:\n",
    "    print(\"Cannot proceed: Inference encoder not found (Run Block 9?).\")\n",
    "    summarization_possible = False\n",
    "elif 'inference_decoder' not in locals() or inference_decoder is None:\n",
    "    print(\"Cannot proceed: Inference decoder not found (Run Block 9?).\")\n",
    "    summarization_possible = False\n",
    "elif 'clean_text' not in globals():\n",
    "     print(\"Cannot proceed: clean_text function not defined (Run Block 2?).\")\n",
    "     summarization_possible = False\n",
    "elif 'generate_summary_beam_search' not in globals(): # Or check for greedy if preferred\n",
    "      print(\"Cannot proceed: Generation function not defined (Run Block 10?).\")\n",
    "      summarization_possible = False\n",
    "\n",
    "\n",
    "if summarization_possible:\n",
    "    # 1. Extract Text\n",
    "    print(\"\\nStep 1: Extracting text from PDF...\")\n",
    "    start_extract = time.time()\n",
    "    raw_text, error_msg = extract_text_from_pdf(pdf_path_to_summarize)\n",
    "    print(f\"Extraction time: {time.time() - start_extract:.2f}s\")\n",
    "\n",
    "    if error_msg:\n",
    "        print(f\"Extraction Failed: {error_msg}\")\n",
    "    elif not raw_text or not raw_text.strip():\n",
    "         print(\"Extraction Failed: No text found in the PDF.\")\n",
    "    else:\n",
    "        print(f\"Successfully extracted {len(raw_text)} characters.\")\n",
    "\n",
    "        # 2. Clean Text\n",
    "        print(\"\\nStep 2: Cleaning extracted text...\")\n",
    "        start_clean = time.time()\n",
    "        # Use the clean_text function defined in Block 2\n",
    "        cleaned_input_text = clean_text(raw_text)\n",
    "        print(f\"Cleaning time: {time.time() - start_clean:.2f}s\")\n",
    "\n",
    "        if not cleaned_input_text or not cleaned_input_text.strip():\n",
    "            print(\"Cleaning Failed: Text became empty after cleaning.\")\n",
    "        else:\n",
    "            print(\"Text cleaned successfully.\")\n",
    "            print(\"\\nCleaned Text (First 500 chars):\\n\", cleaned_input_text[:500], \"...\")\n",
    "\n",
    "            # 3. Generate Summary\n",
    "            print(\"\\nStep 3: Generating summary using Beam Search...\")\n",
    "            start_gen = time.time()\n",
    "            # Use the generate_summary_beam_search function from Block 10\n",
    "            generated_summary = generate_summary_beam_search(\n",
    "                input_text=cleaned_input_text,\n",
    "                tokenizer_obj=tokenizer,\n",
    "                inf_encoder_model=inference_encoder,\n",
    "                inf_decoder_model=inference_decoder,\n",
    "                max_len_input=MAX_LEN_INPUT,\n",
    "                max_len_summary=MAX_LEN_SUMMARY,\n",
    "                start_token_id=START_ID,\n",
    "                end_token_id=END_ID,\n",
    "                pad_token_id=PAD_ID,\n",
    "                beam_width=BEAM_WIDTH\n",
    "            )\n",
    "            print(f\"Generation time: {time.time() - start_gen:.2f}s\")\n",
    "\n",
    "            # 4. Display Summary\n",
    "            print(\"\\n--- Generated Summary ---\")\n",
    "            print(generated_summary)\n",
    "            print(\"-\" * 25)\n",
    "\n",
    "print(\"\\nPDF Summarization Test Complete.\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-tensorflow-tensorflow",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "TensorFlow 2-11 (Local)",
   "language": "python",
   "name": "conda-env-tensorflow-tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
